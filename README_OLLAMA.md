# Ollama æœ¬åœ°å¤§æ¨¡å‹é›†æˆæŒ‡å—

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [å®‰è£… Ollama](#å®‰è£…-ollama)
- [ä¸‹è½½æ¨¡å‹](#ä¸‹è½½æ¨¡å‹)
- [é…ç½®ç³»ç»Ÿ](#é…ç½®ç³»ç»Ÿ)
- [å¯åŠ¨æœåŠ¡](#å¯åŠ¨æœåŠ¡)
- [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)
- [æ”¯æŒçš„æ¨¡å‹](#æ”¯æŒçš„æ¨¡å‹)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## æ¦‚è¿°

æœ¬ç³»ç»Ÿç°å·²æ”¯æŒä½¿ç”¨ **Ollama** åœ¨æœ¬åœ°è¿è¡Œå¤§è¯­è¨€æ¨¡å‹,å®ç°å®Œå…¨ç¦»çº¿ã€éšç§ä¿æŠ¤çš„ AI å¯¹è¯åŠŸèƒ½ã€‚

### âœ¨ Ollama é›†æˆä¼˜åŠ¿

- ğŸ”’ **å®Œå…¨ç¦»çº¿**: æ— éœ€äº’è”ç½‘è¿æ¥,æ•°æ®å®Œå…¨æœ¬åœ°åŒ–
- ğŸ’° **é›¶æˆæœ¬**: æ—  API è°ƒç”¨è´¹ç”¨,æ— ä½¿ç”¨é™åˆ¶
- ğŸš€ **ä½å»¶è¿Ÿ**: æœ¬åœ°æ¨ç†,å“åº”é€Ÿåº¦å¿«
- ğŸ” **éšç§ä¿æŠ¤**: æ•æ„Ÿæ•°æ®ä¸ç¦»å¼€æœ¬åœ°æœåŠ¡å™¨
- ğŸ¯ **å¯å®šåˆ¶**: æ”¯æŒåŠ è½½è‡ªå®šä¹‰å¾®è°ƒæ¨¡å‹

### ğŸ¯ é€‚ç”¨åœºæ™¯

- ä¼ä¸šå†…ç½‘éƒ¨ç½²(æ•°æ®å®‰å…¨è¦æ±‚é«˜)
- å¼€å‘æµ‹è¯•ç¯å¢ƒ(é™ä½æˆæœ¬)
- ç¦»çº¿ç¯å¢ƒä½¿ç”¨
- ç§å¯†å¯¹è¯åœºæ™¯
- è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²

---

## å¿«é€Ÿå¼€å§‹

**5 åˆ†é’Ÿå¿«é€Ÿä½“éªŒ Ollama é›†æˆ**:

```bash
# 1. å®‰è£… Ollama (Windows)
# è®¿é—® https://ollama.com/download/windows ä¸‹è½½å®‰è£…åŒ…

# 2. ä¸‹è½½æ¨¡å‹
ollama pull qwen2.5:latest

# 3. å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# 4. å¯åŠ¨è¯­éŸ³ä»£ç†ç³»ç»Ÿ (æ–°ç»ˆç«¯)
cd d:\Projects\ivanHappyWoods\backEnd
$env:VOICE_AGENT_ENVIRONMENT="ollama"; python start_server.py

# 5. æµ‹è¯•
python test_persistence_simple.py
```

---

## å®‰è£… Ollama

### Windows ç³»ç»Ÿ

1. **ä¸‹è½½å®‰è£…åŒ…**:
   - è®¿é—® [Ollama å®˜ç½‘](https://ollama.com/download/windows)
   - ä¸‹è½½ Windows å®‰è£…ç¨‹åº
   - åŒå‡»è¿è¡Œå®‰è£…

2. **éªŒè¯å®‰è£…**:
   ```powershell
   ollama --version
   # è¾“å‡ºç¤ºä¾‹: ollama version 0.1.25
   ```

3. **ç¯å¢ƒå˜é‡** (é€šå¸¸è‡ªåŠ¨é…ç½®):
   - `OLLAMA_HOST=127.0.0.1:11434`
   - `PATH` ä¸­åŒ…å« Ollama å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„

### Linux / macOS

```bash
# ä¸€é”®å®‰è£…
curl -fsSL https://ollama.com/install.sh | sh

# éªŒè¯
ollama --version
```

### Docker éƒ¨ç½² (å¯é€‰)

```bash
docker pull ollama/ollama
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

---

## ä¸‹è½½æ¨¡å‹

### æ¨èæ¨¡å‹åˆ—è¡¨

| æ¨¡å‹åç§° | å¤§å° | ç”¨é€” | ä¸‹è½½å‘½ä»¤ |
|---------|------|------|----------|
| **qwen2.5:latest** | ~4.7GB | ä¸­æ–‡å¯¹è¯(æ¨è) | `ollama pull qwen2.5:latest` |
| **llama3.2:latest** | ~2GB | å¿«é€Ÿå“åº” | `ollama pull llama3.2:latest` |
| **llama3.1:8b** | ~4.7GB | è‹±æ–‡å¯¹è¯ | `ollama pull llama3.1:8b` |
| **mistral:latest** | ~4.1GB | å¹³è¡¡æ€§èƒ½ | `ollama pull mistral:latest` |
| **deepseek-coder:latest** | ~3.8GB | ä»£ç ç”Ÿæˆ | `ollama pull deepseek-coder:latest` |

### ä¸‹è½½æ­¥éª¤

```bash
# 1. æŸ¥çœ‹å¯ç”¨æ¨¡å‹
ollama list

# 2. æœç´¢æ¨¡å‹ (æµè§ˆå™¨è®¿é—®)
# https://ollama.com/library

# 3. ä¸‹è½½æ¨¡å‹
ollama pull qwen2.5:latest

# 4. éªŒè¯ä¸‹è½½
ollama list
# è¾“å‡º:
# NAME                 ID              SIZE      MODIFIED
# qwen2.5:latest       abc123def456    4.7 GB    2 minutes ago

# 5. æµ‹è¯•æ¨¡å‹
ollama run qwen2.5:latest
# è¾“å…¥: ä½ å¥½
# è¾“å‡º: ä½ å¥½!æˆ‘æ˜¯ Qwen,å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡...
# (è¾“å…¥ /bye é€€å‡º)
```

### æ¨¡å‹é€‰æ‹©å»ºè®®

- **ä¸­æ–‡å¯¹è¯ä¼˜å…ˆ**: qwen2.5 (é˜¿é‡Œé€šä¹‰åƒé—®)
- **é€Ÿåº¦ä¼˜å…ˆ**: llama3.2 (è¾ƒå°æ¨¡å‹)
- **è‹±æ–‡å¯¹è¯**: llama3.1 (Meta å®˜æ–¹)
- **ä»£ç ä»»åŠ¡**: deepseek-coder (æ·±åº¦æ±‚ç´¢)
- **å¹³è¡¡é€‰æ‹©**: mistral (Mistral AI)

---

## é…ç½®ç³»ç»Ÿ

### æ–¹å¼ 1: ä½¿ç”¨é¢„é…ç½®æ–‡ä»¶ (æ¨è)

é¡¹ç›®å·²åŒ…å« `config/ollama.yaml` é…ç½®æ–‡ä»¶:

```yaml
llm:
  provider: "ollama"
  base_url: "http://localhost:11434/v1"  # Ollama API åœ°å€
  api_key: ""  # Ollama æ— éœ€ API Key
  timeout: 60  # æœ¬åœ°æ¨ç†å¯èƒ½è¾ƒæ…¢,å¢åŠ è¶…æ—¶
  
  models:
    default: "qwen2.5:latest"    # é»˜è®¤æ¨¡å‹
    fast: "llama3.2:latest"       # å¿«é€Ÿæ¨¡å‹
    creative: "qwen2.5:latest"    # åˆ›æ„æ¨¡å‹

  max_tokens: 4096
  temperature: 0.7
  stream: true
```

**ä½¿ç”¨æ–¹æ³•**:
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡æŒ‡å‘ Ollama é…ç½®
$env:VOICE_AGENT_ENVIRONMENT="ollama"
python start_server.py
```

### æ–¹å¼ 2: ç¯å¢ƒå˜é‡è¦†ç›–

```bash
# è®¾ç½® Ollama ç›¸å…³ç¯å¢ƒå˜é‡
$env:VOICE_AGENT_LLM__PROVIDER="ollama"
$env:VOICE_AGENT_LLM__BASE_URL="http://localhost:11434/v1"
$env:VOICE_AGENT_LLM__API_KEY=""
$env:VOICE_AGENT_LLM__MODELS__DEFAULT="qwen2.5:latest"
$env:VOICE_AGENT_LLM__TIMEOUT="60"

# å¯åŠ¨æœåŠ¡
python start_server.py
```

### é…ç½®é¡¹è¯´æ˜

| é…ç½®é¡¹ | è¯´æ˜ | Ollama å€¼ |
|-------|------|-----------|
| `provider` | æä¾›å•†åç§° | `"ollama"` |
| `base_url` | API åŸºç¡€åœ°å€ | `"http://localhost:11434/v1"` |
| `api_key` | API å¯†é’¥ | `""` (ç•™ç©º) |
| `timeout` | è¶…æ—¶æ—¶é—´(ç§’) | `60` (æœ¬åœ°æ¨ç†è¾ƒæ…¢) |
| `models.default` | é»˜è®¤æ¨¡å‹ | `"qwen2.5:latest"` |
| `max_tokens` | æœ€å¤§ç”Ÿæˆé•¿åº¦ | `4096` |

---

## å¯åŠ¨æœåŠ¡

### æ­¥éª¤ 1: å¯åŠ¨ Ollama æœåŠ¡

```bash
# æ–°ç»ˆç«¯ 1: å¯åŠ¨ Ollama
ollama serve

# è¾“å‡º:
# Ollama is running on http://localhost:11434
```

**éªŒè¯ Ollama è¿è¡Œ**:
```bash
# æ–°ç»ˆç«¯æµ‹è¯•
curl http://localhost:11434/api/tags

# æˆ–è®¿é—®æµè§ˆå™¨
# http://localhost:11434
```

### æ­¥éª¤ 2: å¯åŠ¨è¯­éŸ³ä»£ç†ç³»ç»Ÿ

```bash
# æ–°ç»ˆç«¯ 2: å¯åŠ¨åç«¯æœåŠ¡
cd d:\Projects\ivanHappyWoods\backEnd

# ä½¿ç”¨ Ollama é…ç½®å¯åŠ¨
$env:VOICE_AGENT_ENVIRONMENT="ollama"; python start_server.py

# çœ‹åˆ°ä»¥ä¸‹æ—¥å¿—è¡¨ç¤ºæˆåŠŸ:
# âœ… ä½¿ç”¨ PostgreSQL æ•°æ®åº“æŒä¹…åŒ–
# âœ… LLM Provider: ollama
# âœ… LLM Base URL: http://localhost:11434/v1
# âœ… Default Model: qwen2.5:latest
# INFO:     Uvicorn running on http://0.0.0.0:8000
```

### æ­¥éª¤ 3: éªŒè¯ç³»ç»ŸçŠ¶æ€

```bash
# æ–°ç»ˆç«¯ 3: å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# é¢„æœŸè¾“å‡º:
# {
#   "status": "healthy",
#   "database": "connected",
#   "llm_provider": "ollama"
# }
```

---

## æµ‹è¯•éªŒè¯

### æµ‹è¯•è„šæœ¬ 1: ç®€å•å¯¹è¯

åˆ›å»º `test_ollama_basic.py`:

```python
import asyncio
import httpx

async def test_ollama_chat():
    """æµ‹è¯• Ollama åŸºç¡€å¯¹è¯"""
    url = "http://localhost:8000/api/conversation/send"
    
    payload = {
        "session_id": "ollama_test_001",
        "message": "ä½ å¥½,è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±",
        "stream": False
    }
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(url, json=payload)
        print(f"Status: {response.status_code}")
        print(f"Response: {response.json()}")

asyncio.run(test_ollama_chat())
```

**è¿è¡Œæµ‹è¯•**:
```bash
python test_ollama_basic.py

# é¢„æœŸè¾“å‡º:
# Status: 200
# Response: {
#   "session_id": "ollama_test_001",
#   "message": "ä½ å¥½!æˆ‘æ˜¯ Qwen,ä¸€ä¸ªç”±é˜¿é‡Œäº‘å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹...",
#   "model_used": "qwen2.5:latest"
# }
```

### æµ‹è¯•è„šæœ¬ 2: æŒä¹…åŒ–éªŒè¯

ä½¿ç”¨ç°æœ‰çš„ `test_persistence_simple.py`,ä¿®æ”¹ä¸º Ollama é…ç½®:

```python
# åœ¨è„šæœ¬å¼€å¤´æ·»åŠ 
import os
os.environ['VOICE_AGENT_ENVIRONMENT'] = 'ollama'

# ç„¶åè¿è¡Œ
python test_persistence_simple.py
```

**é¢„æœŸç»“æœ**:
- âœ… å‘é€åˆå§‹æ¶ˆæ¯æˆåŠŸ
- âœ… æ•°æ®åº“ä¿å­˜ checkpoint
- âœ… é‡å¯æœåŠ¡åèƒ½å¤Ÿæ¢å¤ä¸Šä¸‹æ–‡

### æµ‹è¯•è„šæœ¬ 3: æµå¼å“åº”

```python
import asyncio
import httpx

async def test_ollama_streaming():
    """æµ‹è¯• Ollama æµå¼å“åº”"""
    url = "http://localhost:8000/api/conversation/stream"
    
    params = {
        "session_id": "ollama_stream_001",
        "message": "åˆ—ä¸¾3ä¸ª Python çš„ç‰¹ç‚¹,æ¯ä¸ªç”¨ä¸€å¥è¯è¯´æ˜"
    }
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        async with client.stream("GET", url, params=params) as response:
            print("Streaming response:")
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    print(line[6:])  # å»æ‰ "data: " å‰ç¼€

asyncio.run(test_ollama_streaming())
```

---

## æ”¯æŒçš„æ¨¡å‹

### å·²é…ç½®æ¨¡å‹ç‰¹æ€§

ç³»ç»Ÿå·²åœ¨ `src/utils/llm_compat.py` ä¸­é…ç½®ä»¥ä¸‹ Ollama æ¨¡å‹:

| æ¨¡å‹ | ä¸Šä¸‹æ–‡é•¿åº¦ | å‚æ•°æ ¼å¼ | æ¸©åº¦æ”¯æŒ | å‡½æ•°è°ƒç”¨ |
|------|-----------|---------|---------|---------|
| **qwen2.5** | 32K | max_tokens | âœ… | âœ… |
| **qwen2** | 32K | max_tokens | âœ… | âœ… |
| **llama3.2** | 8K | max_tokens | âœ… | âœ… |
| **llama3.1** | 128K | max_tokens | âœ… | âœ… |
| **mistral** | 32K | max_tokens | âœ… | âœ… |
| **deepseek-coder** | 16K | max_tokens | âœ… | âœ… |

### æ·»åŠ æ–°æ¨¡å‹

å¦‚éœ€æ·»åŠ å…¶ä»– Ollama æ¨¡å‹:

1. **åœ¨ `src/utils/llm_compat.py` ä¸­æ·»åŠ æ¨¡å‹ç‰¹æ€§**:
   ```python
   "your-model-name": {
       "max_tokens_param": "max_tokens",
       "supports_temperature": True,
       "supports_vision": False,
       "supports_function_calling": True,
       "max_context": 8192,
       "provider": "ollama",
   },
   ```

2. **ä¸‹è½½æ¨¡å‹**:
   ```bash
   ollama pull your-model-name:latest
   ```

3. **æ›´æ–°é…ç½®æ–‡ä»¶** `config/ollama.yaml`:
   ```yaml
   llm:
     models:
       default: "your-model-name:latest"
   ```

### æ¨¡å‹æ ‡ç­¾æ”¯æŒ

ç³»ç»Ÿæ”¯æŒ Ollama çš„æ¨¡å‹æ ‡ç­¾æ ¼å¼:

- âœ… `qwen2.5:latest` - ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
- âœ… `qwen2.5:7b` - æŒ‡å®šå‚æ•°é‡ç‰ˆæœ¬
- âœ… `llama3.1:8b-instruct-fp16` - å®Œæ•´æ ‡ç­¾

**è‡ªåŠ¨åŒ¹é…é€»è¾‘**:
- ç²¾ç¡®åŒ¹é…: `qwen2.5:latest` â†’ æŸ¥æ‰¾ `qwen2.5:latest`
- æ ‡ç­¾å‰¥ç¦»: `qwen2.5:latest` â†’ æŸ¥æ‰¾ `qwen2.5` â†’ åŒ¹é…æˆåŠŸ
- å‰ç¼€åŒ¹é…: `qwen2.5-custom` â†’ æŸ¥æ‰¾ `qwen2.5` â†’ åŒ¹é…æˆåŠŸ

---

## å¸¸è§é—®é¢˜

### Q1: Ollama æœåŠ¡æœªå¯åŠ¨

**ç—‡çŠ¶**: 
```
httpx.ConnectError: [Errno 10061] No connection could be made
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®è®¤ Ollama æ­£åœ¨è¿è¡Œ
curl http://localhost:11434/api/tags

# å¦‚æœå¤±è´¥,å¯åŠ¨ Ollama
ollama serve
```

### Q2: æ¨¡å‹æœªä¸‹è½½

**ç—‡çŠ¶**:
```
{"error": "model not found"}
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥å·²ä¸‹è½½æ¨¡å‹
ollama list

# ä¸‹è½½ç¼ºå¤±æ¨¡å‹
ollama pull qwen2.5:latest
```

### Q3: æ¨ç†é€Ÿåº¦æ…¢

**ç—‡çŠ¶**: å“åº”æ—¶é—´ > 30 ç§’

**è§£å†³æ–¹æ¡ˆ**:

1. **ä½¿ç”¨æ›´å°çš„æ¨¡å‹**:
   ```bash
   ollama pull llama3.2:latest  # 2GB,é€Ÿåº¦æ›´å¿«
   ```

2. **å‡å°‘ç”Ÿæˆé•¿åº¦**:
   ```yaml
   llm:
     max_tokens: 1024  # ä» 4096 å‡å°‘åˆ° 1024
   ```

3. **GPU åŠ é€Ÿ**:
   - Ollama è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ GPU
   - ç¡®è®¤ NVIDIA é©±åŠ¨å·²å®‰è£…
   - æŸ¥çœ‹ GPU ä½¿ç”¨: `nvidia-smi`

### Q4: ç«¯å£å†²çª

**ç—‡çŠ¶**:
```
Error: listen tcp 127.0.0.1:11434: bind: address already in use
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
netstat -ano | findstr :11434

# æ€æ‰è¿›ç¨‹ (æ›¿æ¢ PID)
taskkill /PID <pid> /F

# æˆ–æ›´æ”¹ Ollama ç«¯å£
$env:OLLAMA_HOST="127.0.0.1:11435"
ollama serve
```

### Q5: æ•°æ®åº“è¿æ¥å¤±è´¥

**ç—‡çŠ¶**: 
```
Database connection failed: could not connect to server
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®è®¤ PostgreSQL å®¹å™¨è¿è¡Œ
docker ps | findstr voice_agent_postgres

# å¦‚æœæœªè¿è¡Œ,å¯åŠ¨å®¹å™¨
docker start voice_agent_postgres

# æˆ–å¯åŠ¨æ•´ä¸ªç¯å¢ƒ
docker-compose up -d
```

### Q6: ä¸­æ–‡ä¹±ç 

**ç—‡çŠ¶**: Ollama è¿”å›ä¹±ç æˆ–æ–¹å—å­—ç¬¦

**è§£å†³æ–¹æ¡ˆ**:

1. **ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹**:
   ```bash
   ollama pull qwen2.5:latest  # é˜¿é‡Œé€šä¹‰åƒé—®,ä¸­æ–‡æ•ˆæœå¥½
   ```

2. **æ£€æŸ¥ç¼–ç **:
   ```python
   # åœ¨ä»£ç ä¸­ç¡®ä¿ UTF-8 ç¼–ç 
   response = await client.post(url, json=payload, headers={"Content-Type": "application/json; charset=utf-8"})
   ```

---

## æ€§èƒ½ä¼˜åŒ–

### ç¡¬ä»¶è¦æ±‚

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|---------|---------|
| **CPU** | 4 æ ¸ | 8 æ ¸ä»¥ä¸Š |
| **å†…å­˜** | 8GB | 16GB ä»¥ä¸Š |
| **æ˜¾å¡** | æ—  (CPU æ¨ç†) | NVIDIA RTX 3060 (6GB VRAM) æˆ–æ›´é«˜ |
| **ç¡¬ç›˜** | 20GB å¯ç”¨ç©ºé—´ | SSD, 50GB ä»¥ä¸Š |

### GPU åŠ é€Ÿ

**æ£€æŸ¥ GPU æ”¯æŒ**:
```bash
# Ollama ä¼šè‡ªåŠ¨æ£€æµ‹ GPU
ollama serve

# æŸ¥çœ‹æ—¥å¿—
# å¦‚æœæœ‰ GPU: "CUDA available, using GPU"
# å¦‚æœæ—  GPU: "CUDA not available, using CPU"
```

**å¼ºåˆ¶ä½¿ç”¨ CPU** (è°ƒè¯•ç”¨):
```bash
$env:CUDA_VISIBLE_DEVICES=""
ollama serve
```

### æ¨¡å‹é‡åŒ–

Ollama æ”¯æŒé‡åŒ–æ¨¡å‹ä»¥å‡å°‘å†…å­˜å ç”¨:

```bash
# åŸå§‹æ¨¡å‹ (çº¦ 15GB)
ollama pull llama3.1:70b

# 4-bit é‡åŒ– (çº¦ 4GB)
ollama pull llama3.1:70b-q4_0

# 8-bit é‡åŒ– (çº¦ 8GB)
ollama pull llama3.1:70b-q8_0
```

### å¹¶å‘ä¼˜åŒ–

ä¿®æ”¹ `config/ollama.yaml`:

```yaml
api:
  workers: 2  # å‡å°‘å¹¶å‘æ•°,é¿å… OOM
  
llm:
  timeout: 120  # å¢åŠ è¶…æ—¶æ—¶é—´
  max_tokens: 2048  # å‡å°‘ç”Ÿæˆé•¿åº¦
```

### ç¼“å­˜ç­–ç•¥

Ollama è‡ªåŠ¨ç¼“å­˜æ¨¡å‹åˆ°å†…å­˜:

```bash
# æŸ¥çœ‹ Ollama å†…å­˜å ç”¨
ollama ps

# æ¸…ç†ç¼“å­˜
ollama stop <model_name>
```

---

## æ··åˆéƒ¨ç½²

### åœºæ™¯: Ollama å¯¹è¯ + äº‘ç«¯ TTS

ä½¿ç”¨ Ollama è¿›è¡Œå¯¹è¯,ä½¿ç”¨ç§‘å¤§è®¯é£è¿›è¡Œè¯­éŸ³åˆæˆ:

```yaml
# config/hybrid.yaml
llm:
  provider: "ollama"
  base_url: "http://localhost:11434/v1"
  models:
    default: "qwen2.5:latest"

speech:
  tts:
    provider: "iflytek"  # ä½¿ç”¨äº‘ç«¯ TTS
    appid: "${IFLYTEK_APPID}"
    apikey: "${IFLYTEK_APIKEY}"
```

**å¯åŠ¨**:
```bash
$env:VOICE_AGENT_ENVIRONMENT="hybrid"; python start_server.py
```

---

## ç›‘æ§ä¸è°ƒè¯•

### æŸ¥çœ‹ Ollama æ—¥å¿—

```bash
# Windows
Get-Content "$env:USERPROFILE\.ollama\logs\server.log" -Tail 50 -Wait

# Linux / macOS
tail -f ~/.ollama/logs/server.log
```

### æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—

```bash
# åç«¯æ—¥å¿—
Get-Content logs\voice_agent.log -Tail 50 -Wait

# æ•°æ®åº“æ—¥å¿—
docker logs -f voice_agent_postgres
```

### æ€§èƒ½ç›‘æ§

```python
# æ·»åŠ åˆ°ä»£ç ä¸­
import time

start = time.time()
response = await llm_call()
elapsed = time.time() - start

print(f"LLM æ¨ç†è€—æ—¶: {elapsed:.2f}ç§’")
```

---

## æ•…éšœæ’æŸ¥æ¸…å•

**é‡åˆ°é—®é¢˜æ—¶,æŒ‰é¡ºåºæ£€æŸ¥**:

- [ ] Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ: `curl http://localhost:11434/api/tags`
- [ ] æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½: `ollama list`
- [ ] PostgreSQL æ˜¯å¦è¿è¡Œ: `docker ps | findstr postgres`
- [ ] é…ç½®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®: `$env:VOICE_AGENT_ENVIRONMENT="ollama"`
- [ ] ç¯å¢ƒå˜é‡æ˜¯å¦ç”Ÿæ•ˆ: `echo $env:VOICE_AGENT_ENVIRONMENT`
- [ ] ç«¯å£æ˜¯å¦è¢«å ç”¨: `netstat -ano | findstr :8000`
- [ ] æ—¥å¿—ä¸­æ˜¯å¦æœ‰é”™è¯¯: `Get-Content logs\voice_agent.log`
- [ ] API å¥åº·æ£€æŸ¥: `curl http://localhost:8000/health`

---

## ä¸‹ä¸€æ­¥

âœ… **Ollama é›†æˆå®Œæˆå,å¯ä»¥å°è¯•**:

1. **è®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹**: ä½¿ç”¨ Ollama åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
2. **å¤šæ¨¡å‹åˆ‡æ¢**: åœ¨å¯¹è¯ä¸­åŠ¨æ€åˆ‡æ¢ä¸åŒæ¨¡å‹
3. **RAG é›†æˆ**: ç»“åˆæœ¬åœ°å‘é‡æ•°æ®åº“(å¦‚ ChromaDB)
4. **ç¦»çº¿éƒ¨ç½²**: å®Œå…¨æ–­ç½‘ç¯å¢ƒä¸‹çš„ AI ç³»ç»Ÿ

---

## å‚è€ƒé“¾æ¥

- [Ollama å®˜æ–¹ç½‘ç«™](https://ollama.com/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Ollama æ¨¡å‹åº“](https://ollama.com/library)
- [é€šä¹‰åƒé—®å®˜ç½‘](https://tongyi.aliyun.com/)
- [é¡¹ç›®æ–‡æ¡£](./PROJECT.md)

---

## ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | è¯´æ˜ |
|------|------|------|
| 1.0 | 2025-01-18 | åˆå§‹ç‰ˆæœ¬,æ”¯æŒ 6 ä¸ª Ollama æ¨¡å‹ |

---

**è´¡çŒ®è€…**: Ivan_HappyWoods Team  
**æœ€åæ›´æ–°**: 2025-01-18  
**è®¸å¯è¯**: [é¡¹ç›®è®¸å¯è¯]
