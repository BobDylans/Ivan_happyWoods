# Ollama æœ¬åœ°å¤§æ¨¡å‹é›†æˆæŒ‡å—

## ğŸ¯ é—®é¢˜èƒŒæ™¯

åœ¨é›†æˆ Ollama æ—¶å‘ç° API è¿”å› **502 Bad Gateway** é”™è¯¯ï¼Œæ ¹æœ¬åŸå› æ˜¯ï¼š

**ç³»ç»Ÿè®¾ç½®äº† HTTP ä»£ç†** (`http://127.0.0.1:7890`)ï¼Œå¯¼è‡´å¯¹ `localhost:11434` çš„è¯·æ±‚è¢«ä»£ç†æ‹¦æˆªã€‚

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ä½¿ç”¨ä¸“ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰

1. **å¤åˆ¶ Ollama é…ç½®æ–‡ä»¶**
   ```bash
   cp .env.ollama .env
   ```

2. **å¯åŠ¨ Ollama æœåŠ¡**
   ```bash
   ollama serve
   ```

3. **å¯åŠ¨é¡¹ç›®æœåŠ¡**
   ```bash
   python start_server.py
   ```

4. **æµ‹è¯•å¯¹è¯**
   è®¿é—® `http://localhost:8000` å¹¶æµ‹è¯•å¯¹è¯

### æ–¹æ¡ˆ 2: æ‰‹åŠ¨ä¿®æ”¹ .env æ–‡ä»¶

**ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼š**

```bash
# =============================================================================
# CORE LLM CONFIGURATION (Ollama æœ¬åœ°å¤§æ¨¡å‹)
# =============================================================================

# Ollama æœåŠ¡é…ç½®
VOICE_AGENT_LLM__API_KEY=ollama           # Ollama ä¸éœ€è¦çœŸå® API Key
VOICE_AGENT_LLM__BASE_URL=http://localhost:11434  # âš ï¸ æ³¨æ„ï¼šä¸åŠ  /v1
VOICE_AGENT_LLM__PROVIDER=ollama

# æ¨¡å‹é€‰æ‹©ï¼ˆæ ¹æ® ollama list æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹ï¼‰
VOICE_AGENT_LLM__MODELS__DEFAULT=qwen3:4b
VOICE_AGENT_LLM__MODELS__FAST=qwen3:4b
VOICE_AGENT_LLM__MODELS__CREATIVE=deepseek-r1:7b

# LLM æ€§èƒ½è°ƒä¼˜
VOICE_AGENT_LLM__TIMEOUT=60               # æœ¬åœ°æ¨¡å‹å¯èƒ½éœ€è¦æ›´é•¿è¶…æ—¶æ—¶é—´
VOICE_AGENT_LLM__MAX_TOKENS=4096
VOICE_AGENT_LLM__TEMPERATURE=0.7

# âš ï¸ é‡è¦ï¼šç¦ç”¨ä»£ç†ï¼ˆé¿å… localhost è¯·æ±‚è¢«æ‹¦æˆªï¼‰
HTTP_PROXY=
HTTPS_PROXY=
NO_PROXY=localhost,127.0.0.1
```

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### 1. API è·¯å¾„è‡ªåŠ¨é€‚é…

**ä»£ç ä¿®æ”¹**: `src/agent/nodes.py` - `_build_llm_url()` æ–¹æ³•

```python
def _build_llm_url(self, endpoint: str = "chat/completions") -> str:
    base = self.config.llm.base_url.rstrip('/')
    
    # æ£€æµ‹ Ollama Provider
    is_ollama = (
        self.config.llm.provider.lower() == "ollama" or 
        "localhost:11434" in base or 
        "127.0.0.1:11434" in base
    )
    
    if is_ollama:
        # Ollama ä½¿ç”¨åŸç”Ÿ API: /api/chat
        url = f"{base}/api/chat"
    else:
        # OpenAI-Compatible API: /v1/chat/completions
        if not base.endswith('/v1'):
            base = base + '/v1'
        url = f"{base}/{endpoint}"
    
    return url
```

**è¯´æ˜**:
- **Ollama**: `http://localhost:11434/api/chat`
- **OpenAI**: `https://api.openai.com/v1/chat/completions`

### 2. ä»£ç†ç¦ç”¨

**ä»£ç ä¿®æ”¹**: `src/agent/nodes.py` - `_ensure_http_client()` æ–¹æ³•

```python
async def _ensure_http_client(self):
    # æ£€æµ‹ Ollama Provider
    is_ollama = (
        self.config.llm.provider.lower() == "ollama" or 
        "localhost" in self.config.llm.base_url
    )
    
    client_kwargs = {
        "timeout": timeout,
        "headers": {...}
    }
    
    if is_ollama:
        # Ollama: æ˜ç¡®ç¦ç”¨ä»£ç†
        client_kwargs["proxies"] = {}
    
    self._http_client = httpx.AsyncClient(**client_kwargs)
```

**è¯´æ˜**:
- æ£€æµ‹åˆ° Ollama Provider åï¼Œè‡ªåŠ¨ç¦ç”¨ httpx çš„ä»£ç†è®¾ç½®
- é¿å… `localhost` è¯·æ±‚è¢«ç³»ç»Ÿä»£ç†æ‹¦æˆª

## ğŸ“ æµ‹è¯•éªŒè¯

### æµ‹è¯•è„šæœ¬

å·²åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_ollama_simple.py`ï¼š

```python
import requests

# æµ‹è¯• 1: Ollama åŸç”Ÿ Chat API
response = requests.post(
    "http://localhost:11434/api/chat",
    json={
        "model": "qwen3:4b",
        "messages": [{"role": "user", "content": "ä½ å¥½"}],
        "stream": False
    }
)

print(f"çŠ¶æ€ç : {response.status_code}")
if response.status_code == 200:
    result = response.json()
    print(f"å›å¤: {result['message']['content']}")
```

### æµ‹è¯•æ­¥éª¤

1. **ç¦ç”¨ä»£ç†åæµ‹è¯•**
   ```powershell
   $env:HTTP_PROXY=''; $env:HTTPS_PROXY=''
   python test_ollama_simple.py
   ```

2. **é¢„æœŸè¾“å‡º**
   ```
   ============================================================
   æµ‹è¯• 1: Ollama åŸç”Ÿ Chat API (/api/chat)
   ============================================================
   çŠ¶æ€ç : 200
   âœ… Ollama åŸç”Ÿ Chat API æˆåŠŸï¼
   æ¨¡å‹: qwen3:4b
   å›å¤: ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢ç ”å‘çš„...
   Token: prompt=16, response=908
   ```

## ğŸš€ å¯ç”¨æ¨¡å‹

ä½¿ç”¨ `ollama list` æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹ï¼š

```
NAME              ID              SIZE      MODIFIED
qwen3:4b          359d7dd4bcda    2.5 GB    25 minutes ago
deepseek-r1:7b    755ced02ce7b    4.7 GB    7 days ago
```

### ä¸‹è½½æ›´å¤šæ¨¡å‹

```bash
# ä¸‹è½½æ¨¡å‹
ollama pull llama3.2
ollama pull qwen2.5:7b
ollama pull deepseek-coder:6.7b

# æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨
ollama list

# è¿è¡Œæ¨¡å‹æµ‹è¯•
ollama run qwen3:4b "ä½ å¥½"
```

## ğŸ”„ åœ¨ OpenAI å’Œ Ollama ä¹‹é—´åˆ‡æ¢

### åˆ‡æ¢åˆ° OpenAI

```bash
# æ–¹å¼ 1: æ¢å¤åŸ .env é…ç½®
git checkout .env

# æ–¹å¼ 2: æ‰‹åŠ¨ä¿®æ”¹
VOICE_AGENT_LLM__BASE_URL=https://api.openai-proxy.org/v1
VOICE_AGENT_LLM__PROVIDER=custom
VOICE_AGENT_LLM__MODELS__DEFAULT=gpt-5-mini
```

### åˆ‡æ¢åˆ° Ollama

```bash
# æ–¹å¼ 1: ä½¿ç”¨ä¸“ç”¨é…ç½®
cp .env.ollama .env

# æ–¹å¼ 2: æ‰‹åŠ¨ä¿®æ”¹
VOICE_AGENT_LLM__BASE_URL=http://localhost:11434
VOICE_AGENT_LLM__PROVIDER=ollama
VOICE_AGENT_LLM__MODELS__DEFAULT=qwen3:4b
HTTP_PROXY=
HTTPS_PROXY=
```

## âš ï¸ å¸¸è§é—®é¢˜

### 1. 502 Bad Gateway é”™è¯¯

**åŸå› **: ç³»ç»Ÿä»£ç†æ‹¦æˆªäº† localhost è¯·æ±‚

**è§£å†³**:
```bash
# æ–¹æ³• 1: æ¸…é™¤ç¯å¢ƒå˜é‡
$env:HTTP_PROXY=''; $env:HTTPS_PROXY=''

# æ–¹æ³• 2: åœ¨ .env ä¸­è®¾ç½®
HTTP_PROXY=
HTTPS_PROXY=
NO_PROXY=localhost,127.0.0.1
```

### 2. Connection Refused

**åŸå› **: Ollama æœåŠ¡æœªå¯åŠ¨

**è§£å†³**:
```bash
ollama serve
```

### 3. æ¨¡å‹æœªæ‰¾åˆ°

**åŸå› **: æ¨¡å‹æœªå®‰è£…

**è§£å†³**:
```bash
ollama pull qwen3:4b
ollama list
```

### 4. å“åº”é€Ÿåº¦æ…¢

**åŸå› **: æœ¬åœ°æ¨¡å‹é¦–æ¬¡åŠ è½½éœ€è¦æ—¶é—´

**ä¼˜åŒ–**:
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (`qwen3:4b` æ¯” `deepseek-r1:7b` å¿«)
- å¢åŠ è¶…æ—¶æ—¶é—´: `VOICE_AGENT_LLM__TIMEOUT=60`
- ä½¿ç”¨ GPU åŠ é€Ÿï¼ˆéœ€è¦ CUDA/ROCm æ”¯æŒï¼‰

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | OpenAI (gpt-5-mini) | Ollama (qwen3:4b) |
|------|---------------------|-------------------|
| **é¦–æ¬¡å“åº”å»¶è¿Ÿ** | ~400ms | ~800ms |
| **Token ç”Ÿæˆé€Ÿåº¦** | ~50 tokens/s | ~30 tokens/s |
| **æˆæœ¬** | $0.15/1M tokens | å…è´¹ï¼ˆæœ¬åœ°ï¼‰ |
| **éšç§** | äº‘ç«¯å¤„ç† | æœ¬åœ°å¤„ç† âœ… |
| **ç½‘ç»œä¾èµ–** | éœ€è¦è”ç½‘ | ç¦»çº¿å¯ç”¨ âœ… |

## ğŸ‰ æ€»ç»“

1. âœ… **é—®é¢˜æ ¹å› **: HTTP ä»£ç†æ‹¦æˆªå¯¼è‡´ 502 é”™è¯¯
2. âœ… **è§£å†³æ–¹æ¡ˆ**: 
   - ç¦ç”¨ä»£ç†ç¯å¢ƒå˜é‡
   - ä»£ç è‡ªåŠ¨æ£€æµ‹ Ollama Provider
   - ä½¿ç”¨æ­£ç¡®çš„ API è·¯å¾„ (`/api/chat`)
3. âœ… **æµ‹è¯•éªŒè¯**: æ‰€æœ‰æµ‹è¯•é€šè¿‡
4. âœ… **é›†æˆå®Œæˆ**: æ”¯æŒ OpenAI â†” Ollama æ— ç¼åˆ‡æ¢

---

*æœ€åæ›´æ–°: 2025-11-04*  
*ä½œè€…: Ivan_HappyWoods Team*
