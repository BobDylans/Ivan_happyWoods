# 长对话截断问题诊断指南

**问题**: 面对长对话时，LLM 返回的信息被截断，无法完整显示  
**创建时间**: 2025-10-17  
**状态**: 🔍 诊断中

---

## 🎯 问题分析

长对话截断可能发生在以下环节：

```
用户输入 → LLM API → 后端接收 → 流式传输 → 前端解析 → 页面渲染
              ↓          ↓          ↓          ↓          ↓
            ❓1        ❓2        ❓3        ❓4        ❓5
```

### 可能原因

1. **❓ LLM API 限制** (最常见)
   - `max_tokens` 不够，LLM 直接截断响应
   - `finish_reason = "length"` 表示因 token 限制停止

2. **❓ 后端接收不完整**
   - httpx 超时
   - 网络连接中断
   - 缓冲区溢出

3. **❓ 流式传输丢失**
   - SSE 分片时数据丢失
   - `buffer = lines.pop()` 导致数据丢失（已修复）

4. **❓ 前端解析错误**
   - JavaScript buffer 处理不当
   - SSE 事件解析失败

5. **❓ 前端渲染限制**
   - DOM 更新失败
   - Markdown 渲染超时

---

## 🔍 诊断步骤

### 步骤 1: 启用详细日志

我已经在以下位置添加了详细的诊断日志：

#### 📤 LLM 请求日志
```
============================================================
📤 LLM API 请求参数:
  Model: gpt-5-mini
  Max Tokens: 8192
  Temperature: 0.7
  Messages Count: 5
  Tools Count: 7
  估算输入 Tokens: ~1500
============================================================
```

#### 📥 LLM 响应日志（非流式）
```
============================================================
📥 LLM API 响应:
  Choices Count: 1
  ⭐ Finish Reason: length  ← ⚠️ 关键指标！
  Content Length: 15234 字符
  Tool Calls Count: 0
  Token Usage:
    - Prompt Tokens: 1523
    - Completion Tokens: 8192  ← ⚠️ 达到上限！
    - Total Tokens: 9715
  Content Preview: 这是响应的前 200 字符...
============================================================
```

#### 📥 LLM 流式响应日志
```
============================================================
📥 [STREAM] LLM 流式响应总结:
  Total Content Length: 12345 字符
  Delta Events Count: 245
  Tool Calls: 0
  Content Preview: 这是响应的前 200 字符...
  ⚠️ 响应内容较长，如果看起来不完整，可能是 max_tokens 限制
============================================================
```

---

### 步骤 2: 运行测试

#### 测试用例 1: 短对话（基线）

**输入**:
```
"你好"
```

**预期**:
- ✅ 正常完整响应
- ✅ Finish Reason: stop
- ✅ Completion Tokens: < 100

---

#### 测试用例 2: 中等长度（常规）

**输入**:
```
"请详细解释 Python 的垃圾回收机制"
```

**预期**:
- ✅ 响应完整
- ✅ Finish Reason: stop
- ✅ Completion Tokens: 500-1000

---

#### 测试用例 3: 长对话（问题场景）⭐

**输入**:
```
"请详细解释 Python 的垃圾回收机制，包括引用计数、标记清除、分代回收三个部分，每个部分至少 500 字，并给出代码示例"
```

**预期问题**:
- ❌ 响应被截断
- ❌ Finish Reason: length （关键！）
- ❌ Completion Tokens: 8192（达到上限）

**观察点**:
1. 查看后端日志中的 `Finish Reason`
2. 查看 `Completion Tokens` 是否达到 `Max Tokens`
3. 查看前端接收的总字符数

---

### 步骤 3: 分析日志

#### 场景 A: finish_reason = "length"

**结论**: ✅ **根本原因找到！** LLM API 的 max_tokens 限制

**日志特征**:
```
⭐ Finish Reason: length
❌ 响应被截断！原因: max_tokens 限制
💡 建议: 增加 max_tokens 或减少输入长度
Completion Tokens: 8192 (达到上限)
```

**解决方案**:
1. **增加 max_tokens**（推荐）
   ```python
   # src/agent/state.py
   max_tokens=16384  # 从 8192 提升到 16384
   ```

2. **减少输入长度**
   - 缩短对话历史
   - 精简系统提示词

3. **分段回复**
   - 让 LLM 分多轮回复

---

#### 场景 B: finish_reason = "stop"，但前端不完整

**结论**: ✅ **LLM 返回完整**，问题在传输/解析

**日志特征**:
```
✅ Finish Reason: stop (正常结束)
Content Length: 12345 字符  ← 后端接收完整
```

**但前端显示不完整**

**解决方案**:
1. 检查前端控制台日志
   ```
   📦 接收到数据块，待处理行数: X
   📝 累积内容，当前长度: X
   🎨 最终渲染，总字符数: X
   ```

2. 对比后端 `Content Length` 和前端 `总字符数`
   - 如果一致 → 渲染问题
   - 如果不一致 → 传输问题

3. 检查 SSE buffer 处理（已优化）

---

#### 场景 C: 后端无完整日志

**结论**: ✅ **请求未到达 LLM**，可能是网络/配置问题

**日志特征**:
```
(缺少 📤 请求日志)
或
LLM HTTP 500: Internal Server Error
```

**解决方案**:
1. 检查 API Key 和 Base URL
2. 检查网络连接
3. 查看 LLM 服务商状态

---

## 📋 信息收集清单

请提供以下信息以便诊断：

### 1. 配置信息
- [ ] `.env` 中的 `VOICE_AGENT_LLM__MAX_TOKENS` 值
- [ ] 使用的模型名称（如 `gpt-5-mini`）
- [ ] 是否启用了流式模式

### 2. 问题描述
- [ ] 触发截断的具体输入（问题内容）
- [ ] 大约在多少字符/行数后截断
- [ ] 是突然停止还是逐渐变慢

### 3. 日志信息（重要！）⭐

#### 后端日志（查找以下关键行）
```bash
# 在终端输出或 logs/ 目录中查找

# 请求日志
📤 LLM API 请求参数:
  Model: ...
  Max Tokens: ...

# 响应日志
📥 LLM API 响应:
  ⭐ Finish Reason: ...  ← 最关键！
  Content Length: ...
  Token Usage:
    - Completion Tokens: ...
```

#### 前端日志（浏览器 F12 控制台）
```
🚀 开始接收流式响应
📦 接收到数据块，待处理行数: X
📝 累积内容，当前长度: X
🎨 最终渲染，总字符数: X
✅ 流式响应接收完成，总字符数: X
```

### 4. 对比数据
- [ ] 后端 `Content Length`: _____ 字符
- [ ] 前端 `总字符数`: _____ 字符
- [ ] 页面实际显示: _____ 字符（目测）

---

## 🛠️ 快速修复方案

### 方案 1: 提升 max_tokens（推荐）⭐

如果日志显示 `finish_reason = "length"`，直接增加 max_tokens：

**修改 `.env` 文件**:
```bash
# 从 8192 提升到 16384 或更高
VOICE_AGENT_LLM__MAX_TOKENS=16384
```

**或修改代码**:
```python
# src/agent/state.py
max_tokens=16384  # 从 8192 提升
```

**重启服务**:
```bash
python start_server.py
```

---

### 方案 2: 分段回复

如果 max_tokens 已经很大，但内容仍然太长：

**在系统提示词中添加**:
```python
# src/agent/nodes.py - _get_system_message()

"如果回复内容超过 5000 字，请分多次回复，每次回复结束时提示'[待续]'"
```

---

### 方案 3: 优化输入长度

减少对话历史长度：

```python
# src/agent/nodes.py - _prepare_llm_messages()

# 只保留最近 10 条消息
MAX_HISTORY_MESSAGES = 10
```

---

## 📊 诊断流程图

```
开始诊断
   ↓
运行测试用例 3 (长对话)
   ↓
查看后端日志
   ↓
┌─────────────────┐
│ Finish Reason?  │
└────────┬────────┘
         │
    ┌────┴────┐
    │         │
"length"   "stop"
    │         │
    ↓         ↓
max_tokens  后端完整
不足         ↓
    │     前端日志
    │         │
    │    ┌────┴────┐
    │    │         │
    │  一致     不一致
    │    │         │
    │  渲染     传输
    │  问题     问题
    │
    ↓
解决方案:
1. 提升 max_tokens
2. 分段回复
3. 优化输入
```

---

## 🎯 执行计划

### 立即执行

1. **重启服务**（加载新日志）
   ```bash
   # 停止当前服务 (Ctrl+C)
   python start_server.py
   ```

2. **运行测试用例**
   ```
   输入: "请详细解释 Python 的垃圾回收机制，包括引用计数、标记清除、分代回收，每个部分至少 500 字"
   ```

3. **收集日志**
   - 复制后端终端输出
   - 复制前端控制台日志
   - 提供给我分析

### 预期结果

我们将看到类似这样的日志：

```
============================================================
📤 LLM API 请求参数:
  Model: gpt-5-mini
  Max Tokens: 8192
  ...
============================================================
📥 LLM API 响应:
  ⭐ Finish Reason: length  ← 如果这里是 "length"，问题就找到了！
  Content Length: 15234 字符
  Token Usage:
    - Completion Tokens: 8192  ← 达到上限
============================================================
```

如果确认是 `finish_reason = "length"`，我们就可以直接增加 `max_tokens` 解决问题！

---

## 📞 需要帮助

如果按照以上步骤仍无法解决，请提供：

1. **完整的后端日志**（从 📤 到 📥）
2. **前端控制台截图**
3. **测试输入和观察到的现象**

我会根据具体日志进一步诊断！

---

*诊断指南创建于 2025-10-17*  
*维护者: Ivan_HappyWoods Team*
