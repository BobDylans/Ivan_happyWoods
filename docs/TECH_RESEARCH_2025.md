# ğŸš€ Ivan_HappyWoods 2025 æŠ€æœ¯ç ”ç©¶ä¸å‡çº§å»ºè®®

> **æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0
> **åˆ›å»ºæ—¥æœŸ**: 2025-11-12
> **ç ”ç©¶èŒƒå›´**: LangGraphã€RAGã€å¯è§‚æµ‹æ€§ã€å®æ—¶è¯­éŸ³ã€å¤šAgentç³»ç»Ÿ
> **çŠ¶æ€**: å»ºè®®æ–¹æ¡ˆ

---

## ğŸ“‹ ç›®å½•

- [ç ”ç©¶èƒŒæ™¯](#ç ”ç©¶èƒŒæ™¯)
- [æŠ€æœ¯è°ƒç ”æˆæœ](#æŠ€æœ¯è°ƒç ”æˆæœ)
  - [LangGraph 2025 æœ€æ–°ç‰¹æ€§](#1-langgraph-2025-æœ€æ–°ç‰¹æ€§)
  - [RAG ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯](#2-rag-ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯)
  - [FastAPI å¯è§‚æµ‹æ€§æœ€ä½³å®è·µ](#3-fastapi-å¯è§‚æµ‹æ€§æœ€ä½³å®è·µ)
  - [å®æ—¶è¯­éŸ³æŠ€æœ¯](#4-å®æ—¶è¯­éŸ³æŠ€æœ¯)
  - [å¤šAgentåä½œæ¡†æ¶](#5-å¤šagentåä½œæ¡†æ¶)
- [è¿‘æœŸå¯ç«‹å³åº”ç”¨çš„æŠ€æœ¯å‡çº§](#è¿‘æœŸå¯ç«‹å³åº”ç”¨çš„æŠ€æœ¯å‡çº§)
- [ä¸­æœŸå¯æ¢ç´¢çš„æ–°æŠ€æœ¯](#ä¸­æœŸå¯æ¢ç´¢çš„æ–°æŠ€æœ¯)
- [æ¶æ„æ”¹è¿›å»ºè®®](#æ¶æ„æ”¹è¿›å»ºè®®)
- [æŠ€æœ¯é€‰å‹ä¼˜å…ˆçº§çŸ©é˜µ](#æŠ€æœ¯é€‰å‹ä¼˜å…ˆçº§çŸ©é˜µ)
- [å®æ–½è·¯çº¿å›¾](#å®æ–½è·¯çº¿å›¾)

---

## ç ”ç©¶èƒŒæ™¯

åŸºäºé¡¹ç›®å½“å‰çŠ¶æ€ï¼ˆç‰ˆæœ¬ 0.4.0-betaï¼‰ï¼Œé’ˆå¯¹ä»¥ä¸‹æ–¹é¢è¿›è¡Œäº†2025å¹´æœ€æ–°æŠ€æœ¯è°ƒç ”ï¼š

**å½“å‰æŠ€æœ¯æ ˆ**:
- LangGraph 0.6+ (AI Agent ç¼–æ’)
- FastAPI + Uvicorn (Webæ¡†æ¶)
- Qdrant (å‘é‡æ•°æ®åº“)
- ç§‘å¤§è®¯é£ STT/TTS (è¯­éŸ³æœåŠ¡)
- PostgreSQL + Alembic (æ•°æ®æŒä¹…åŒ–)

**è°ƒç ”ç›®æ ‡**:
- å‘æ˜æœ€æ–°æ¡†æ¶å’Œå·¥å…·
- ä¼˜åŒ–ç°æœ‰æ¶æ„æ€§èƒ½
- æå‡ç³»ç»Ÿç¨³å®šæ€§å’Œå¯è§‚æµ‹æ€§
- æ¢ç´¢å·®å¼‚åŒ–ç«äº‰åŠ›

---

## æŠ€æœ¯è°ƒç ”æˆæœ

### 1. LangGraph 2025 æœ€æ–°ç‰¹æ€§

#### æ ¸å¿ƒæ¶æ„æ¼”è¿›

LangGraph åœ¨ 2025 å¹´å·²æˆä¸ºæœ€æˆç†Ÿçš„ AI Agent ç¼–æ’æ¡†æ¶ä¹‹ä¸€ï¼Œä» LangChain å›¢é˜Ÿç‹¬ç«‹å‘å±•ï¼Œä¸“æ³¨äº**å›¾çŠ¶æ€ç®¡ç†**å’Œ**é«˜çº§Agentæ„å»º**ã€‚

**å…³é”®ç‰¹æ€§**:
- **Graph-Based Execution**: å°† Agent è¡Œä¸ºå»ºæ¨¡ä¸ºæœ‰å‘å›¾ï¼Œæ”¯æŒæ¡ä»¶å†³ç­–ã€å¹¶è¡Œæ‰§è¡Œå’ŒæŒä¹…åŒ–çŠ¶æ€ç®¡ç†
- **Persistence & State Management**: å†…ç½® Checkpointerï¼Œåœ¨é”™è¯¯ã€ä¸­æ–­æˆ–ç³»ç»Ÿæ•…éšœæ—¶è‡ªåŠ¨ä¿å­˜å’Œæ¢å¤å·¥ä½œæµçŠ¶æ€
- **Low-level Control**: æä¾›ç²¾ç»†åŒ–æ§åˆ¶ï¼Œé€‚åˆéœ€è¦ç²¾ç¡®å®šä¹‰ Agent æ€è€ƒã€è¡ŒåŠ¨å’Œååº”é€»è¾‘çš„åœºæ™¯

#### Interrupt 2025 å¤§ä¼šé‡å¤§å‘å¸ƒ

**LangGraph Platform GA** (æ­£å¼å¯ç”¨):
- 1-click éƒ¨ç½²ï¼ˆCloudã€Hybridã€Self-hostedï¼‰
- é•¿æ—¶è¿è¡Œã€æœ‰çŠ¶æ€ Agent çš„éƒ¨ç½²å’Œç®¡ç†å¹³å°
- å†…ç½®ç›‘æ§å’Œå¯è§‚æµ‹æ€§

**LangGraph Studio v2**:
- å¯æœ¬åœ°è¿è¡Œï¼ˆæ— éœ€æ¡Œé¢åº”ç”¨ï¼‰
- æ”¯æŒæ‹‰å– Trace è¿›è¡Œè°ƒæŸ¥
- ç›´æ¥åœ¨ UI ä¸­æ›´æ–° Prompt
- æ·»åŠ è¯„ä¼°æ•°æ®é›†

**LangGraph Pre-Builts** (é¢„æ„å»ºæ¶æ„):
- æä¾›å¸¸è§æ¶æ„æ¨¡å¼ï¼šSwarmã€Supervisorã€Tool-calling Agent
- å¤§å¹…å‡å°‘é…ç½®ä»£ç 
- åŠ é€Ÿ Agent å¼€å‘

#### 2025 å¹´ 6 æœˆæ–°å¢åŠŸèƒ½

**æ— ä»£ç è¯„ä¼°**:
- ç›´æ¥åœ¨ LangGraph Studio ä¸­è¿è¡Œè¯„ä¼°ï¼Œæ— éœ€ç¼–å†™ä»£ç 

**èŠ‚ç‚¹ç¼“å­˜**:
- åŸºäºèŠ‚ç‚¹è¾“å…¥è‡ªåŠ¨ç¼“å­˜ç»“æœ
- é¿å…é‡å¤è®¡ç®—ï¼ŒåŠ é€Ÿæ‰§è¡Œ

**å»¶è¿Ÿæ‰§è¡Œ (Deferred Execution)**:
- èŠ‚ç‚¹å¯ç­‰å¾…æ‰€æœ‰å¹¶è¡Œåˆ†æ”¯å®Œæˆåå†æ‰§è¡Œ
- æ”¯æŒæ›´å¤æ‚çš„ç¼–æ’æµç¨‹

**MCP ç«¯ç‚¹**:
- æ¯ä¸ªéƒ¨ç½²çš„ Agent è‡ªåŠ¨æš´éœ² MCP ç«¯ç‚¹
- ç®€åŒ–å·¥å…·é›†æˆ

#### æ€§èƒ½å’Œé‡‡ç”¨æƒ…å†µ

- GitHub Stars: åŒæ¯”å¢é•¿ 220% (Q1 2024 â†’ Q1 2025)
- PyPI ä¸‹è½½é‡: å¢é•¿ 300%
- è¢«è¯„ä¸ºæœ€é€‚åˆ"ç»“æ„åŒ– Agent å·¥ä½œæµ"çš„æ¡†æ¶

#### æœ€ä½³å®è·µ

**é€‚ç”¨åœºæ™¯**:
- éœ€è¦å›¾çŠ¶æ€æ§åˆ¶æµçš„å¤æ‚ Agent
- å¤šæ­¥éª¤ã€æœ‰çŠ¶æ€çš„å·¥ä½œæµ
- éœ€è¦ç²¾ç»†åŒ–ç¼–æ’çš„ç”Ÿäº§ç¯å¢ƒ

**å­¦ä¹ æ›²çº¿**:
- ç”±äºæŠ½è±¡å±‚æ¬¡è¾ƒé«˜ï¼Œéœ€è¦ä¸€å®šå­¦ä¹ æˆæœ¬
- ä½†æä¾›äº†å¼ºå¤§çš„çµæ´»æ€§å’Œæ§åˆ¶åŠ›

---

### 2. RAG ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯

#### é«˜çº§ RAG å˜ä½“

2025 å¹´å‡ºç°äº†å¤šç§ä¸“é—¨åŒ–çš„ RAG æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸åŒåœºæ™¯ä¼˜åŒ–ï¼š

**Self-RAG** (è‡ªé€‚åº”æ£€ç´¢):
- éªŒè¯ä¿¡æ¯é›†æˆ
- ä»…åœ¨éœ€è¦æ—¶æ£€ç´¢æ•°æ®ï¼Œä¼˜åŒ–è®¡ç®—èµ„æº
- é€‚åˆå®æ—¶æ€§è¦æ±‚é«˜çš„åœºæ™¯

**Corrective RAG (CRAG)**:
- Decompose-then-Recompose ç®—æ³•
- å°†æ£€ç´¢æ–‡æ¡£åˆ†è§£ä¸ºå…³é”®æ´å¯Ÿï¼Œå†é‡ç»„ä¸ºè¿è´¯æ•°æ®é›†
- æé«˜æ£€ç´¢è´¨é‡å’Œç›¸å…³æ€§

**Long RAG**:
- å¤„ç†æ›´é•¿çš„æ£€ç´¢å•å…ƒï¼ˆç« èŠ‚æˆ–æ•´ä¸ªæ–‡æ¡£ï¼‰
- æ”¹å–„æ£€ç´¢æ•ˆç‡ï¼Œä¿æŒä¸Šä¸‹æ–‡å®Œæ•´æ€§
- é€‚åˆéœ€è¦ç†è§£é•¿æ–‡æ¡£çš„åœºæ™¯

**GraphRAG**:
- åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢
- æ”¯æŒå¤æ‚å…³ç³»æ¨ç†
- é€‚åˆç»“æ„åŒ–çŸ¥è¯†åœºæ™¯

#### æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯

**1. Adaptive Retrieval (è‡ªé€‚åº”æ£€ç´¢)**

åŠ¨æ€è°ƒæ•´æ£€ç´¢ç­–ç•¥ï¼ŒåŸºäºï¼š
- ç”¨æˆ·æ„å›¾åˆ†æ
- æŸ¥è¯¢å¤æ‚åº¦è¯„ä¼°
- å¼ºåŒ–å­¦ä¹ å®æ—¶ä¼˜åŒ–æ•°æ®æºé€‰æ‹©

**å®ç°æ€è·¯**:
```python
async def adaptive_retrieve(query: str, context: dict):
    # åˆ†ææŸ¥è¯¢å¤æ‚åº¦
    complexity = analyze_query_complexity(query)

    if complexity == "simple":
        # ç®€å•æŸ¥è¯¢ï¼šä»…å‘é‡æ£€ç´¢
        return await vector_search(query, top_k=5)
    elif complexity == "medium":
        # ä¸­ç­‰æŸ¥è¯¢ï¼šæ··åˆæ£€ç´¢
        return await hybrid_search(query, top_k=10)
    else:
        # å¤æ‚æŸ¥è¯¢ï¼šå¤šé˜¶æ®µæ£€ç´¢ + é‡æ’åº
        candidates = await hybrid_search(query, top_k=20)
        return await rerank(query, candidates, final_k=5)
```

**2. Query Augmentation (æŸ¥è¯¢å¢å¼º)**

åœ¨æ£€ç´¢å‰ä¿®æ”¹æˆ–æ‰©å±•ç”¨æˆ·æŸ¥è¯¢ï¼š
- æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
- æŸ¥è¯¢æ”¹å†™ï¼ˆQuery Rewritingï¼‰
- æŸ¥è¯¢æ‰©å±•ï¼ˆQuery Expansionï¼‰
- å¤šæŸ¥è¯¢ç”Ÿæˆï¼ˆMulti-Queryï¼‰

**å®ç°ç¤ºä¾‹**:
```python
async def augment_query(original_query: str, history: list):
    # ä½¿ç”¨ LLM ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“
    augmented_queries = await llm.generate([
        f"æ”¹å†™ä»¥ä¸‹æŸ¥è¯¢ä½¿å…¶æ›´æ¸…æ™°: {original_query}",
        f"ç”Ÿæˆ3ä¸ªç›¸å…³æŸ¥è¯¢å˜ä½“: {original_query}",
        f"åŸºäºå¯¹è¯å†å²ä¼˜åŒ–æŸ¥è¯¢: {history[-3:]} | {original_query}"
    ])

    # åˆå¹¶æ£€ç´¢ç»“æœ
    all_results = []
    for query in augmented_queries:
        results = await vector_search(query)
        all_results.extend(results)

    # å»é‡å¹¶æ’åº
    return deduplicate_and_rank(all_results)
```

**3. Two-Phase Retrieval with Reranking (ä¸¤é˜¶æ®µæ£€ç´¢ + é‡æ’åº)**

**ç¬¬ä¸€é˜¶æ®µ - å¬å› (Recall)**:
- å¿«é€Ÿæ£€ç´¢å¤§é‡å€™é€‰æ–‡æ¡£ï¼ˆTop-K=20-100ï¼‰
- ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼ˆå¦‚ BM25 + å‘é‡æ£€ç´¢ï¼‰
- ç›®æ ‡ï¼šé«˜å¬å›ç‡

**ç¬¬äºŒé˜¶æ®µ - é‡æ’åº (Rerank)**:
- ä½¿ç”¨ç²¾ç»†æ¨¡å‹é‡æ–°æ‰“åˆ†
- é€‰æ‹©æœ€ç›¸å…³çš„æ–‡æ¡£ï¼ˆTop-K=3-10ï¼‰
- ç›®æ ‡ï¼šé«˜ç²¾ç¡®ç‡

**æ¨èæ¨¡å‹**:
- **Cohere Rerank API** (å•†ä¸šï¼Œé«˜è´¨é‡)
- **BAAI/bge-reranker-large** (å¼€æºï¼Œä¸­æ–‡å‹å¥½)
- **cross-encoder/ms-marco-MiniLM-L-12-v2** (å¼€æºï¼Œè½»é‡)

**å®ç°ç¤ºä¾‹**:
```python
from sentence_transformers import CrossEncoder

class TwoPhaseRetriever:
    def __init__(self):
        self.reranker = CrossEncoder('BAAI/bge-reranker-large')

    async def retrieve(self, query: str, top_k=20, final_k=5):
        # é˜¶æ®µ1ï¼šå¿«é€Ÿå¬å›
        candidates = await self.hybrid_search(query, limit=top_k)

        # é˜¶æ®µ2ï¼šç²¾ç»†é‡æ’åº
        pairs = [[query, doc.text] for doc in candidates]
        scores = self.reranker.predict(pairs)

        # è¿”å› Top-K
        reranked = sorted(
            zip(candidates, scores),
            key=lambda x: x[1],
            reverse=True
        )
        return [doc for doc, score in reranked[:final_k]]
```

**4. Hybrid Search (æ··åˆæœç´¢)**

ç»“åˆå¤šç§æ£€ç´¢æŠ€æœ¯çš„ä¼˜åŠ¿ï¼š
- **å…³é”®è¯æœç´¢ (BM25)**: ç²¾ç¡®åŒ¹é…ã€æœ¯è¯­æŸ¥æ‰¾
- **è¯­ä¹‰æœç´¢ (Vector)**: ç†è§£è¯­ä¹‰ç›¸ä¼¼æ€§
- **çŸ¥è¯†å›¾è°±**: ç»“æ„åŒ–å…³ç³»æ¨ç†
- **å…ƒæ•°æ®è¿‡æ»¤**: æ—¶é—´ã€ç±»å‹ã€æ¥æºè¿‡æ»¤

**Qdrant å®ç°ç¤ºä¾‹**:
```python
from qdrant_client.models import Filter, FieldCondition, SearchRequest

async def hybrid_search(query: str, user_id: str, top_k=10):
    collection = f"user_{user_id}"

    # 1. å‘é‡æœç´¢
    query_vector = await embed_service.embed(query)
    vector_results = await qdrant.search(
        collection_name=collection,
        query_vector=query_vector,
        limit=top_k
    )

    # 2. å…³é”®è¯æœç´¢ï¼ˆé€šè¿‡ payload è¿‡æ»¤ï¼‰
    keyword_results = await qdrant.scroll(
        collection_name=collection,
        scroll_filter=Filter(
            must=[
                FieldCondition(
                    key="text",
                    match={"text": query}  # å…¨æ–‡æœç´¢
                )
            ]
        ),
        limit=top_k
    )

    # 3. ç»“æœèåˆï¼ˆReciprocal Rank Fusionï¼‰
    return reciprocal_rank_fusion([vector_results, keyword_results])

def reciprocal_rank_fusion(result_lists, k=60):
    """RRF èåˆç®—æ³•"""
    scores = {}
    for results in result_lists:
        for rank, doc in enumerate(results):
            doc_id = doc.id
            if doc_id not in scores:
                scores[doc_id] = 0
            scores[doc_id] += 1 / (k + rank + 1)

    # æŒ‰åˆ†æ•°æ’åº
    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return [doc_id for doc_id, score in sorted_docs]
```

#### ç ”ç©¶çƒ­ç‚¹

**Chunk Size ä¼˜åŒ–**:
- ç ”ç©¶è¡¨æ˜ï¼šæœ€ä½³ chunk size å–å†³äºæ–‡æ¡£ç±»å‹å’Œä»»åŠ¡
- å»ºè®®ï¼šåŠ¨æ€ chunkingï¼ˆæ ¹æ®å†…å®¹è¯­ä¹‰åˆ†å—ï¼‰

**Multilingual RAG**:
- è·¨è¯­è¨€æ£€ç´¢å’Œç”Ÿæˆ
- ä½¿ç”¨å¤šè¯­è¨€ Embedding æ¨¡å‹ï¼ˆå¦‚ mE5, multilingual-e5-largeï¼‰

**Multimodal RAG**:
- æ”¯æŒå›¾åƒã€è¡¨æ ¼ã€å›¾è¡¨æ£€ç´¢
- ä½¿ç”¨ CLIP ç­‰å¤šæ¨¡æ€æ¨¡å‹

**Real-time Retrieval**:
- å®æ—¶ç´¢å¼•æ›´æ–°
- å¢é‡å‘é‡åŒ–
- æµå¼æ£€ç´¢

#### æœªæ¥è¶‹åŠ¿

- **Hybrid Search**: æ··åˆæ£€ç´¢æˆä¸ºæ ‡é…
- **Multimodal RAG**: å¤šæ¨¡æ€æ”¯æŒæ™®åŠ
- **Adaptive Intelligence**: è‡ªæ”¹è¿›çš„ RAG ç³»ç»Ÿï¼ˆåŸºäºç”¨æˆ·åé¦ˆï¼‰

---

### 3. FastAPI å¯è§‚æµ‹æ€§æœ€ä½³å®è·µ

#### ä¸‰å¤§æ”¯æŸ±æ¡†æ¶

2025 å¹´å¯è§‚æµ‹æ€§çš„é»„é‡‘æ ‡å‡†æ˜¯ **Metrics + Logs + Traces**ï¼š

**Metrics (æŒ‡æ ‡)**:
- ç”¨é€”ï¼šç³»ç»Ÿå¥åº·æ¦‚è§ˆ
- å·¥å…·ï¼šPrometheus
- ç¤ºä¾‹ï¼šè¯·æ±‚ç‡ã€å»¶è¿Ÿã€é”™è¯¯ç‡

**Logs (æ—¥å¿—)**:
- ç”¨é€”ï¼šè¯¦ç»†äº‹ä»¶ä¿¡æ¯
- å·¥å…·ï¼šLoki
- ç¤ºä¾‹ï¼šé”™è¯¯æ ˆã€ç”¨æˆ·è¡Œä¸º

**Traces (è¿½è¸ª)**:
- ç”¨é€”ï¼šè¯·æ±‚ç”Ÿå‘½å‘¨æœŸåˆ†æ
- å·¥å…·ï¼šTempo / Jaeger
- ç¤ºä¾‹ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿã€æœåŠ¡ä¾èµ–

#### æ¨èå·¥å…·åº“

**prometheus-fastapi-instrumentator**:
- æœ€æµè¡Œçš„ FastAPI ç›‘æ§åº“ï¼ˆ2000+ starsï¼‰
- è‡ªåŠ¨æ·»åŠ  Prometheus æŒ‡æ ‡
- ä¸€é”®æš´éœ² `/metrics` ç«¯ç‚¹

**å®‰è£…å’Œé…ç½®**:
```bash
pip install prometheus-fastapi-instrumentator
```

```python
from prometheus_fastapi_instrumentator import Instrumentator
from fastapi import FastAPI

app = FastAPI()

# ä¸€è¡Œä»£ç é›†æˆ
Instrumentator().instrument(app).expose(app)
```

**è‡ªåŠ¨æš´éœ²çš„æŒ‡æ ‡**:
- `http_requests_total`: æ€»è¯·æ±‚æ•°ï¼ˆæŒ‰ methodã€endpointã€statusï¼‰
- `http_request_duration_seconds`: è¯·æ±‚å»¶è¿Ÿï¼ˆP50/P95/P99ï¼‰
- `http_requests_inprogress`: å½“å‰è¿›è¡Œä¸­çš„è¯·æ±‚

#### OpenTelemetry é›†æˆ

**å®Œæ•´å¯è§‚æµ‹æ€§**:
```bash
pip install opentelemetry-api opentelemetry-sdk
pip install opentelemetry-instrumentation-fastapi
pip install opentelemetry-exporter-otlp
```

```python
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# é…ç½® Tracer
provider = TracerProvider()
processor = BatchSpanProcessor(OTLPSpanExporter(endpoint="http://tempo:4317"))
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

# è‡ªåŠ¨æ³¨å…¥ Trace
app = FastAPI()
FastAPIInstrumentor.instrument_app(app)
```

#### å…³é”®æŒ‡æ ‡å®šä¹‰

**HTTP å±‚**:
```
http_requests_total{method="POST", endpoint="/api/conversation/send", status="200"}
http_request_duration_seconds{method="POST", endpoint="/api/conversation/send"}
http_requests_inprogress
```

**LLM å±‚**:
```
llm_calls_total{model="gpt-4", status="success"}
llm_call_duration_seconds{model="gpt-4"}
llm_tokens_used_total{model="gpt-4", type="prompt"}
llm_tokens_used_total{model="gpt-4", type="completion"}
```

**å·¥å…·å±‚**:
```
tool_calls_total{tool="search", status="success"}
tool_execution_duration_seconds{tool="search"}
```

**RAG å±‚**:
```
rag_retrievals_total{collection="user_123"}
rag_retrieval_duration_seconds{collection="user_123"}
rag_documents_retrieved{collection="user_123"}
```

**ä¼šè¯å±‚**:
```
session_cache_hits_total
session_db_operations_total{operation="load"}
active_sessions
```

#### Docker Compose å®Œæ•´æ ˆ

```yaml
version: '3.8'

services:
  # åº”ç”¨æœåŠ¡
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4317

  # Metrics - Prometheus
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'

  # Logs - Loki
  loki:
    image: grafana/loki:latest
    ports:
      - "3100:3100"
    volumes:
      - ./config/loki-config.yml:/etc/loki/local-config.yaml
      - loki_data:/loki

  # Traces - Tempo
  tempo:
    image: grafana/tempo:latest
    ports:
      - "4317:4317"  # OTLP gRPC
      - "3200:3200"  # Tempo HTTP
    volumes:
      - ./config/tempo-config.yml:/etc/tempo.yaml
      - tempo_data:/tmp/tempo
    command: [ "-config.file=/etc/tempo.yaml" ]

  # Visualization - Grafana
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./config/grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana_data:/var/lib/grafana

volumes:
  prometheus_data:
  loki_data:
  tempo_data:
  grafana_data:
```

#### Grafana ä»ªè¡¨æ¿

**é¢„æ„å»ºä»ªè¡¨æ¿**:
- [FastAPI Observability Dashboard](https://grafana.com/grafana/dashboards/16110)ï¼ˆå®˜æ–¹æ¨èï¼‰
- åŒ…å«ï¼šè¯·æ±‚ç‡ã€å»¶è¿Ÿåˆ†å¸ƒã€é”™è¯¯ç‡ã€ååé‡

**å…³é”®å¯è§†åŒ–**:
- **è¯·æ±‚ç‡é¢æ¿**: `rate(http_requests_total[5m])`
- **å»¶è¿Ÿåˆ†å¸ƒ**: `histogram_quantile(0.95, http_request_duration_seconds)`
- **é”™è¯¯ç‡**: `rate(http_requests_total{status=~"5.."}[5m])`
- **LLM Token ç”¨é‡**: `sum(llm_tokens_used_total) by (model, type)`

#### å‘Šè­¦è§„åˆ™

```yaml
# Prometheus å‘Šè­¦é…ç½®
groups:
  - name: api_alerts
    interval: 30s
    rules:
      # LLM è°ƒç”¨å»¶è¿Ÿè¿‡é«˜
      - alert: HighLLMLatency
        expr: histogram_quantile(0.95, llm_call_duration_seconds) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LLM è°ƒç”¨ P95 å»¶è¿Ÿè¶…è¿‡ 2 ç§’"

      # å·¥å…·æ‰§è¡Œå¤±è´¥ç‡è¿‡é«˜
      - alert: HighToolFailureRate
        expr: rate(tool_calls_total{status="error"}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "å·¥å…·æ‰§è¡Œå¤±è´¥ç‡è¶…è¿‡ 10%"

      # æ•°æ®åº“è¿æ¥æ± è€—å°½
      - alert: DatabaseConnectionPoolExhausted
        expr: db_connections_in_use / db_connections_max > 0.9
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "æ•°æ®åº“è¿æ¥æ± ä½¿ç”¨è¶…è¿‡ 90%"
```

#### å‚è€ƒé¡¹ç›®

**fastapi-observability** ([GitHub](https://github.com/blueswen/fastapi-observability)):
- å®Œæ•´çš„ FastAPI + Prometheus + Grafana + Loki + Tempo ç¤ºä¾‹
- 1500+ starsï¼Œç”Ÿäº§çº§é…ç½®
- åŒ…å« Docker Compose å’Œä»ªè¡¨æ¿æ¨¡æ¿

---

### 4. å®æ—¶è¯­éŸ³æŠ€æœ¯

#### æ¶æ„æ¨¡å¼æ¼”è¿›

2025 å¹´å®æ—¶è¯­éŸ³ç³»ç»Ÿæœ‰ä¸‰ç§ä¸»æµæ¶æ„ï¼š

**1. Turn-Based (çº§è”å¼) - å½“å‰é¡¹ç›®ä½¿ç”¨**
```
Voice â†’ STT â†’ Text â†’ LLM â†’ Text â†’ TTS â†’ Voice
```
- **å»¶è¿Ÿ**: ~1000ms
- **ä¼˜åŠ¿**: æ¨¡å—åŒ–ï¼Œæ˜“äºè°ƒè¯•å’Œæ›¿æ¢
- **åŠ£åŠ¿**: å»¶è¿Ÿè¾ƒé«˜ï¼Œæ— æ³•å®ç°çœŸæ­£çš„å®æ—¶å¯¹è¯

**2. Streaming (æµå¼)**
```
Voice Stream â†’ STT Stream â†’ LLM Stream â†’ TTS Stream â†’ Voice Stream
```
- **å»¶è¿Ÿ**: ~500ms
- **ä¼˜åŠ¿**: é™ä½æ„ŸçŸ¥å»¶è¿Ÿï¼Œæ”¯æŒéƒ¨åˆ†å¹¶å‘
- **åŠ£åŠ¿**: éœ€è¦æ‰€æœ‰ç»„ä»¶æ”¯æŒæµå¼å¤„ç†

**3. Speech-to-Speech (ç«¯åˆ°ç«¯)**
```
Voice â†’ Unified Model â†’ Voice
```
- **å»¶è¿Ÿ**: <300ms
- **ä¼˜åŠ¿**: æœ€ä½å»¶è¿Ÿï¼Œä¿ç•™æƒ…æ„Ÿå’Œè¯­è°ƒ
- **åŠ£åŠ¿**: æ¨¡å‹é€‰æ‹©æœ‰é™ï¼Œå®šåˆ¶æ€§è¾ƒå·®

#### ä½å»¶è¿Ÿå¼€æºæ–¹æ¡ˆ

**RealtimeSTT** ([GitHub](https://github.com/KoljaB/RealtimeSTT)):
- å®æ—¶è¯­éŸ³è½¬æ–‡å­—ï¼Œæ”¯æŒ VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰
- åŸºäº Whisperï¼Œæ”¯æŒå¤šè¯­è¨€
- ç‰¹æ€§ï¼š
  - æµå¼è½¬å½•
  - å”¤é†’è¯æ£€æµ‹
  - å³æ—¶è½¬å½•ï¼ˆæ— ç¼“å†²ï¼‰
  - å»¶è¿Ÿ <150ms

**å®‰è£…å’Œä½¿ç”¨**:
```bash
pip install RealtimeSTT
```

```python
from RealtimeSTT import AudioToTextRecorder

def on_transcription(text):
    print(f"è¯†åˆ«åˆ°: {text}")

recorder = AudioToTextRecorder(
    model="large-v2",  # Whisper æ¨¡å‹
    language="zh",
    enable_realtime_transcription=True,
    on_recording_stop=on_transcription
)

recorder.start()
```

**RealtimeTTS** ([GitHub](https://github.com/KoljaB/RealtimeTTS)):
- å®æ—¶æ–‡å­—è½¬è¯­éŸ³
- æ”¯æŒå¤šç§ TTS å¼•æ“ï¼ˆAzure, ElevenLabs, Coquiï¼‰
- æµå¼è¾“å‡ºï¼Œå»¶è¿Ÿ <100ms

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from RealtimeTTS import TextToAudioStream, SystemEngine, ElevenlabsEngine

# ä½¿ç”¨ç³»ç»Ÿ TTSï¼ˆå…è´¹ï¼Œå»¶è¿Ÿä½ï¼‰
engine = SystemEngine()
stream = TextToAudioStream(engine)

# æµå¼æ’­æ”¾
stream.feed("è¿™æ˜¯å®æ—¶è¯­éŸ³è¾“å‡ºæµ‹è¯•ã€‚").play()

# æˆ–è€…å¼‚æ­¥æ’­æ”¾
stream.feed("å¼‚æ­¥æ’­æ”¾ç¤ºä¾‹").play_async()
```

#### å•†ä¸šæ–¹æ¡ˆ

**GPT-4o Realtime API** (OpenAI):
- çœŸæ­£çš„ Speech-to-Speech
- å»¶è¿Ÿ <300ms
- æ”¯æŒæ‰“æ–­ã€æƒ…æ„Ÿä¿ç•™
- ä»·æ ¼ï¼šè¾“å…¥ $100/1M tokensï¼Œè¾“å‡º $200/1M tokens

**Gemini 2.5 Flash Live** (Google):
- è¶…ä½å»¶è¿Ÿï¼ˆ<200msï¼‰
- å¤šæ¨¡æ€è¾“å…¥ï¼ˆè¯­éŸ³+å›¾åƒï¼‰
- ç›®å‰å¤„äº Preview é˜¶æ®µ

**Kyutai Moshi** (å¼€æº):
- å®Œå…¨å¼€æºçš„ Speech-to-Speech æ¨¡å‹
- å»¶è¿Ÿ <150ms
- æ”¯æŒæœ¬åœ°éƒ¨ç½²ï¼ˆéœ€è¦é«˜æ€§èƒ½ GPUï¼‰

#### æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | å»¶è¿Ÿ | æˆæœ¬ | éƒ¨ç½² | è´¨é‡ |
|------|------|------|------|------|
| ç§‘å¤§è®¯é£ (å½“å‰) | ~500ms | ä¸­ | äº‘ç«¯ | é«˜ |
| RealtimeSTT/TTS | <150ms | ä½ | æœ¬åœ° | ä¸­ |
| GPT-4o Realtime | <300ms | é«˜ | äº‘ç«¯ | æé«˜ |
| Gemini 2.5 Flash | <200ms | ä¸­ | äº‘ç«¯ | é«˜ |
| Kyutai Moshi | <150ms | ä½ | æœ¬åœ° | ä¸­ |

#### ä¼˜åŒ–æŠ€æœ¯

**1. VAD (Voice Activity Detection)**:
- æ£€æµ‹è¯­éŸ³æ´»åŠ¨ï¼Œå‡å°‘æ— æ•ˆå¤„ç†
- æ”¯æŒæ‰“æ–­æ£€æµ‹

**2. Dual Streaming**:
- STT å’Œ TTS åŒæ—¶æµå¼å¤„ç†
- æ–‡æœ¬é€è¯/é€å­—ç¬¦ä¼ é€’

**3. Edge Inference**:
- åœ¨è®¾å¤‡ç«¯æˆ–è¾¹ç¼˜æœåŠ¡å™¨è¿è¡Œæ¨¡å‹
- æ¶ˆé™¤ç½‘ç»œå»¶è¿Ÿ

**4. Model Optimization**:
- ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼ˆFastSpeechã€Glow-TTSï¼‰
- æ¨¡å‹é‡åŒ–ï¼ˆINT8ã€FP16ï¼‰

---

### 5. å¤šAgentåä½œæ¡†æ¶

#### 2025 å¹´é¡¶çº§æ¡†æ¶

**1. CrewAI** (æ¨èï¼Œç”Ÿäº§çº§)

**ç‰¹ç‚¹**:
- ä¸“æ³¨äºå¤š Agent åä½œ
- è§’è‰²å®šä¹‰æ¸…æ™°ï¼ˆRole-Basedï¼‰
- ä»»åŠ¡åˆ†é…å’Œç»“æœèšåˆè‡ªåŠ¨åŒ–
- ç”Ÿäº§å°±ç»ª

**å®‰è£…å’Œä½¿ç”¨**:
```bash
pip install crewai crewai-tools
```

```python
from crewai import Agent, Task, Crew
from crewai_tools import SerperDevTool, WebsiteSearchTool

# å®šä¹‰å·¥å…·
search_tool = SerperDevTool()

# å®šä¹‰ Agents
researcher = Agent(
    role='ç ”ç©¶å‘˜',
    goal='æ”¶é›†æœ€æ–° AI æŠ€æœ¯ä¿¡æ¯',
    backstory='èµ„æ·±æŠ€æœ¯ç ”ç©¶å‘˜ï¼Œæ“…é•¿ä¿¡æ¯æ”¶é›†å’Œåˆ†æ',
    tools=[search_tool],
    verbose=True
)

writer = Agent(
    role='æŠ€æœ¯ä½œå®¶',
    goal='æ’°å†™é«˜è´¨é‡æŠ€æœ¯æ–‡æ¡£',
    backstory='æŠ€æœ¯å†™ä½œä¸“å®¶ï¼Œæ“…é•¿å°†å¤æ‚æ¦‚å¿µç®€åŒ–',
    verbose=True
)

reviewer = Agent(
    role='è´¨é‡è¯„å®¡å‘˜',
    goal='ç¡®ä¿å†…å®¹å‡†ç¡®æ€§å’Œè´¨é‡',
    backstory='ä¸¥è°¨çš„æŠ€æœ¯å®¡æ ¸ä¸“å®¶',
    verbose=True
)

# å®šä¹‰ä»»åŠ¡
research_task = Task(
    description='ç ”ç©¶ 2025 å¹´ AI Agent æœ€æ–°è¿›å±•',
    agent=researcher,
    expected_output='è¯¦ç»†çš„ç ”ç©¶æŠ¥å‘Š'
)

write_task = Task(
    description='åŸºäºç ”ç©¶ç»“æœæ’°å†™æŠ€æœ¯åšå®¢',
    agent=writer,
    expected_output='2000å­—æŠ€æœ¯åšå®¢'
)

review_task = Task(
    description='å®¡æŸ¥åšå®¢å†…å®¹å¹¶æå‡ºæ”¹è¿›å»ºè®®',
    agent=reviewer,
    expected_output='å®¡æ ¸æ„è§å’Œæœ€ç»ˆç‰ˆæœ¬'
)

# åˆ›å»º Crew
crew = Crew(
    agents=[researcher, writer, reviewer],
    tasks=[research_task, write_task, review_task],
    verbose=True
)

# æ‰§è¡Œ
result = crew.kickoff()
print(result)
```

**ä¼˜åŠ¿**:
- å­¦ä¹ æ›²çº¿å¹³ç¼“
- æ–‡æ¡£å®Œå–„
- ç¤¾åŒºæ´»è·ƒ

**2. AutoGen** (Microsoft)

**ç‰¹ç‚¹**:
- çµæ´»çš„ Agent äº¤äº’æ¨¡å¼
- æ”¯æŒäººç±»å‚ä¸ï¼ˆHuman-in-the-loopï¼‰
- é€‚åˆç ”ç©¶å’ŒåŸå‹å¼€å‘

**ç¤ºä¾‹**:
```python
import autogen

config_list = [{"model": "gpt-4", "api_key": "..."}]

assistant = autogen.AssistantAgent(
    name="assistant",
    llm_config={"config_list": config_list}
)

user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    code_execution_config={"work_dir": "coding"}
)

user_proxy.initiate_chat(
    assistant,
    message="å¸®æˆ‘åˆ†æè¿™ä¸ªæ•°æ®é›†å¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"
)
```

**3. Microsoft Agent Framework** (ä¼ä¸šçº§)

**ç‰¹ç‚¹** (2025 å¹´å…¬å¼€é¢„è§ˆ):
- ä¼ä¸šçº§åŠŸèƒ½ï¼ˆå¯è§‚æµ‹æ€§ã€æŒä¹…åŒ–ã€åˆè§„ï¼‰
- ç»Ÿä¸€æ¡†æ¶ï¼Œæ•´åˆå‰æ²¿ç ”ç©¶
- å†…ç½®å¤š Agent ç¼–æ’
- é€‚åˆå¤§å‹ä¼ä¸šéƒ¨ç½²

**4. Google ADK (Agent Development Kit)**

**ç‰¹ç‚¹** (2025 å¹´ Google Cloud NEXT å‘å¸ƒ):
- å¼€æºæ¡†æ¶
- ç®€åŒ–ç«¯åˆ°ç«¯ Agent å¼€å‘
- æ”¯æŒå¤š Agent ç³»ç»Ÿ
- ä¸ Google Cloud æ·±åº¦é›†æˆ

#### æ ¸å¿ƒç¼–æ’æ¨¡å¼

**1. Sequential (é¡ºåºæ‰§è¡Œ)**:
```
Agent A â†’ Agent B â†’ Agent C â†’ Result
```
- é€‚åˆï¼šçº¿æ€§å·¥ä½œæµ
- ç¤ºä¾‹ï¼šç ”ç©¶ â†’ æ’°å†™ â†’ å®¡æ ¸

**2. Concurrent (å¹¶å‘æ‰§è¡Œ)**:
```
       â”Œâ”€ Agent A â”€â”
Task â†’ â”œâ”€ Agent B â”€â”¤ â†’ Aggregator â†’ Result
       â””â”€ Agent C â”€â”˜
```
- é€‚åˆï¼šç‹¬ç«‹å­ä»»åŠ¡
- ç¤ºä¾‹ï¼šå¤šæºä¿¡æ¯æ”¶é›†

**3. Hierarchical (å±‚çº§ç¼–æ’)**:
```
Supervisor Agent
    â”œâ”€ Worker Agent 1
    â”œâ”€ Worker Agent 2
    â””â”€ Worker Agent 3
```
- é€‚åˆï¼šå¤æ‚ä»»åŠ¡åˆ†è§£
- ç¤ºä¾‹ï¼šé¡¹ç›®ç®¡ç†

**4. Debate/Voting (è¾©è®º/æŠ•ç¥¨)**:
```
Task â†’ [Agent 1, Agent 2, Agent 3] â†’ Vote â†’ Best Result
```
- é€‚åˆï¼šéœ€è¦å¤šè§’åº¦è¯„ä¼°
- ç¤ºä¾‹ï¼šæ–¹æ¡ˆé€‰æ‹©

#### å…³é”®ç‰¹æ€§

**State Management (çŠ¶æ€ç®¡ç†)**:
- è·¨ Agent å…±äº«çŠ¶æ€
- æŒä¹…åŒ–ä¼šè¯ä¸Šä¸‹æ–‡
- ä¸­é—´ç»“æœä¼ é€’

**Communication Protocols (é€šä¿¡åè®®)**:
- ç»“æ„åŒ–æ¶ˆæ¯ä¼ é€’
- äº‹ä»¶é©±åŠ¨é€šçŸ¥
- Agent é—´æ¡æ‰‹åè®®

**Tool Sharing (å·¥å…·å…±äº«)**:
- Agent ä¹‹é—´å…±äº«å·¥å…·é›†
- å·¥å…·è°ƒç”¨åè°ƒ
- ç»“æœç¼“å­˜

---

## è¿‘æœŸå¯ç«‹å³åº”ç”¨çš„æŠ€æœ¯å‡çº§

### 1. LangGraph Pre-Builts + èŠ‚ç‚¹ç¼“å­˜

**ä»·å€¼**: ç®€åŒ–ä»£ç ï¼Œæå‡æ€§èƒ½
**éš¾åº¦**: ä½
**æ—¶é—´**: 1-2 å¤©

**å®æ–½æ­¥éª¤**:

1. **å‡çº§ LangGraph**:
```bash
pip install --upgrade langgraph
```

2. **ä½¿ç”¨ Pre-Builts ç®€åŒ– Agent æ„å»º**:
```python
# å½“å‰ä»£ç ï¼ˆå¤æ‚ï¼‰
from langgraph.graph import StateGraph
workflow = StateGraph(AgentState)
workflow.add_node("llm", nodes.call_llm)
workflow.add_node("tools", nodes.use_tools)
# ... æ›´å¤šé…ç½®

# æ–°æ–¹æ³•ï¼ˆç®€åŒ–ï¼‰
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model=llm,
    tools=tools,
    checkpointer=checkpointer
)
```

3. **æ·»åŠ èŠ‚ç‚¹ç¼“å­˜**:
```python
from langgraph.graph import StateGraph
from langgraph.checkpoint import MemorySaver

# ä¸º RAG æ£€ç´¢èŠ‚ç‚¹æ·»åŠ ç¼“å­˜
@node(cache=True)  # åŸºäºè¾“å…¥è‡ªåŠ¨ç¼“å­˜
async def rag_retrieve(state: AgentState):
    query = state["messages"][-1].content
    docs = await rag_service.search(query, user_id=state["user_id"])
    return {"documents": docs}

workflow = StateGraph(AgentState)
workflow.add_node("rag", rag_retrieve)
```

**é¢„æœŸæ”¶ç›Š**:
- ä»£ç é‡å‡å°‘ 30-40%
- RAG æ£€ç´¢ç¼“å­˜å‘½ä¸­ç‡ ~50%ï¼ˆèŠ‚çœ API è°ƒç”¨ï¼‰
- å“åº”é€Ÿåº¦æå‡ 20-30%

---

### 2. Prometheus FastAPI Instrumentator

**ä»·å€¼**: å¿«é€Ÿå»ºç«‹ç›‘æ§
**éš¾åº¦**: ä½
**æ—¶é—´**: 1 å°æ—¶

**å®æ–½æ­¥éª¤**:

1. **å®‰è£…**:
```bash
pip install prometheus-fastapi-instrumentator
```

2. **é›†æˆåˆ° main.py**:
```python
from fastapi import FastAPI
from prometheus_fastapi_instrumentator import Instrumentator

app = FastAPI()

# ä¸€è¡Œä»£ç å¯ç”¨ç›‘æ§
Instrumentator().instrument(app).expose(app)

# ç°åœ¨è®¿é—® http://localhost:8000/metrics æŸ¥çœ‹æŒ‡æ ‡
```

3. **è‡ªå®šä¹‰æŒ‡æ ‡**:
```python
from prometheus_client import Counter, Histogram

# è‡ªå®šä¹‰ LLM è°ƒç”¨æŒ‡æ ‡
llm_calls = Counter(
    'llm_calls_total',
    'Total LLM calls',
    ['model', 'status']
)

llm_latency = Histogram(
    'llm_call_duration_seconds',
    'LLM call duration',
    ['model']
)

# åœ¨ä»£ç ä¸­ä½¿ç”¨
@llm_latency.labels(model="gpt-4").time()
async def call_llm(prompt):
    result = await llm.ainvoke(prompt)
    llm_calls.labels(model="gpt-4", status="success").inc()
    return result
```

4. **Prometheus é…ç½®**:
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'fastapi'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:8000']
```

**é¢„æœŸæ”¶ç›Š**:
- ç«‹å³å¯è§†åŒ–ç³»ç»ŸæŒ‡æ ‡
- å»ºç«‹æ€§èƒ½åŸºçº¿
- ä¸ºå‘Šè­¦ç³»ç»Ÿæ‰“åŸºç¡€

---

### 3. RAG Hybrid Search

**ä»·å€¼**: æå‡æ£€ç´¢è´¨é‡
**éš¾åº¦**: ä¸­
**æ—¶é—´**: 1 å‘¨

**å®æ–½æ­¥éª¤**:

1. **åœ¨ Qdrant ä¸­å¯ç”¨å…¨æ–‡æœç´¢**:
```python
from qdrant_client.models import Distance, VectorParams, TextIndexParams

# åˆ›å»ºé›†åˆæ—¶é…ç½®å…¨æ–‡ç´¢å¼•
await qdrant.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
    # å¯ç”¨å…¨æ–‡æœç´¢
    text_index_params=TextIndexParams(
        type="text",
        tokenizer="word",
        min_token_len=2,
        max_token_len=20
    )
)
```

2. **å®ç°æ··åˆæ£€ç´¢**:
```python
from qdrant_client.models import Filter, FieldCondition, SearchRequest

async def hybrid_search(
    query: str,
    user_id: str,
    top_k=10,
    vector_weight=0.7,
    keyword_weight=0.3
):
    collection = f"user_{user_id}"

    # å‘é‡æ£€ç´¢
    query_vector = await embedding_service.embed(query)
    vector_results = await qdrant.search(
        collection_name=collection,
        query_vector=query_vector,
        limit=top_k * 2  # æ£€ç´¢æ›´å¤šå€™é€‰
    )

    # å…³é”®è¯æ£€ç´¢
    keyword_results = await qdrant.scroll(
        collection_name=collection,
        scroll_filter=Filter(
            must=[
                FieldCondition(
                    key="text",
                    match={"text": query}
                )
            ]
        ),
        limit=top_k * 2
    )

    # èåˆç»“æœï¼ˆRRFï¼‰
    return reciprocal_rank_fusion(
        [vector_results, keyword_results],
        weights=[vector_weight, keyword_weight],
        top_k=top_k
    )
```

3. **RRF ç®—æ³•å®ç°**:
```python
def reciprocal_rank_fusion(
    result_lists: list,
    weights: list = None,
    k=60,
    top_k=10
):
    """Reciprocal Rank Fusion ç®—æ³•"""
    if weights is None:
        weights = [1.0] * len(result_lists)

    scores = {}
    for weight, results in zip(weights, result_lists):
        for rank, doc in enumerate(results):
            doc_id = doc.id
            if doc_id not in scores:
                scores[doc_id] = {"score": 0, "doc": doc}
            scores[doc_id]["score"] += weight / (k + rank + 1)

    # æ’åºå¹¶è¿”å›
    sorted_docs = sorted(
        scores.values(),
        key=lambda x: x["score"],
        reverse=True
    )
    return [item["doc"] for item in sorted_docs[:top_k]]
```

**é¢„æœŸæ”¶ç›Š**:
- æ£€ç´¢å‡†ç¡®ç‡æå‡ 15-25%
- æ›´å¥½å¤„ç†ç²¾ç¡®æŸ¥è¯¢å’Œè¯­ä¹‰æŸ¥è¯¢
- é™ä½ RAG å¹»è§‰

---

## ä¸­æœŸå¯æ¢ç´¢çš„æ–°æŠ€æœ¯

### 4. Two-Phase Retrieval + Reranking

**ä»·å€¼**: å¤§å¹…æå‡æ£€ç´¢ç²¾åº¦
**éš¾åº¦**: ä¸­
**æ—¶é—´**: 1-2 å‘¨

**å®æ–½æ­¥éª¤**:

1. **å®‰è£… Reranker æ¨¡å‹**:
```bash
pip install sentence-transformers
```

2. **å®ç°ä¸¤é˜¶æ®µæ£€ç´¢**:
```python
from sentence_transformers import CrossEncoder

class TwoPhaseRAGService:
    def __init__(self):
        # è½»é‡çº§å‘é‡æ¨¡å‹ï¼ˆå¬å›ï¼‰
        self.embedder = SentenceTransformer('moka-ai/m3e-base')

        # é‡æ’åºæ¨¡å‹ï¼ˆç²¾æ’ï¼‰
        self.reranker = CrossEncoder('BAAI/bge-reranker-large')

    async def retrieve(
        self,
        query: str,
        user_id: str,
        recall_k=20,
        final_k=5
    ):
        # é˜¶æ®µ1ï¼šå¿«é€Ÿå¬å›
        candidates = await self.hybrid_search(
            query,
            user_id,
            top_k=recall_k
        )

        # é˜¶æ®µ2ï¼šç²¾ç»†é‡æ’åº
        if len(candidates) == 0:
            return []

        pairs = [[query, doc.payload["text"]] for doc in candidates]
        scores = self.reranker.predict(pairs)

        # ç»„åˆå¹¶æ’åº
        reranked = sorted(
            zip(candidates, scores),
            key=lambda x: x[1],
            reverse=True
        )

        return [doc for doc, score in reranked[:final_k]]
```

3. **é›†æˆåˆ° Agent**:
```python
# agent/nodes.py
async def retrieve_context(state: AgentState):
    query = state["messages"][-1].content

    # ä½¿ç”¨ä¸¤é˜¶æ®µæ£€ç´¢
    docs = await two_phase_rag.retrieve(
        query=query,
        user_id=state["user_id"],
        recall_k=20,
        final_k=5
    )

    return {"documents": docs}
```

**æ¨èæ¨¡å‹**:
- **BAAI/bge-reranker-large**: ä¸­æ–‡å‹å¥½ï¼Œå‡†ç¡®ç‡é«˜
- **BAAI/bge-reranker-base**: è½»é‡çº§ï¼Œé€Ÿåº¦å¿«
- **maidalun1020/bce-reranker-base_v1**: ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–

**é¢„æœŸæ”¶ç›Š**:
- æ£€ç´¢ç²¾åº¦æå‡ 25-40%
- Top-5 å‡†ç¡®ç‡æ˜¾è‘—æé«˜
- ç”Ÿæˆè´¨é‡æ”¹å–„

---

### 5. å®Œæ•´å¯è§‚æµ‹æ€§æ ˆï¼ˆPrometheus + Loki + Tempo + Grafanaï¼‰

**ä»·å€¼**: å…¨æ–¹ä½ç³»ç»Ÿç›‘æ§
**éš¾åº¦**: ä¸­
**æ—¶é—´**: 2-3 å‘¨

**å®æ–½æ­¥éª¤**:

1. **åˆ›å»º docker-compose.observability.yml**ï¼ˆè§ä¸Šæ–‡ "3. FastAPI å¯è§‚æµ‹æ€§æœ€ä½³å®è·µ" éƒ¨åˆ†ï¼‰

2. **é…ç½® Prometheus**:
```yaml
# config/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'fastapi'
    static_configs:
      - targets: ['api:8000']
```

3. **é…ç½® Loki**:
```yaml
# config/loki-config.yml
auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1

schema_config:
  configs:
    - from: 2023-01-01
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/boltdb-shipper-active
    cache_location: /loki/boltdb-shipper-cache
  filesystem:
    directory: /loki/chunks

limits_config:
  reject_old_samples: true
  reject_old_samples_max_age: 168h
```

4. **é…ç½® Tempo**:
```yaml
# config/tempo-config.yml
server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
        http:

storage:
  trace:
    backend: local
    local:
      path: /tmp/tempo/traces

query_frontend:
  search:
    max_duration: 0s
```

5. **é…ç½® Grafana æ•°æ®æº**:
```yaml
# config/grafana/datasources.yml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true

  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100

  - name: Tempo
    type: tempo
    access: proxy
    url: http://tempo:3200
```

6. **å¯åŠ¨å®Œæ•´æ ˆ**:
```bash
docker-compose -f docker-compose.yml -f docker-compose.observability.yml up -d
```

7. **è®¿é—® Grafana**:
- URL: http://localhost:3000
- ç”¨æˆ·å: admin
- å¯†ç : admin

**é¢„æœŸæ”¶ç›Š**:
- å®Œæ•´çš„å¯è§‚æµ‹æ€§èƒ½åŠ›
- å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆ
- æ”¯æŒç”Ÿäº§ç¯å¢ƒç›‘æ§

---

### 6. RealtimeSTT/TTS è¯„ä¼°

**ä»·å€¼**: é™ä½è¯­éŸ³å»¶è¿Ÿï¼Œæœ¬åœ°éƒ¨ç½²
**éš¾åº¦**: ä¸­
**æ—¶é—´**: 1-2 å‘¨ï¼ˆPOCï¼‰

**å®æ–½æ­¥éª¤**:

1. **å®‰è£…**:
```bash
pip install RealtimeSTT RealtimeTTS
```

2. **POC æµ‹è¯• - STT**:
```python
from RealtimeSTT import AudioToTextRecorder
import asyncio

class RealtimeSTTService:
    def __init__(self):
        self.recorder = AudioToTextRecorder(
            model="large-v2",  # Whisper æ¨¡å‹
            language="zh",
            enable_realtime_transcription=True,
            silero_sensitivity=0.4,  # VAD çµæ•åº¦
            webrtc_sensitivity=3,
            post_speech_silence_duration=0.3
        )

    async def transcribe_stream(self, callback):
        """æµå¼è½¬å½•"""
        def on_text(text):
            asyncio.create_task(callback(text))

        self.recorder.text(on_text)
        self.recorder.start()
```

3. **POC æµ‹è¯• - TTS**:
```python
from RealtimeTTS import TextToAudioStream, SystemEngine

class RealtimeTTSService:
    def __init__(self):
        self.engine = SystemEngine()
        self.stream = TextToAudioStream(self.engine)

    async def synthesize_stream(self, text_stream):
        """æµå¼åˆæˆ"""
        async for text_chunk in text_stream:
            self.stream.feed(text_chunk)

        self.stream.play_async()
```

4. **æ€§èƒ½å¯¹æ¯”æµ‹è¯•**:
```python
import time

async def benchmark():
    # æµ‹è¯•ç§‘å¤§è®¯é£
    start = time.time()
    await iflytek_stt.transcribe(audio)
    iflytek_latency = time.time() - start

    # æµ‹è¯• RealtimeSTT
    start = time.time()
    await realtime_stt.transcribe(audio)
    realtime_latency = time.time() - start

    print(f"ç§‘å¤§è®¯é£å»¶è¿Ÿ: {iflytek_latency*1000:.0f}ms")
    print(f"RealtimeSTT å»¶è¿Ÿ: {realtime_latency*1000:.0f}ms")
```

**è¯„ä¼°æŒ‡æ ‡**:
- å»¶è¿Ÿï¼ˆTTFBã€æ€»å»¶è¿Ÿï¼‰
- å‡†ç¡®ç‡ï¼ˆWER - Word Error Rateï¼‰
- èµ„æºå ç”¨ï¼ˆCPUã€å†…å­˜ã€GPUï¼‰
- æˆæœ¬ï¼ˆAPI è°ƒç”¨ vs æœ¬åœ°éƒ¨ç½²ï¼‰

**å†³ç­–æ ‡å‡†**:
- å¦‚æœå»¶è¿Ÿé™ä½ >30% ä¸”å‡†ç¡®ç‡ç›¸å½“ â†’ åˆ‡æ¢
- å¦‚æœå‡†ç¡®ç‡ä¸‹é™ >10% â†’ ä¿ç•™ç§‘å¤§è®¯é£

---

## æ¶æ„æ”¹è¿›å»ºè®®

### 7. æµå¼æ¶æ„å…¨é¢å‡çº§

**å½“å‰æ¶æ„é—®é¢˜**:
- é¡ºåºå¤„ç†ï¼Œå»¶è¿Ÿç´¯åŠ 
- ç”¨æˆ·éœ€ç­‰å¾…å®Œæ•´å“åº”

**å»ºè®®æ¶æ„**:
```python
# ç«¯åˆ°ç«¯æµå¼å¤„ç†
async def streaming_conversation(audio_stream, session_id):
    """å®Œå…¨æµå¼çš„å¯¹è¯å¤„ç†"""

    # æµå¼ STT
    async for text_chunk in stt_stream(audio_stream):
        # ç´¯ç§¯æ–‡æœ¬åˆ°å®Œæ•´å¥å­
        if is_complete_sentence(text_chunk):
            # æµå¼ LLM
            async for llm_chunk in llm_stream(text_chunk, session_id):
                # æµå¼ TTS
                async for audio_chunk in tts_stream(llm_chunk):
                    # å®æ—¶è¾“å‡º
                    yield audio_chunk
```

**å®æ–½å»ºè®®**:
```python
# src/api/routes/conversation.py

@router.post("/stream")
async def streaming_conversation_endpoint(
    request: ConversationRequest,
    session_id: str = Header(...)
):
    async def event_generator():
        # è·å–éŸ³é¢‘æµ
        audio_stream = request.audio_stream

        # æµå¼å¤„ç†
        async for audio_chunk in streaming_conversation(
            audio_stream,
            session_id
        ):
            yield {
                "event": "audio",
                "data": base64.b64encode(audio_chunk).decode()
            }

        yield {"event": "done", "data": ""}

    return EventSourceResponse(event_generator())
```

**é¢„æœŸæ”¶ç›Š**:
- é¦–å­—èŠ‚æ—¶é—´ (TTFB) é™ä½ 60-70%
- ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿå¤§å¹…é™ä½
- æ”¯æŒæ‰“æ–­å’Œå®æ—¶åé¦ˆ

---

### 8. å¤šçº§ç¼“å­˜ç³»ç»Ÿ

**æ¶æ„è®¾è®¡**:
```
L1: å†…å­˜ç¼“å­˜ï¼ˆLRUï¼‰- çƒ­ç‚¹æ•°æ®ï¼Œå®¹é‡æœ‰é™
L2: Redis - åˆ†å¸ƒå¼å…±äº«ï¼Œå®¹é‡ä¸­ç­‰
L3: æ•°æ®åº“ - æŒä¹…åŒ–ï¼Œå®¹é‡å¤§
```

**å®æ–½æ–¹æ¡ˆ**:

1. **L1: å†…å­˜ç¼“å­˜**:
```python
from functools import lru_cache
from cachetools import TTLCache

# LLM å“åº”ç¼“å­˜ï¼ˆåŸºäº prompt hashï¼‰
llm_cache = TTLCache(maxsize=1000, ttl=3600)

@lru_cache(maxsize=500)
def cached_embedding(text: str):
    """Embedding ç¼“å­˜"""
    return embedding_model.encode(text)
```

2. **L2: Redis ç¼“å­˜**:
```python
import redis
import json
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

async def cached_llm_call(prompt: str, **kwargs):
    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = f"llm:{hashlib.md5(prompt.encode()).hexdigest()}"

    # å°è¯•ä» Redis è¯»å–
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)

    # è°ƒç”¨ LLM
    result = await llm.ainvoke(prompt, **kwargs)

    # å†™å…¥ Redisï¼ˆTTL 1å°æ—¶ï¼‰
    redis_client.setex(cache_key, 3600, json.dumps(result))

    return result
```

3. **L3: RAG ç»“æœç¼“å­˜**:
```python
# åœ¨æ•°æ®åº“ä¸­ç¼“å­˜ RAG æ£€ç´¢ç»“æœ
class RAGCache(Base):
    __tablename__ = "rag_cache"

    id = Column(Integer, primary_key=True)
    query_hash = Column(String, index=True, unique=True)
    results = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)

async def cached_rag_search(query: str, user_id: str):
    query_hash = hashlib.md5(f"{user_id}:{query}".encode()).hexdigest()

    # æ£€æŸ¥æ•°æ®åº“ç¼“å­˜ï¼ˆ24å°æ—¶å†…ï¼‰
    cached = await db.query(RAGCache).filter(
        RAGCache.query_hash == query_hash,
        RAGCache.created_at > datetime.utcnow() - timedelta(hours=24)
    ).first()

    if cached:
        return cached.results

    # æ‰§è¡Œæ£€ç´¢
    results = await rag_service.search(query, user_id)

    # å†™å…¥ç¼“å­˜
    await db.add(RAGCache(query_hash=query_hash, results=results))
    await db.commit()

    return results
```

**ç¼“å­˜ç­–ç•¥**:
- **Embedding**: æ°¸ä¹…ç¼“å­˜ï¼ˆå†…å®¹ä¸å˜ï¼‰
- **LLM å“åº”**: 1å°æ—¶ç¼“å­˜ï¼ˆå‡å°‘é‡å¤è°ƒç”¨ï¼‰
- **RAG æ£€ç´¢**: 24å°æ—¶ç¼“å­˜ï¼ˆæ–‡æ¡£æ›´æ–°é¢‘ç‡ä½ï¼‰
- **ä¼šè¯çŠ¶æ€**: æŒ‰éœ€ç¼“å­˜ï¼ˆçƒ­æ•°æ®åœ¨ Redisï¼‰

**é¢„æœŸæ”¶ç›Š**:
- LLM API è°ƒç”¨å‡å°‘ 40-60%
- å“åº”é€Ÿåº¦æå‡ 30-50%
- æˆæœ¬é™ä½ 30-40%

---

### 9. æ¨¡å—åŒ–å·¥å…·ç³»ç»Ÿæ‰©å±•

**å½“å‰å·¥å…·**:
- search (Tavily)
- calculator
- time
- weather
- (å…¶ä»– MCP å·¥å…·)

**å»ºè®®æ–°å¢**:

1. **Code Interpreter (ä»£ç æ‰§è¡Œ)**:
```python
from langchain.tools import PythonREPLTool

code_interpreter = PythonREPLTool()

# ç¤ºä¾‹ä½¿ç”¨
result = code_interpreter.run("""
import pandas as pd
data = {'A': [1, 2, 3], 'B': [4, 5, 6]}
df = pd.DataFrame(data)
df.describe()
""")
```

2. **File Operations (æ–‡ä»¶ç®¡ç†)**:
```python
from langchain.tools import FileManagementToolkit

file_tools = FileManagementToolkit(
    root_dir="./user_files",
    selected_tools=["read_file", "write_file", "list_directory"]
).get_tools()
```

3. **Database Query (æ•°æ®åº“æŸ¥è¯¢)**:
```python
from langchain_community.utilities import SQLDatabase
from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool

db = SQLDatabase.from_uri("postgresql://...")
db_tool = QuerySQLDataBaseTool(db=db)
```

4. **Web Scraper (ç½‘é¡µæŠ“å–)**:
```python
from langchain_community.tools import SerpAPIWrapper

web_scraper = SerpAPIWrapper()
```

5. **API Caller (é€šç”¨ API è°ƒç”¨)**:
```python
from langchain.tools import APIOperation
from langchain.chains import APIChain

api_chain = APIChain.from_llm_and_api_docs(
    llm,
    api_docs="...",
    verbose=True
)
```

**å·¥å…·æ³¨å†Œç³»ç»Ÿ**:
```python
# src/mcp/tool_registry.py

class ToolRegistry:
    def __init__(self):
        self.tools = {}

    def register(self, name: str, tool: Any, category: str = "general"):
        """æ³¨å†Œå·¥å…·"""
        self.tools[name] = {
            "tool": tool,
            "category": category,
            "enabled": True
        }

    def get_tools(self, categories: list = None, user_id: str = None):
        """è·å–å·¥å…·åˆ—è¡¨ï¼ˆæ”¯æŒæƒé™è¿‡æ»¤ï¼‰"""
        if categories is None:
            return [t["tool"] for t in self.tools.values() if t["enabled"]]

        return [
            t["tool"]
            for t in self.tools.values()
            if t["category"] in categories and t["enabled"]
        ]

# ä½¿ç”¨
registry = ToolRegistry()
registry.register("search", search_tool, category="information")
registry.register("calculator", calculator_tool, category="computation")
registry.register("code_interpreter", code_tool, category="advanced")

# Agent è·å–å·¥å…·
tools = registry.get_tools(categories=["information", "computation"])
```

---

## æŠ€æœ¯é€‰å‹ä¼˜å…ˆçº§çŸ©é˜µ

### ç«‹å³å®æ–½ï¼ˆ1-2 å‘¨ï¼‰

| æŠ€æœ¯ | ä¸šåŠ¡ä»·å€¼ | å®æ–½éš¾åº¦ | é¢„è®¡æ—¶é—´ | ROI |
|------|---------|---------|---------|-----|
| prometheus-fastapi-instrumentator | â­â­â­â­â­ | â­ | 1 å°æ—¶ | æé«˜ |
| LangGraph Pre-Builts + ç¼“å­˜ | â­â­â­â­â­ | â­â­ | 1-2 å¤© | æé«˜ |
| RAG Hybrid Search | â­â­â­â­ | â­â­â­ | 1 å‘¨ | é«˜ |

**ä¼˜å…ˆçº§æ’åº**:
1. prometheus-fastapi-instrumentator (æœ€å¿«è§æ•ˆ)
2. LangGraph å‡çº§ï¼ˆæå‡ä»£ç è´¨é‡ï¼‰
3. Hybrid Searchï¼ˆæå‡ RAG è´¨é‡ï¼‰

---

### çŸ­æœŸæ¢ç´¢ï¼ˆ1-2 æœˆï¼‰

| æŠ€æœ¯ | ä¸šåŠ¡ä»·å€¼ | å®æ–½éš¾åº¦ | é¢„è®¡æ—¶é—´ | ROI |
|------|---------|---------|---------|-----|
| Two-Phase Retrieval + Reranking | â­â­â­â­â­ | â­â­â­ | 1-2 å‘¨ | é«˜ |
| å®Œæ•´å¯è§‚æµ‹æ€§æ ˆ | â­â­â­â­ | â­â­â­ | 2-3 å‘¨ | ä¸­é«˜ |
| RealtimeSTT/TTS è¯„ä¼° | â­â­â­ | â­â­â­ | 1-2 å‘¨ | ä¸­ |
| å¤šçº§ç¼“å­˜ç³»ç»Ÿ | â­â­â­â­ | â­â­ | 1 å‘¨ | é«˜ |

**ä¼˜å…ˆçº§æ’åº**:
1. Two-Phase Retrievalï¼ˆæ˜¾è‘—æå‡ RAGï¼‰
2. å¤šçº§ç¼“å­˜ï¼ˆé™ä½æˆæœ¬ï¼‰
3. å®Œæ•´å¯è§‚æµ‹æ€§æ ˆï¼ˆç”Ÿäº§å°±ç»ªï¼‰
4. RealtimeSTT/TTSï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰

---

### ä¸­æœŸè§„åˆ’ï¼ˆ3-6 æœˆï¼‰

| æŠ€æœ¯ | ä¸šåŠ¡ä»·å€¼ | å®æ–½éš¾åº¦ | é¢„è®¡æ—¶é—´ | ROI |
|------|---------|---------|---------|-----|
| CrewAI å¤š Agent ç³»ç»Ÿ | â­â­â­â­â­ | â­â­â­â­ | 3-4 å‘¨ | ä¸­é«˜ |
| Speech-to-Speech æ¶æ„ | â­â­â­ | â­â­â­â­â­ | 4-6 å‘¨ | ä¸­ |
| æµå¼æ¶æ„å‡çº§ | â­â­â­â­ | â­â­â­â­ | 3-4 å‘¨ | é«˜ |
| å·¥å…·ç³»ç»Ÿæ‰©å±• | â­â­â­â­ | â­â­â­ | 2-3 å‘¨ | ä¸­é«˜ |

**ä¼˜å…ˆçº§æ’åº**:
1. CrewAI å¤š Agentï¼ˆå·®å¼‚åŒ–ç«äº‰åŠ›ï¼‰
2. æµå¼æ¶æ„ï¼ˆç”¨æˆ·ä½“éªŒï¼‰
3. å·¥å…·ç³»ç»Ÿæ‰©å±•ï¼ˆåŠŸèƒ½ä¸°å¯Œåº¦ï¼‰
4. Speech-to-Speechï¼ˆé•¿æœŸæ¢ç´¢ï¼‰

---

## å®æ–½è·¯çº¿å›¾

### ç¬¬ä¸€å‘¨ï¼šå¿«é€Ÿèƒœåˆ© ğŸƒ

**Day 1-2: Prometheus é›†æˆ**
```bash
# 1. å®‰è£…
pip install prometheus-fastapi-instrumentator

# 2. é›†æˆåˆ° main.py
from prometheus_fastapi_instrumentator import Instrumentator
Instrumentator().instrument(app).expose(app)

# 3. å¯åŠ¨ Prometheus
docker run -p 9090:9090 -v ./prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus

# 4. éªŒè¯
curl http://localhost:8000/metrics
```

**Day 3-4: LangGraph å‡çº§**
```bash
# 1. å‡çº§
pip install --upgrade langgraph

# 2. é‡æ„ agent/nodes.py
# ä½¿ç”¨ Pre-Builts å’ŒèŠ‚ç‚¹ç¼“å­˜

# 3. æµ‹è¯•
pytest tests/agent/
```

**Day 5-7: RAG Hybrid Search**
```bash
# 1. å®ç° hybrid_search()
# 2. å®ç° RRF ç®—æ³•
# 3. é›†æˆåˆ° RAG Service
# 4. å¯¹æ¯”æµ‹è¯•ï¼ˆå‡†ç¡®ç‡ï¼‰
```

**é¢„æœŸæˆæœ**:
- âœ… ç›‘æ§ç³»ç»Ÿä¸Šçº¿
- âœ… ä»£ç è´¨é‡æå‡
- âœ… RAG æ£€ç´¢æ”¹å–„

---

### ç¬¬äºŒå‘¨ï¼šè´¨é‡æå‡ ğŸ”§

**Week 2: Two-Phase Retrieval**
```bash
# 1. å®‰è£… Reranker
pip install sentence-transformers

# 2. å®ç°ä¸¤é˜¶æ®µæ£€ç´¢
# 3. æ€§èƒ½å¯¹æ¯”æµ‹è¯•
# 4. ä¸Šçº¿ A/B æµ‹è¯•
```

**é¢„æœŸæˆæœ**:
- âœ… RAG ç²¾åº¦æå‡ 25-40%
- âœ… å»ºç«‹è¯„ä¼°åŸºå‡†

---

### ç¬¬ä¸‰å‘¨ï¼šå¯è§‚æµ‹æ€§å®Œå–„ ğŸ“Š

**Week 3: å®Œæ•´å¯è§‚æµ‹æ€§æ ˆ**
```bash
# 1. åˆ›å»º docker-compose.observability.yml
# 2. é…ç½® Prometheus + Loki + Tempo + Grafana
# 3. é›†æˆ OpenTelemetry
# 4. åˆ›å»º Grafana ä»ªè¡¨æ¿
# 5. é…ç½®å‘Šè­¦è§„åˆ™
```

**é¢„æœŸæˆæœ**:
- âœ… å®Œæ•´ç›‘æ§ä½“ç³»
- âœ… ç”Ÿäº§ç¯å¢ƒå°±ç»ª

---

### ç¬¬å››å‘¨ï¼šç¼“å­˜ä¼˜åŒ– âš¡

**Week 4: å¤šçº§ç¼“å­˜ç³»ç»Ÿ**
```bash
# 1. éƒ¨ç½² Redis
# 2. å®ç° L1/L2/L3 ç¼“å­˜
# 3. æ€§èƒ½æµ‹è¯•
# 4. æˆæœ¬åˆ†æ
```

**é¢„æœŸæˆæœ**:
- âœ… API è°ƒç”¨å‡å°‘ 40-60%
- âœ… å“åº”é€Ÿåº¦æå‡ 30-50%

---

### ç¬¬äºŒä¸ªæœˆï¼šèƒ½åŠ›å¢å¼º ğŸš€

**Week 5-6: RealtimeSTT/TTS è¯„ä¼°**
- POC æµ‹è¯•
- æ€§èƒ½å¯¹æ¯”
- å†³ç­–æ˜¯å¦åˆ‡æ¢

**Week 7-8: æµå¼æ¶æ„å‡çº§**
- é‡æ„å¯¹è¯æµç¨‹
- å®ç°ç«¯åˆ°ç«¯æµå¼
- ç”¨æˆ·æµ‹è¯•

---

### ç¬¬ä¸‰ä¸ªæœˆï¼šå·®å¼‚åŒ–ç«äº‰åŠ› ğŸ’

**Week 9-12: CrewAI å¤š Agent ç³»ç»Ÿ**
- å­¦ä¹  CrewAI
- è®¾è®¡ Agent è§’è‰²
- å®ç°åä½œæµç¨‹
- åœºæ™¯æµ‹è¯•

---

## æ€»ç»“ä¸å»ºè®®

### å…³é”®å‘ç°

1. **LangGraph 2025 å·²éå¸¸æˆç†Ÿ**ï¼ŒPre-Builts å’Œç¼“å­˜åŠŸèƒ½å¯ç«‹å³æå‡é¡¹ç›®è´¨é‡
2. **RAG ä¼˜åŒ–ç©ºé—´å·¨å¤§**ï¼ŒHybrid Search + Reranking å¯æ˜¾è‘—æå‡å‡†ç¡®ç‡
3. **å¯è§‚æµ‹æ€§æ˜¯ç”Ÿäº§ç¯å¢ƒå¿…éœ€**ï¼ŒPrometheus + Grafana åº”å°½å¿«éƒ¨ç½²
4. **å®æ—¶è¯­éŸ³æœ‰å¤šç§æ–¹æ¡ˆ**ï¼ŒRealtimeSTT/TTS æ˜¯ä½æˆæœ¬æ›¿ä»£æ–¹æ¡ˆ
5. **å¤š Agent åä½œæ˜¯æœªæ¥è¶‹åŠ¿**ï¼ŒCrewAI æ˜¯æœ€æ˜“ä¸Šæ‰‹çš„æ¡†æ¶

### ç«‹å³è¡ŒåŠ¨å»ºè®®

**æœ¬å‘¨å¿…åš** (P0):
1. é›†æˆ `prometheus-fastapi-instrumentator`ï¼ˆ1 å°æ—¶ï¼‰
2. å‡çº§ LangGraph å¹¶æ·»åŠ èŠ‚ç‚¹ç¼“å­˜ï¼ˆ1 å¤©ï¼‰
3. éƒ¨ç½² Prometheus + Grafanaï¼ˆ2 å¤©ï¼‰

**æœ¬æœˆå®Œæˆ** (P1):
4. å®ç° RAG Hybrid Searchï¼ˆ1 å‘¨ï¼‰
5. é›†æˆ Reranking æ¨¡å‹ï¼ˆ1 å‘¨ï¼‰
6. éƒ¨ç½²å®Œæ•´å¯è§‚æµ‹æ€§æ ˆï¼ˆ2 å‘¨ï¼‰

**ä¸‹å­£åº¦æ¢ç´¢** (P2):
7. è¯„ä¼° RealtimeSTT/TTSï¼ˆ1-2 å‘¨ï¼‰
8. å­¦ä¹  CrewAI æ¡†æ¶ï¼ˆ2 å‘¨ï¼‰
9. é‡æ„æµå¼æ¶æ„ï¼ˆ3-4 å‘¨ï¼‰

### é£é™©æç¤º

- **æŠ€æœ¯å€ºåŠ¡ä¼˜å…ˆå¤„ç†**: æµ‹è¯•è¦†ç›–ç‡ã€ç›‘æ§ç³»ç»Ÿæ¯”æ–°åŠŸèƒ½æ›´é‡è¦
- **æ¸è¿›å¼å‡çº§**: é¿å…å¤§è§„æ¨¡é‡æ„ï¼Œé‡‡ç”¨å¢é‡è¿­ä»£
- **æ€§èƒ½åŸºå‡†å»ºç«‹**: ä¼˜åŒ–å‰å…ˆå»ºç«‹åŸºçº¿ï¼Œæ‰èƒ½é‡åŒ–æ”¶ç›Š
- **æˆæœ¬æ§åˆ¶**: å¼•å…¥ç¼“å­˜ç³»ç»Ÿï¼Œé¿å… API è°ƒç”¨æ¿€å¢

### æˆåŠŸæŒ‡æ ‡

**æŠ€æœ¯æŒ‡æ ‡**:
- ç›‘æ§è¦†ç›–ç‡: 100%
- RAG å‡†ç¡®ç‡: +25%
- å“åº”å»¶è¿Ÿ: -30%
- API æˆæœ¬: -40%

**ä¸šåŠ¡æŒ‡æ ‡**:
- ç”¨æˆ·æ»¡æ„åº¦: 4.5+/5.0
- å¯¹è¯å®Œæˆç‡: 90%+
- å·¥å…·è°ƒç”¨æˆåŠŸç‡: 95%+

---

## å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- [LangGraph æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [Prometheus æœ€ä½³å®è·µ](https://prometheus.io/docs/practices/)
- [Grafana ä»ªè¡¨æ¿åº“](https://grafana.com/grafana/dashboards/)
- [OpenTelemetry Python](https://opentelemetry.io/docs/instrumentation/python/)

### å¼€æºé¡¹ç›®

- [fastapi-observability](https://github.com/blueswen/fastapi-observability)
- [RealtimeSTT](https://github.com/KoljaB/RealtimeSTT)
- [RealtimeTTS](https://github.com/KoljaB/RealtimeTTS)
- [CrewAI](https://github.com/joaomdmoura/crewAI)

### å­¦æœ¯è®ºæ–‡

- [Enhancing Retrieval-Augmented Generation: A Study of Best Practices](https://arxiv.org/abs/2501.07391)
- [Toward Low-Latency End-to-End Voice Agents](https://arxiv.org/html/2508.04721v1)

### ç¤¾åŒºèµ„æº

- [LangChain Discord](https://discord.gg/langchain)
- [AI Agent Builders Community](https://www.skool.com/ai-agent-builders)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0
**æœ€åæ›´æ–°**: 2025-11-12
**ä¸‹æ¬¡å®¡æŸ¥**: 2025-12-12
**ç»´æŠ¤è€…**: Development Team

---

*æœ¬æ–‡æ¡£å°†æ ¹æ®æŠ€æœ¯å‘å±•å’Œé¡¹ç›®éœ€æ±‚åŠ¨æ€æ›´æ–°*
