# LLM API å…¼å®¹æ€§ä¿®å¤æ€»ç»“

**æ—¥æœŸ**: 2025-10-14  
**é—®é¢˜**: GPT-5 ç³»åˆ—æ¨¡å‹ä¸æ”¯æŒ `max_tokens` å‚æ•°

---

## ğŸ› é—®é¢˜æè¿°

æµ‹è¯• LLM è¿æ¥æ—¶é‡åˆ°é”™è¯¯ï¼š
```
"Unsupported parameter: 'max_tokens' is not supported with this model. 
Use 'max_completion_tokens' instead."
```

**åŸå› **: GPT-5 ç³»åˆ—æ¨¡å‹ä½¿ç”¨æ–°çš„ API å‚æ•°ï¼š
- **æ—§å‚æ•°**: `max_tokens` (GPT-4 åŠæ›´æ—©ç‰ˆæœ¬)
- **æ–°å‚æ•°**: `max_completion_tokens` (GPT-5 ç³»åˆ—)

---

## âœ… è§£å†³æ–¹æ¡ˆ

### 1. åˆ›å»º LLM å…¼å®¹å±‚

**æ–‡ä»¶**: `src/utils/llm_compat.py`

**æ ¸å¿ƒåŠŸèƒ½**:
```python
def prepare_llm_params(model, messages, temperature, max_tokens, **kwargs):
    """è‡ªåŠ¨é€‚é…ä¸åŒæ¨¡å‹çš„å‚æ•°"""
    params = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
    }
    
    # GPT-5 ç³»åˆ—ä½¿ç”¨ max_completion_tokens
    if model.startswith("gpt-5"):
        params["max_completion_tokens"] = max_tokens
    else:
        params["max_tokens"] = max_tokens
    
    return params
```

**å…¶ä»–å·¥å…·å‡½æ•°**:
- `is_gpt5_model(model)` - åˆ¤æ–­æ¨¡å‹ç±»å‹
- `get_max_tokens_param_name(model)` - è·å–å‚æ•°å
- `get_model_features(model)` - è·å–æ¨¡å‹ç‰¹æ€§
- `validate_model_params(model, **params)` - éªŒè¯å‚æ•°

### 2. æ›´æ–° Agent ä»£ç 

**ä¿®æ”¹æ–‡ä»¶**:
1. `src/agent/nodes.py`
   - å¯¼å…¥å…¼å®¹å±‚
   - æ›´æ–° `call_llm()` æ–¹æ³•
   - æ›´æ–° `_make_llm_call()` æ–¹æ³•
   - æ›´æ–° `stream_llm_call()` æ–¹æ³•

2. `src/agent/graph.py`
   - å¯¼å…¥å…¼å®¹å±‚
   - æ›´æ–° `process_message_stream()` æ–¹æ³•

**ä¿®æ”¹ç¤ºä¾‹**:
```python
# æ—§ä»£ç 
llm_config = {
    "model": model,
    "messages": messages,
    "temperature": temperature,
    "max_tokens": max_tokens
}

# æ–°ä»£ç ï¼ˆä½¿ç”¨å…¼å®¹å±‚ï¼‰
llm_config = prepare_llm_params(
    model=model,
    messages=messages,
    temperature=temperature,
    max_tokens=max_tokens
)
```

### 3. æ›´æ–°æµ‹è¯•ä»£ç 

**æ–‡ä»¶**: `test_llm_connection.py`

**ä¿®æ”¹**:
```python
payload = {
    "model": config.llm.models.fast,
    "messages": [...],
    "max_completion_tokens": 100  # ä½¿ç”¨æ–°å‚æ•°
}
```

---

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹æ˜ å°„

| æ¨¡å‹ç³»åˆ— | max_tokens å‚æ•° | ä¸Šä¸‹æ–‡é•¿åº¦ | åŠŸèƒ½æ”¯æŒ |
|---------|----------------|-----------|---------|
| **gpt-5-pro** | `max_completion_tokens` | 128K | Vision + Function Calling |
| **gpt-5-mini** | `max_completion_tokens` | 128K | Vision + Function Calling |
| **gpt-5-chat-latest** | `max_completion_tokens` | 128K | Vision + Function Calling |
| **gpt-5-nano** | `max_completion_tokens` | 32K | Function Calling |
| **gpt-4-turbo** | `max_tokens` | 128K | Vision + Function Calling |
| **gpt-4** | `max_tokens` | 8K | Function Calling |
| **gpt-3.5-turbo** | `max_tokens` | 16K | Function Calling |

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### æµ‹è¯•å·¥å…·

1. **é…ç½®éªŒè¯**: `python test_config.py`
   - âœ… 7/7 æµ‹è¯•é€šè¿‡

2. **LLM è¿æ¥æµ‹è¯•**: `python test_llm_connection.py`
   - æµ‹è¯• `/models` ç«¯ç‚¹
   - æµ‹è¯• `/chat/completions` ç«¯ç‚¹
   - éªŒè¯æ¨¡å‹å¯ç”¨æ€§

3. **å¿«é€Ÿæµ‹è¯•**: `python test_quick_llm.py`
   - ç®€åŒ–ç‰ˆ API æµ‹è¯•
   - ç›´æ¥æµ‹è¯• max_completion_tokens

### é¢„æœŸç»“æœ

```bash
$ python test_quick_llm.py

ğŸ§ª å¿«é€Ÿ LLM API æµ‹è¯•
URL: https://api.openai-proxy.org/v1
Model: gpt-5-mini
API Key: sk-M9DIQm5fQ66GgUtC...

æµ‹è¯•å¯¹è¯...
Status: 200
âœ… æˆåŠŸ!
å“åº”: é…ç½®æµ‹è¯•æˆåŠŸï¼

ğŸ‰ é…ç½®éªŒè¯é€šè¿‡ï¼LLM API å·¥ä½œæ­£å¸¸ã€‚
```

---

## ğŸ”„ å‘åå…¼å®¹æ€§

å…¼å®¹å±‚**å®Œå…¨å‘åå…¼å®¹**ï¼Œæ”¯æŒï¼š
- âœ… GPT-5 ç³»åˆ—ï¼ˆè‡ªåŠ¨ä½¿ç”¨ max_completion_tokensï¼‰
- âœ… GPT-4 ç³»åˆ—ï¼ˆä½¿ç”¨ max_tokensï¼‰
- âœ… GPT-3.5 ç³»åˆ—ï¼ˆä½¿ç”¨ max_tokensï¼‰
- âœ… è‡ªå®šä¹‰æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨ max_tokensï¼‰

---

## ğŸ“ ä»£ç å˜æ›´æ¸…å•

### æ–°å¢æ–‡ä»¶
- `src/utils/llm_compat.py` (200+ è¡Œ)
- `src/utils/__init__.py`
- `test_quick_llm.py`
- `LLM_API_FIX_SUMMARY.md` (æœ¬æ–‡æ¡£)

### ä¿®æ”¹æ–‡ä»¶
- `src/agent/nodes.py`
  - å¯¼å…¥ `prepare_llm_params`
  - 3 å¤„ä½¿ç”¨å…¼å®¹å±‚
- `src/agent/graph.py`
  - å¯¼å…¥ `prepare_llm_params`
  - 1 å¤„ä½¿ç”¨å…¼å®¹å±‚
- `test_llm_connection.py`
  - ä½¿ç”¨ `max_completion_tokens`

---

## ğŸ’¡ ä½¿ç”¨æŒ‡å—

### åœ¨ä»£ç ä¸­ä½¿ç”¨å…¼å®¹å±‚

```python
from utils.llm_compat import prepare_llm_params

# è‡ªåŠ¨é€‚é…å‚æ•°
params = prepare_llm_params(
    model="gpt-5-mini",  # æˆ– "gpt-4", "gpt-3.5-turbo"
    messages=messages,
    temperature=0.7,
    max_tokens=2048
)

# params å°†åŒ…å«æ­£ç¡®çš„å‚æ•°åï¼š
# GPT-5: {"model": "gpt-5-mini", "max_completion_tokens": 2048, ...}
# GPT-4: {"model": "gpt-4", "max_tokens": 2048, ...}
```

### æ£€æŸ¥æ¨¡å‹ç±»å‹

```python
from utils.llm_compat import is_gpt5_model, get_model_features

# åˆ¤æ–­æ¨¡å‹ç±»å‹
if is_gpt5_model("gpt-5-mini"):
    print("This is GPT-5")

# è·å–æ¨¡å‹ç‰¹æ€§
features = get_model_features("gpt-5-pro")
print(features["max_tokens_param"])  # "max_completion_tokens"
print(features["max_context"])       # 128000
```

---

## âœ… éªŒè¯æ¸…å•

- [x] åˆ›å»º LLM å…¼å®¹å±‚ (`utils/llm_compat.py`)
- [x] æ›´æ–° `nodes.py` ä½¿ç”¨å…¼å®¹å±‚
- [x] æ›´æ–° `graph.py` ä½¿ç”¨å…¼å®¹å±‚
- [x] ä¿®å¤ `test_llm_connection.py`
- [x] åˆ›å»ºå¿«é€Ÿæµ‹è¯•è„šæœ¬
- [x] æ–‡æ¡£åŒ–è§£å†³æ–¹æ¡ˆ
- [ ] è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
- [ ] éªŒè¯ API æœåŠ¡å™¨å¯åŠ¨
- [ ] æµ‹è¯•ç«¯åˆ°ç«¯å¯¹è¯åŠŸèƒ½

---

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **è¿è¡Œå®Œæ•´æµ‹è¯•**:
   ```bash
   python test_llm_connection.py
   python test_quick_llm.py
   ```

2. **å¯åŠ¨æœåŠ¡å™¨**:
   ```bash
   python start_server.py
   ```

3. **æµ‹è¯• API**:
   ```bash
   python test_api.py
   ```

4. **ç»§ç»­ Phase 2B å¼€å‘**:
   - å®ç° STT æœåŠ¡
   - å®ç° TTS æœåŠ¡
   - é›†æˆè¯­éŸ³æµç¨‹

---

**ä¿®å¤çŠ¶æ€**: âœ… å·²å®Œæˆ  
**æµ‹è¯•çŠ¶æ€**: â³ å¾…éªŒè¯  
**å½±å“èŒƒå›´**: Agent æ ¸å¿ƒä»£ç ï¼Œå®Œå…¨å‘åå…¼å®¹
