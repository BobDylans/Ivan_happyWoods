"""
Agent Nodes Implementation

This module contains the core LangGraph nodes that handle different stages
of conversation processing, including input processing, LLM calls, tool handling,
and response formatting.
"""

import json
import logging
import random
import time
from typing import Dict, Any, List, Optional, Union
import asyncio
import httpx
from datetime import datetime

from .state import AgentState, ConversationMessage, MessageRole, ToolCall, ToolResult

# å¯¼å…¥ LLM å…¼å®¹æ€§å·¥å…·
try:
    from utils.llm_compat import prepare_llm_params
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªç®€å•çš„å…¼å®¹å‡½æ•°
    def prepare_llm_params(model, messages, temperature=0.7, max_tokens=16384, **kwargs):  # ğŸ”§ ä¿®å¤é»˜è®¤å€¼
        params = {
            "model": model,
            "messages": messages,
        }
        # GPT-5 ç³»åˆ—ä¸ä¼  temperatureï¼Œä½¿ç”¨ API é»˜è®¤å€¼
        if not model.startswith("gpt-5"):
            params["temperature"] = temperature
        # GPT-5 ç³»åˆ—ä½¿ç”¨ max_completion_tokens
        if model.startswith("gpt-5"):
            params["max_completion_tokens"] = max_tokens
        else:
            params["max_tokens"] = max_tokens
        params.update(kwargs)
        return params


class DateTimeJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder that handles datetime objects."""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)

try:
    from config.models import VoiceAgentConfig
except ImportError:
    # Fallback for when running as script
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from config.models import VoiceAgentConfig


logger = logging.getLogger(__name__)


class AgentNodes:
    """LangGraph å¯¹è¯å¤„ç†èŠ‚ç‚¹é›†åˆ
    
    è´Ÿè´£å¯¹è¯æµç¨‹ä¸­çš„å„ä¸ªå¤„ç†é˜¶æ®µï¼š
    - è¾“å…¥å¤„ç†å’ŒéªŒè¯
    - LLM è°ƒç”¨ï¼ˆåŒæ­¥/æµå¼ï¼‰
    - å·¥å…·è°ƒç”¨å¤„ç†
    - å“åº”æ ¼å¼åŒ–
    """
    
    def __init__(self, config: VoiceAgentConfig, trace=None):
        """åˆå§‹åŒ–èŠ‚ç‚¹é…ç½®
        
        Args:
            config: è¯­éŸ³åŠ©æ‰‹é…ç½®å¯¹è±¡
            trace: å¯é€‰çš„ TraceEmitter å®ä¾‹ï¼ˆç”¨äºå¯è§†åŒ–äº‹ä»¶ï¼‰
        """
        self.config = config
        self.logger = logger
        self._http_client: Optional[httpx.AsyncClient] = None
        self._client_lock = asyncio.Lock()
        
        # æ¥æ”¶ trace å®ä¾‹
        self.trace = trace
        
        # ğŸ†• å·¥å…·ç»“æœç¼“å­˜ï¼ˆä¼˜åŒ–ï¼šé¿å…é‡å¤è°ƒç”¨ç›¸åŒå·¥å…·ï¼‰
        # æ ¼å¼: {cache_key: (result, timestamp)}
        self._tool_cache: Dict[str, tuple[ToolResult, float]] = {}
        self._cache_ttl = 300  # ç¼“å­˜æœ‰æ•ˆæœŸï¼š5åˆ†é’Ÿ
        
        # ğŸ†• é¢„æ„å»ºåŸºç¡€ç³»ç»Ÿæç¤ºè¯ï¼ˆä¼˜åŒ–ï¼šé¿å…æ¯æ¬¡é‡æ–°ç”Ÿæˆï¼‰
        self._base_system_prompt = self._build_base_system_prompt()
    
    def _build_base_system_prompt(self) -> str:
        """æ„å»ºåŸºç¡€ç³»ç»Ÿæç¤ºè¯ï¼ˆé™æ€éƒ¨åˆ†ï¼‰
        
        åªåœ¨åˆå§‹åŒ–æ—¶æ„å»ºä¸€æ¬¡ï¼Œé¿å…æ¯æ¬¡ LLM è°ƒç”¨éƒ½é‡æ–°ç”Ÿæˆã€‚
        
        Returns:
            åŸºç¡€ç³»ç»Ÿæç¤ºè¯
        """
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„ AI åŠ©æ‰‹ï¼Œè‡´åŠ›äºä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯å’Œå¸®åŠ©ã€‚

## æ ¸å¿ƒèƒ½åŠ›

ä½ å¯ä»¥ï¼š
1. **å›ç­”é—®é¢˜**ï¼šåŸºäºçŸ¥è¯†åº“æä¾›å‡†ç¡®çš„ç­”æ¡ˆ
2. **æœç´¢ä¿¡æ¯**ï¼šä½¿ç”¨ web_search å·¥å…·æœç´¢æœ€æ–°ä¿¡æ¯
3. **æ‰§è¡Œè®¡ç®—**ï¼šä½¿ç”¨ calculator å·¥å…·è¿›è¡Œæ•°å­¦è®¡ç®—
4. **æŸ¥è¯¢æ—¶é—´**ï¼šä½¿ç”¨ get_time å·¥å…·è·å–å½“å‰æ—¶é—´
5. **æŸ¥è¯¢å¤©æ°”**ï¼šä½¿ç”¨ get_weather å·¥å…·æŸ¥è¯¢å¤©æ°”ä¿¡æ¯

## äº¤äº’åŸåˆ™

1. **å‡†ç¡®æ€§ç¬¬ä¸€**ï¼šä¸ç¡®å®šæ—¶æ˜ç¡®å‘ŠçŸ¥ï¼Œä¸ç¼–é€ ä¿¡æ¯
2. **ç»“æ„åŒ–å›å¤**ï¼šä½¿ç”¨ Markdown æ ¼å¼ç»„ç»‡å†…å®¹
3. **ä¿¡æ¯æ¥æº**ï¼šæœç´¢ç»“æœéœ€æ ‡æ³¨æ¥æºå’Œé“¾æ¥
4. **ç®€æ´æ˜äº†**ï¼šé¿å…å†—é•¿ï¼Œç›´å‡»è¦ç‚¹
5. **å‹å¥½ä¸“ä¸š**ï¼šä¿æŒç¤¼è²Œï¼Œè¯­æ°”è‡ªç„¶

## Markdown æ ¼å¼è§„èŒƒ

### æ ‡é¢˜å±‚çº§
- ä½¿ç”¨ `## ` ä½œä¸ºä¸»æ ‡é¢˜
- ä½¿ç”¨ `### ` ä½œä¸ºå°æ ‡é¢˜
- **ä¸è¦ä½¿ç”¨** `# ` ä¸€çº§æ ‡é¢˜

### åˆ—è¡¨æ ¼å¼
- é¡¹ç›®ä¹‹é—´**å¿…é¡»æœ‰ç©ºè¡Œ**
- ä½¿ç”¨ `-` æˆ–æ•°å­—åˆ—è¡¨
- é‡è¦ä¿¡æ¯ç”¨ **ç²—ä½“** çªå‡º

### ä»£ç å±•ç¤º
- å•è¡Œä»£ç ç”¨ `åå¼•å·`
- å¤šè¡Œä»£ç ç”¨ ```è¯­è¨€åç§° ä»£ç å—
- å¿…é¡»æŒ‡å®šè¯­è¨€ä»¥å¯ç”¨è¯­æ³•é«˜äº®

### é“¾æ¥æ ¼å¼
- æ ¼å¼ï¼š[é“¾æ¥æ–‡å­—](URL)
- æœç´¢ç»“æœé“¾æ¥å¿…é¡»å¯ç‚¹å‡»

## æœç´¢ç»“æœæ ¼å¼

å½“ä½¿ç”¨æœç´¢å·¥å…·æ—¶ï¼Œ**å¿…é¡»**æŒ‰ä»¥ä¸‹æ ¼å¼ç»„ç»‡ï¼š

```
## ğŸ” æœç´¢ç»“æœ

æ ¹æ®æœ€æ–°æœç´¢ï¼Œæˆ‘ä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š

### 1. [ç»“æœæ ‡é¢˜](URL)
- **æ¥æº**ï¼šç½‘ç«™åç§°
- **å…³é”®ä¿¡æ¯**ï¼šç®€è¦æè¿°

### 2. [ç»“æœæ ‡é¢˜](URL)
- **æ¥æº**ï¼šç½‘ç«™åç§°
- **å…³é”®ä¿¡æ¯**ï¼šç®€è¦æè¿°

## ğŸ“ æ€»ç»“

[å¯¹æœç´¢ç»“æœçš„ç»¼åˆåˆ†æ]
```

## å·¥å…·ä½¿ç”¨æŒ‡å—

### ä½•æ—¶ä½¿ç”¨å·¥å…·
- ç”¨æˆ·è¯¢é—®**æœ€æ–°ä¿¡æ¯**ï¼ˆæ–°é—»ã€æ—¶äº‹ï¼‰â†’ ä½¿ç”¨ web_search
- ç”¨æˆ·éœ€è¦**è®¡ç®—**ï¼ˆæ•°å­¦ã€æ¢ç®—ï¼‰â†’ ä½¿ç”¨ calculator
- ç”¨æˆ·è¯¢é—®**å½“å‰æ—¶é—´/æ—¥æœŸ** â†’ ä½¿ç”¨ get_time
- ç”¨æˆ·è¯¢é—®**å¤©æ°”** â†’ ä½¿ç”¨ get_weather

### å·¥å…·è°ƒç”¨åŸåˆ™
1. **æ˜ç¡®éœ€æ±‚**ï¼šç¡®è®¤æ˜¯å¦çœŸçš„éœ€è¦å·¥å…·
2. **å•æ¬¡è°ƒç”¨**ï¼šä¼˜å…ˆä¸€æ¬¡æ€§è·å–æ‰€éœ€ä¿¡æ¯
3. **ç»“æœæ•´åˆ**ï¼šå°†å·¥å…·ç»“æœè‡ªç„¶åœ°èå…¥å›å¤
4. **å¤±è´¥é™çº§**ï¼šå·¥å…·å¤±è´¥æ—¶æä¾›æ›¿ä»£æ–¹æ¡ˆ"""
    
    async def _ensure_http_client(self):
        """ç¡®ä¿ HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼ˆæ‡’åŠ è½½ï¼‰
        
        ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ç¡®ä¿çº¿ç¨‹å®‰å…¨çš„å•ä¾‹åˆå§‹åŒ–ã€‚
        """
        if self._http_client is None:
            async with self._client_lock:
                if self._http_client is None:  # åŒé‡æ£€æŸ¥
                    timeout = httpx.Timeout(self.config.llm.timeout, connect=10)
                    self._http_client = httpx.AsyncClient(
                        timeout=timeout,
                        headers={
                            "Authorization": f"Bearer {self.config.llm.api_key}",
                            "Content-Type": "application/json"
                        }
                    )
                    self.logger.debug("HTTP å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    def _build_llm_url(self, endpoint: str = "chat/completions") -> str:
        """æ„å»º LLM API å®Œæ•´ URL
        
        è‡ªåŠ¨å¤„ç† base_url ä¸­æ˜¯å¦åŒ…å« /v1 çš„æƒ…å†µã€‚
        
        Args:
            endpoint: API ç«¯ç‚¹è·¯å¾„ï¼Œé»˜è®¤ä¸º "chat/completions"
        
        Returns:
            å®Œæ•´çš„ API URL
        
        Examples:
            >>> # base_url = "https://api.openai-proxy.org/v1"
            >>> self._build_llm_url()
            "https://api.openai-proxy.org/v1/chat/completions"
            
            >>> # base_url = "https://api.openai-proxy.org"
            >>> self._build_llm_url()
            "https://api.openai-proxy.org/v1/chat/completions"
        """
        base = self.config.llm.base_url.rstrip('/')
        
        # ä»…åœ¨ base_url ä¸åŒ…å« /v1 æ—¶æ·»åŠ 
        if not base.endswith('/v1'):
            base = base + '/v1'
        
        url = f"{base}/{endpoint}"
        return url
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº
        
        å…³é—­ HTTP å®¢æˆ·ç«¯è¿æ¥ï¼Œé‡Šæ”¾èµ„æºã€‚
        åº”åœ¨ç¨‹åºé€€å‡ºæˆ–æœåŠ¡åœæ­¢æ—¶è°ƒç”¨ã€‚
        """
        if self._http_client:
            try:
                await self._http_client.aclose()
                self.logger.debug("HTTP å®¢æˆ·ç«¯å·²å…³é—­")
            except Exception as e:
                self.logger.warning(f"å…³é—­ HTTP å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
            finally:
                self._http_client = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ï¼Œè‡ªåŠ¨æ¸…ç†èµ„æº"""
        await self.cleanup()
    async def process_input(self, state: AgentState) -> AgentState:
        """å¤„ç†å’ŒéªŒè¯ç”¨æˆ·è¾“å…¥
        
        è¿™æ˜¯å¯¹è¯å¤„ç†æµç¨‹çš„å…¥å£èŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. éªŒè¯è¾“å…¥ä¸ä¸ºç©º
        2. åˆ›å»ºç”¨æˆ·æ¶ˆæ¯å¯¹è±¡
        3. åˆæ­¥è¯†åˆ«ç”¨æˆ·æ„å›¾
        4. æ›´æ–°çŠ¶æ€å‡†å¤‡è°ƒç”¨ LLM
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        """
        session_id = state.get('session_id', 'unknown')
        
        try:
            self.logger.debug(f"å¤„ç†ä¼šè¯ {session_id} çš„è¾“å…¥")
            
            # ğŸ†• æ€è€ƒé˜¶æ®µï¼šéªŒè¯è¾“å…¥
            if self.trace:
                # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ yieldï¼Œå› ä¸º process_input æ˜¯åŒæ­¥è¿”å› state çš„
                # æˆ‘ä»¬åªè®°å½•æ—¥å¿—ï¼ŒçœŸæ­£çš„äº‹ä»¶åœ¨ Graph å±‚å‘å°„
                self.logger.debug(f"[Trace] process_input: éªŒè¯ç”¨æˆ·è¾“å…¥")
            
            # æ›´æ–°æ—¶é—´æˆ³
            state["last_activity"] = datetime.now()
            
            # è§„èŒƒåŒ–è¾“å…¥ï¼Œç¡®ä¿ä¸ä¸ºç©º
            user_input = state["user_input"].strip()
            if not user_input:
                state["error_state"] = "empty_input"
                state["should_continue"] = False
                state["agent_response"] = "æˆ‘æ²¡æœ‰æ”¶åˆ°ä»»ä½•è¾“å…¥ï¼Œè¯·è¯´ç‚¹ä»€ä¹ˆå§ã€‚"
                state["next_action"] = "format_response"
                return state
            
            # âš¡ å¿«é€Ÿå“åº”ï¼šæ£€æµ‹ç®€å•é—®å€™ï¼ˆä¼˜åŒ–ï¼šè·³è¿‡ LLM è°ƒç”¨ï¼‰
            if self._is_simple_greeting(user_input):
                self.logger.info(f"ğŸš€ æ£€æµ‹åˆ°ç®€å•é—®å€™ï¼Œå¿«é€Ÿå“åº”ï¼ˆè·³è¿‡ LLMï¼‰")
                state["agent_response"] = self._get_greeting_response()
                state["next_action"] = "format_response"
                state["current_intent"] = "greeting"
                
                # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†å²
                user_message = ConversationMessage(
                    id=f"user_{len(state['messages']) + 1}_{int(datetime.now().timestamp())}",
                    role=MessageRole.USER,
                    content=user_input,
                    metadata={"processed_at": datetime.now().isoformat(), "fast_path": True}
                )
                state["messages"].append(user_message)
                
                return state
            
            # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²
            user_message = ConversationMessage(
                id=f"user_{len(state['messages']) + 1}_{int(datetime.now().timestamp())}",
                role=MessageRole.USER,
                content=user_input,
                metadata={"processed_at": datetime.now().isoformat()}
            )
            state["messages"].append(user_message)
            
            # é€šè¿‡å…³é”®è¯åˆæ­¥è¯†åˆ«ç”¨æˆ·æ„å›¾
            state["current_intent"] = self._analyze_intent(user_input)
            
            # è®¾ç½®ä¸‹ä¸€æ­¥åŠ¨ä½œï¼šè°ƒç”¨ LLM
            state["next_action"] = "call_llm"
            
            self.logger.debug(f"è¾“å…¥å¤„ç†å®Œæˆï¼Œæ„å›¾: {state['current_intent']}")
            return state
            
        except Exception as e:
            self.logger.error(f"è¾“å…¥å¤„ç†é”™è¯¯: {e}")
            state["error_state"] = f"input_processing_error: {str(e)}"
            state["should_continue"] = False
            return state
    
    async def call_llm(self, state: AgentState) -> AgentState:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå“åº”
        
        è¿™æ˜¯æ ¸å¿ƒå¤„ç†èŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. å‡†å¤‡å¯¹è¯å†å²æ¶ˆæ¯
        2. é…ç½®æ¨¡å‹å‚æ•°
        3. è°ƒç”¨ LLM API
        4. å¤„ç†å“åº”ï¼ˆæ–‡æœ¬æˆ–å·¥å…·è°ƒç”¨ï¼‰
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        """
        try:
            self.logger.debug(f"ä¸ºä¼šè¯ {state['session_id']} è°ƒç”¨ LLM")
            
            # å‡†å¤‡ LLM æ¶ˆæ¯ï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰
            # å¦‚æœ state ä¸­æœ‰ external_historyï¼Œä¼ é€’ç»™ _prepare_llm_messages
            external_history = state.get("external_history")
            if external_history is not None:
                self.logger.info(f"ğŸ” Found external_history in state: {len(external_history)} messages")
            else:
                self.logger.warning(f"âš ï¸ No external_history found in state for session {state['session_id']}")
            messages = self._prepare_llm_messages(state, external_history=external_history)
            
            # é…ç½®æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨å…¼å®¹å±‚å¤„ç†ä¸åŒæ¨¡å‹çš„å‚æ•°å·®å¼‚ï¼‰
            model = state["model_config"].get("model", self.config.llm.models.default)
            max_tokens = state.get("max_tokens", self.config.llm.max_tokens)
            temperature = state.get("temperature", self.config.llm.temperature)
            
            llm_config = prepare_llm_params(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # è°ƒç”¨ LLM API è·å–å“åº”
            response = await self._make_llm_call(messages, llm_config)
            
            # åˆ¤æ–­å“åº”ç±»å‹ï¼šå·¥å…·è°ƒç”¨ or ç›´æ¥å›å¤
            if self._has_tool_calls(response):
                state["next_action"] = "handle_tools"
                # æå–å·¥å…·è°ƒç”¨è¯·æ±‚å¹¶åŠ å…¥å¾…å¤„ç†é˜Ÿåˆ—
                tool_calls = self._extract_tool_calls(response)
                state["pending_tool_calls"].extend(tool_calls)
            else:
                state["next_action"] = "format_response"
                state["agent_response"] = response.get("content", "")
            
            self.logger.debug("LLM è°ƒç”¨å®Œæˆ")
            return state
            
        except Exception as e:
            self.logger.error(f"LLM è°ƒç”¨é”™è¯¯: {e}")
            state["error_state"] = f"llm_call_error: {str(e)}"
            state["agent_response"] = "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
            state["next_action"] = "format_response"
            return state
    
    async def handle_tools(self, state: AgentState) -> AgentState:
        """å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚
        
        ğŸ†• ä¼˜åŒ–ç‰ˆ: æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
        
        å½“ LLM éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œæ­¤èŠ‚ç‚¹è´Ÿè´£ï¼š
        1. æ‰§è¡Œæ‰€æœ‰å¾…å¤„ç†çš„å·¥å…·è°ƒç”¨
        2. æ”¶é›†å·¥å…·æ‰§è¡Œç»“æœ
        3. å°†ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
        4. ğŸ†• å¢åŠ å·¥å…·è°ƒç”¨è®¡æ•°å™¨
        5. å‡†å¤‡å†æ¬¡è°ƒç”¨ LLMï¼ˆè®©å®ƒåŸºäºå·¥å…·ç»“æœé‡æ–°æ€è€ƒï¼‰
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        """
        try:
            self.logger.debug(f"å¤„ç†ä¼šè¯ {state['session_id']} çš„å·¥å…·è°ƒç”¨")
            
            if not state["pending_tool_calls"]:
                self.logger.warning("æ²¡æœ‰å¾…å¤„ç†çš„å·¥å…·è°ƒç”¨")
                state["next_action"] = "call_llm"
                return state
            
            # ğŸ†• å¢åŠ å·¥å…·è°ƒç”¨è®¡æ•°
            state["tool_call_count"] = state.get("tool_call_count", 0) + 1
            current_iteration = state["tool_call_count"]
            
            self.logger.info(f"ğŸ”§ ç¬¬ {current_iteration} è½®å·¥å…·è°ƒç”¨ï¼Œå¾…æ‰§è¡Œå·¥å…·æ•°: {len(state['pending_tool_calls'])}")
            
            # âš¡ å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨ï¼ˆä¼˜åŒ–ï¼šä»ä¸²è¡Œæ”¹ä¸ºå¹¶è¡Œï¼‰
            tool_tasks = [
                self._execute_tool_call(tool_call) 
                for tool_call in state["pending_tool_calls"]
            ]
            results = await asyncio.gather(*tool_tasks, return_exceptions=True)
            
            # å¤„ç†æ‰§è¡Œç»“æœ
            for tool_call, result in zip(state["pending_tool_calls"], results):
                # å¦‚æœå·¥å…·æ‰§è¡ŒæŠ›å‡ºå¼‚å¸¸ï¼Œè½¬æ¢ä¸ºå¤±è´¥ç»“æœ
                if isinstance(result, Exception):
                    self.logger.error(f"  âŒ å·¥å…· '{tool_call.name}' æ‰§è¡Œå¼‚å¸¸: {str(result)}")
                    result = ToolResult(
                        call_id=tool_call.id,
                        success=False,
                        result=None,
                        error=f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(result)}"
                    )
                
                state["tool_results"].append(result)
                state["tool_calls"].append(tool_call)
                
                status_icon = "âœ…" if result.success else "âŒ"
                self.logger.info(f"  {status_icon} å·¥å…· '{tool_call.name}' æ‰§è¡Œå®Œæˆ: {result.success}")
            
            # æ¸…ç©ºå¾…å¤„ç†é˜Ÿåˆ—
            state["pending_tool_calls"] = []
            
            # å°†å·¥å…·æ‰§è¡Œç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
            for result in state["tool_results"][-len(state["tool_calls"]):]:
                tool_message = ConversationMessage(
                    id=f"tool_{result.call_id}_{int(datetime.now().timestamp())}",
                    role=MessageRole.TOOL,
                    content=json.dumps(result.dict(), cls=DateTimeJSONEncoder),
                    metadata={"tool_call_id": result.call_id, "success": result.success}
                )
                state["messages"].append(tool_message)
            
            # ğŸ†• æ ¸å¿ƒæ”¹åŠ¨: å·¥å…·è°ƒç”¨åè¿”å› LLM è¿›è¡Œé‡æ–°æ€è€ƒ
            # LLM ä¼šåŸºäºå·¥å…·ç»“æœåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´å¤šå·¥å…·æˆ–ç›´æ¥ç”Ÿæˆå›å¤
            state["next_action"] = "call_llm"
            
            self.logger.info(f"ğŸ”„ ç¬¬ {current_iteration} è½®å·¥å…·è°ƒç”¨å®Œæˆï¼Œè¿”å› LLM é‡æ–°è¯„ä¼°")
            return state
            
        except Exception as e:
            self.logger.error(f"å·¥å…·å¤„ç†é”™è¯¯: {e}")
            state["error_state"] = f"tool_handling_error: {str(e)}"
            state["agent_response"] = "æŠ±æ­‰ï¼Œåœ¨ä½¿ç”¨å·¥å…·æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œè®©æˆ‘æ¢ä¸ªæ–¹å¼å¸®æ‚¨ã€‚"
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¿”å› LLM è®©å®ƒç”Ÿæˆ fallback å›å¤
            state["next_action"] = "call_llm"
            return state
    
    async def format_response(self, state: AgentState) -> AgentState:
        """æ ¼å¼åŒ–æœ€ç»ˆå“åº”
        
        è¿™æ˜¯æµç¨‹çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. ç¡®ä¿æœ‰å“åº”å†…å®¹
        2. åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å¯¹è±¡
        3. æ·»åŠ å…ƒæ•°æ®
        4. æ ‡è®°å¯¹è¯å›åˆç»“æŸ
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æœ€ç»ˆçš„å¯¹è¯çŠ¶æ€
        """
        try:
            self.logger.debug(f"æ ¼å¼åŒ–ä¼šè¯ {state['session_id']} çš„å“åº”")
            
            # ç¡®ä¿æœ‰å“åº”å†…å®¹
            if not state["agent_response"]:
                if state["error_state"]:
                    state["agent_response"] = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ã€‚"
                else:
                    state["agent_response"] = "æˆ‘ä¸å¤ªç¡®å®šå¦‚ä½•å›ç­”ï¼Œè¯·æ¢ä¸ªæ–¹å¼é—®æˆ‘å§ã€‚"
            
            # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å¹¶æ·»åŠ åˆ°å†å²
            assistant_message = ConversationMessage(
                id=f"assistant_{len(state['messages']) + 1}_{int(datetime.now().timestamp())}",
                role=MessageRole.ASSISTANT,
                content=state["agent_response"],
                metadata={
                    "generated_at": datetime.now().isoformat(),
                    "model": state["model_config"].get("model", "unknown"),
                    "intent": state.get("current_intent"),
                    "tool_calls_count": len(state["tool_calls"])
                }
            )
            state["messages"].append(assistant_message)
            
            # æ›´æ–°æ´»åŠ¨æ—¶é—´æˆ³
            state["last_activity"] = datetime.now()
            
            # æ ‡è®°å¯¹è¯å›åˆå®Œæˆ
            state["should_continue"] = False
            state["next_action"] = None
            
            self.logger.debug("å“åº”æ ¼å¼åŒ–å®Œæˆ")
            return state
            
        except Exception as e:
            self.logger.error(f"å“åº”æ ¼å¼åŒ–é”™è¯¯: {e}")
            state["error_state"] = f"response_formatting_error: {str(e)}"
            state["agent_response"] = "æŠ±æ­‰ï¼Œå“åº”æ ¼å¼åŒ–æ—¶å‡ºç°äº†é—®é¢˜ã€‚"
            state["should_continue"] = False
            return state
    
    def _analyze_intent(self, user_input: str) -> Optional[str]:
        """åˆ†æç”¨æˆ·æ„å›¾ï¼ˆåŸºäºå…³é”®è¯çš„ç®€å•å®ç°ï¼‰
        
        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„æ„å›¾è¯†åˆ«ï¼Œä»…ç”¨äºåŸºç¡€åˆ†ç±»ã€‚
        ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ NLU æ¨¡å‹æˆ– LLM è¿›è¡Œæ„å›¾è¯†åˆ«ã€‚
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        
        Returns:
            è¯†åˆ«çš„æ„å›¾æ ‡ç­¾ï¼Œå¦‚ "search", "calculation" ç­‰
        """
        input_lower = user_input.lower()
        
        # åŸºäºå…³é”®è¯çš„ç®€å•æ„å›¾æ£€æµ‹
        if any(word in input_lower for word in ["search", "find", "look", "æœç´¢", "æŸ¥æ‰¾"]):
            return "search"
        elif any(word in input_lower for word in ["calculate", "math", "compute", "è®¡ç®—"]):
            return "calculation"
        elif any(word in input_lower for word in ["time", "date", "when", "æ—¶é—´", "æ—¥æœŸ"]):
            return "time_query"
        elif any(word in input_lower for word in ["image", "picture", "generate", "create", "å›¾ç‰‡", "ç”Ÿæˆ"]):
            return "image_generation"
        elif any(word in input_lower for word in ["help", "what", "how", "å¸®åŠ©", "æ€ä¹ˆ"]):
            return "help_request"
        else:
            return "general_conversation"
    
    def _prepare_llm_messages(self, state: AgentState, external_history: List[Dict] = None) -> List[Dict[str, str]]:
        """å‡†å¤‡ LLM API è°ƒç”¨çš„æ¶ˆæ¯åˆ—è¡¨
        
        åŒ…å«ä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯å’Œå†å²å¯¹è¯ã€‚ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„å†å²è®°å½•ã€‚
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
            external_history: å¤–éƒ¨ä¼ å…¥çš„å†å²æ¶ˆæ¯åˆ—è¡¨ (å¯é€‰)
        
        Returns:
            æ ¼å¼åŒ–çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œç¬¦åˆ OpenAI API æ ¼å¼
        """
        messages = []
        
        # æ„å»ºä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯
        system_prompt = self._build_optimized_system_prompt(state)
        system_message = {
            "role": "system",
            "content": system_prompt
        }
        messages.append(system_message)
        
        # ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨å†å²ï¼ˆä» SessionHistoryManagerï¼‰
        if external_history is not None:
            # é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡ï¼ˆæœ€è¿‘ 10 æ¡ï¼‰
            MAX_HISTORY_MESSAGES = 10
            recent_history = external_history[-MAX_HISTORY_MESSAGES:] if external_history else []
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            self.logger.info(f"âœ… Loaded {len(recent_history)} messages from external history for LLM")
            
            # é‡è¦ï¼šæ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ¥è‡ª state["messages"] çš„æœ€åä¸€æ¡ï¼‰
            if state["messages"] and state["messages"][-1].role == MessageRole.USER:
                current_user_msg = state["messages"][-1]
                messages.append({
                    "role": "user",
                    "content": current_user_msg.content
                })
                self.logger.info(f"âœ… Added current user message to LLM input")
        else:
            # å›é€€åˆ° state ä¸­çš„æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            self.logger.info("âš ï¸ No external history provided, using state messages")
            MAX_HISTORY_MESSAGES = 10
            recent_messages = state["messages"][-MAX_HISTORY_MESSAGES:]
            for msg in recent_messages:
                if msg.role in [MessageRole.USER, MessageRole.ASSISTANT]:
                    messages.append({
                        "role": msg.role.value,
                        "content": msg.content
                    })
            self.logger.info(f"ğŸ“ Using {len(recent_messages)} messages from state")
        
        return messages
    
    def _build_optimized_system_prompt(self, state: AgentState) -> str:
        """æ„å»ºä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆä½¿ç”¨é¢„æ„å»ºåŸºç¡€ + åŠ¨æ€ä¸Šä¸‹æ–‡ï¼‰
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä½¿ç”¨é¢„æ„å»ºçš„åŸºç¡€æç¤ºè¯ï¼ˆé™æ€éƒ¨åˆ†ï¼‰
        2. ä»…æ·»åŠ åŠ¨æ€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        3. å‡å°‘å­—ç¬¦ä¸²æ‹¼æ¥å¼€é”€
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            å®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
        """
        # ä½¿ç”¨é¢„æ„å»ºçš„åŸºç¡€æç¤ºè¯
        prompt_parts = [self._base_system_prompt]
        
        # æ·»åŠ åŠ¨æ€ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_additions = []
        
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨å†å²ï¼Œæ·»åŠ æç¤º
        if state.get("tool_calls"):
            tool_count = len(state["tool_calls"])
            context_additions.append(f"\n## å½“å‰ä¸Šä¸‹æ–‡\n\n- å·²æ‰§è¡Œ {tool_count} æ¬¡å·¥å…·è°ƒç”¨")
        
        # å¦‚æœç”¨æˆ·æœ‰æ˜ç¡®æ„å›¾ï¼Œæ·»åŠ æç¤º
        current_intent = state.get("current_intent")
        if current_intent and current_intent != "general":
            intent_hints = {
                "search": "ç”¨æˆ·éœ€è¦æœç´¢ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ web_search å·¥å…·",
                "calculation": "ç”¨æˆ·éœ€è¦è®¡ç®—ï¼Œä½¿ç”¨ calculator å·¥å…·",
                "time_query": "ç”¨æˆ·è¯¢é—®æ—¶é—´ï¼Œä½¿ç”¨ get_time å·¥å…·",
                "weather": "ç”¨æˆ·è¯¢é—®å¤©æ°”ï¼Œä½¿ç”¨ get_weather å·¥å…·"
            }
            if current_intent in intent_hints:
                context_additions.append(f"- {intent_hints[current_intent]}")
        
        # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
        if context_additions:
            prompt_parts.append("\n".join(context_additions))
        
        return "\n\n".join(prompt_parts)
    
    def _get_tools_schema(self) -> List[Dict]:
        """è·å–å·¥å…·çš„ OpenAI Function Calling æ ¼å¼å®šä¹‰
        
        Returns:
            å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ŒOpenAI tools æ ¼å¼
        """
        try:
            from mcp import get_tool_registry
            registry = get_tool_registry()
            tools = registry.list_tools()
            
            if not tools:
                return []
            
            # è½¬æ¢ä¸º OpenAI Function Calling æ ¼å¼
            tools_schema = []
            for tool in tools:
                schema = tool.to_openai_schema()
                tools_schema.append(schema)
            
            self.logger.info(f"âœ… Loaded {len(tools_schema)} tools for LLM")
            return tools_schema
        except Exception as e:
            self.logger.error(f"âŒ Failed to load tools schema: {e}", exc_info=True)
            return []
    
    def _format_available_tools(self) -> str:
        """æ ¼å¼åŒ–å¯ç”¨å·¥å…·åˆ—è¡¨ä¸ºæ˜“è¯»çš„æ–‡æœ¬
        
        Returns:
            æ ¼å¼åŒ–çš„å·¥å…·åˆ—è¡¨å­—ç¬¦ä¸²
        """
        try:
            from mcp import get_tool_registry
            registry = get_tool_registry()
            tools = registry.list_tools()
            
            if not tools:
                return "å½“å‰æš‚æ— å¯ç”¨å·¥å…·ã€‚"
            
            tool_descriptions = []
            for tool in tools:
                name = tool.name
                desc = tool.description
                # ç®€åŒ–æè¿°ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯
                short_desc = desc.split('.')[0] if desc else "æ— æè¿°"
                tool_descriptions.append(f"- **{name}**: {short_desc}")
            
            return "\n".join(tool_descriptions)
        
        except Exception as e:
            self.logger.warning(f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
            return "- **calculator**: æ‰§è¡Œæ•°å­¦è®¡ç®—\n- **get_time**: è·å–å½“å‰æ—¶é—´\n- **get_weather**: æŸ¥è¯¢å¤©æ°”ä¿¡æ¯\n- **web_search**: æœç´¢ç½‘ç»œä¿¡æ¯"
    
    def _build_context_aware_addition(self, state: AgentState) -> str:
        """æ ¹æ®å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡æ„å»ºé¢å¤–çš„æç¤ºè¯å¢å¼º
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            ä¸Šä¸‹æ–‡ç›¸å…³çš„é¢å¤–æç¤ºè¯ï¼Œå¦‚æœä¸éœ€è¦åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        additions = []
        
        # 1. å¦‚æœæœ‰å·¥å…·è°ƒç”¨å†å²ï¼Œæé†’åŸºäºç»“æœå›ç­”
        if state.get("tool_calls") and len(state["tool_calls"]) > 0:
            additions.append(
                """# âš ï¸ Current Context: Tool Results Available

You have just executed tool(s) and received results. **CRITICAL REMINDER**:

âœ… **You MUST**:
- Base your response ENTIRELY on the actual tool results data
- Parse and present the tool response properly (especially for web_search)
- Follow the mandatory search results protocol if it was a web_search call
- Extract and display: ai_answer, titles, snippets, urls from the results
- Format everything in proper Markdown structure

âŒ **You MUST NOT**:
- Fabricate or guess information not in the tool results
- Return tool parameters (e.g., `{"query": "...", "num_results": 8}`) as if they were results
- Say "Found X results" without showing the actual content
- Ignore the structured data in the tool response

**If tool results are incomplete or unclear**: Explicitly inform the user about limitations."""
            )
        
        # 2. å¦‚æœå¯¹è¯è½®æ¬¡è¾ƒå¤šï¼Œæé†’ä¿æŒè¿è´¯æ€§
        message_count = len(state.get("messages", []))
        if message_count > 6:
            additions.append(
                """# ğŸ’¬ Conversation Continuity

This is a multi-turn conversation (6+ messages). Please:
- Maintain context consistency across turns
- Recognize pronouns like "it", "this", "that" referring to previous topics
- Reference earlier discussion points when relevant
- Don't repeat information already established in the conversation"""
            )
        
        # 3. å¦‚æœæ£€æµ‹åˆ°ç‰¹å®šæ„å›¾ï¼Œç»™å‡ºé’ˆå¯¹æ€§æŒ‡å¯¼
        intent = state.get("current_intent")
        user_input = state.get("user_input", "").lower()
        
        # æ£€æµ‹æœç´¢æ„å›¾
        search_keywords = ["search", "find", "latest", "news", "æœç´¢", "æŸ¥æ‰¾", "æœ€æ–°", "æ–°é—»", "æŸ¥è¯¢"]
        if intent == "search" or any(keyword in user_input for keyword in search_keywords):
            additions.append(
                """# ğŸ” Search Task Optimization

User is requesting information search. **Enhanced Protocol**:

**Step 1: Tool Execution**
- Use `web_search` with precise query (English for international topics, Chinese for local topics)
- Set `num_results` to 5-8 for optimal balance

**Step 2: Result Processing (MANDATORY)**
Parse the tool response JSON structure:
```json
{
  "ai_answer": "Use this as executive summary if valuable",
  "results": [
    {"title": "...", "snippet": "...", "url": "...", "score": 0.95}
  ]
}
```

**Step 3: Response Formatting (STRICT)**
```markdown
## ğŸ” Search Results: [Topic]

### ğŸ“Š Executive Summary
[Present ai_answer here, or synthesize from top results]

### ğŸ“° Detailed Findings
1. **[Title 1]**
   - ğŸ“ [Key points from snippet]
   - ğŸ”— [Title](URL)

2. **[Title 2]** ...

---
ğŸ’¡ **Key Insight**: [Your analysis]
```

**Quality Checklist**:
- [ ] ai_answer used as summary (if present)
- [ ] 3-5 results shown with title + snippet + clickable URL
- [ ] Markdown structure with headers and lists
- [ ] Time-sensitive info includes dates
- [ ] Added synthesis or insight beyond raw data

**Common Error to Avoid**:
âŒ Do NOT just output: "Found 8 search results about Trump's Japan visit"
âœ… DO output: Structured results with actual titles, snippets, and links"""
            )
        
        # æ£€æµ‹è®¡ç®—æ„å›¾
        elif intent == "calculation" or any(op in user_input for op in ["+", "-", "*", "/", "calculate", "è®¡ç®—"]):
            additions.append(
                """# ğŸ§® Calculation Task

User needs mathematical computation:
- Use `calculator` tool for complex expressions or to ensure precision
- Show both the expression and result clearly
- Format: "Calculating `expression` = **result**"
- For very simple math (e.g., 2+2), you can answer directly
- For decimals, powers, trigonometry, always use the tool for accuracy"""
            )
        
        # æ£€æµ‹æ—¶é—´æŸ¥è¯¢
        elif "time" in user_input or "date" in user_input or "æ—¶é—´" in user_input or "æ—¥æœŸ" in user_input or "å‡ ç‚¹" in user_input:
            additions.append(
                """# ğŸ• Time/Date Query

User is asking about current time or date:
- Use `get_time` tool with appropriate format parameter
- Present time in user-friendly format with timezone context
- For "what time is it": use format="full"
- For "what date": use format="date"
- For "timestamp": use format="timestamp"
- Always clarify the timezone in your response"""
            )
        
        return "\n\n".join(additions) if additions else ""

    
    async def _make_llm_call(self, messages: List[Dict[str, str]], config: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨ LLM APIï¼ˆOpenAI å…¼å®¹ï¼‰
        
        å¦‚æœçœŸå® HTTP è°ƒç”¨å¤±è´¥ï¼Œä¼šä½¿ç”¨åŸºäºå…³é”®è¯çš„å¯å‘å¼ fallbackã€‚
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            config: LLM é…ç½®å‚æ•°
        
        Returns:
            åŒ…å« content (str) å’Œ tool_calls (list) çš„å­—å…¸
        """
        user_message = messages[-1]["content"] if messages else ""

        # ==== ä¸»è¦è·¯å¾„ï¼šçœŸå® HTTP è°ƒç”¨ ====
        try:
            # ç¡®ä¿ HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            await self._ensure_http_client()

            # è·å–å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰
            self.logger.info(f"ğŸ” Attempting to load tools schema...")
            tools_schema = self._get_tools_schema()
            self.logger.info(f"ğŸ” Tools schema loaded: {len(tools_schema) if tools_schema else 0} tools")
            
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                tools=tools_schema if tools_schema else None  # ä¼ é€’å·¥å…·å®šä¹‰
            )
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - LLM è¯·æ±‚å‚æ•°
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¤ LLM API è¯·æ±‚å‚æ•°:")
            self.logger.info(f"  Model: {payload.get('model')}")
            self.logger.info(f"  Max Tokens: {payload.get('max_tokens') or payload.get('max_completion_tokens')}")
            self.logger.info(f"  Temperature: {payload.get('temperature', 'N/A (æ¨¡å‹é»˜è®¤)')}")
            self.logger.info(f"  Messages Count: {len(messages)}")
            self.logger.info(f"  Tools Count: {len(tools_schema) if tools_schema else 0}")
            
            # ä¼°ç®—è¾“å…¥ token æ•°ï¼ˆç²—ç•¥ä¼°è®¡ï¼šä¸­æ–‡ ~1.5 å­—ç¬¦/tokenï¼Œè‹±æ–‡ ~4 å­—ç¬¦/tokenï¼‰
            total_chars = sum(len(str(m.get('content', ''))) for m in messages)
            estimated_input_tokens = int(total_chars / 2)  # ä¿å®ˆä¼°è®¡
            self.logger.info(f"  ä¼°ç®—è¾“å…¥ Tokens: ~{estimated_input_tokens}")
            self.logger.info("=" * 60)
            
            # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ  tool_choice
            if tools_schema:
                payload["tool_choice"] = "auto"  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
                self.logger.info(f"ğŸ”§ Added {len(tools_schema)} tools to LLM request")
            else:
                self.logger.warning(f"âš ï¸ No tools available for LLM request")

            # æ„å»ºå®Œæ•´ URL
            url = self._build_llm_url()
            self.logger.debug(f"LLM è°ƒç”¨: {url}")

            # å‘é€è¯·æ±‚
            resp = await self._http_client.post(url, json=payload)
            if resp.status_code >= 400:
                error_text = resp.text[:500]
                self.logger.error(f"LLM HTTP {resp.status_code}: {error_text}")
                raise RuntimeError(f"LLM HTTP {resp.status_code}: {error_text}")
            
            data = resp.json()
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - LLM å“åº”ä¿¡æ¯
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¥ LLM API å“åº”:")
            self.logger.info(f"  Choices Count: {len(data.get('choices', []))}")
            
            # æå–å…³é”®ä¿¡æ¯
            choices = data.get("choices", [])
            if choices:
                first = choices[0]
                finish_reason = first.get("finish_reason", "unknown")
                message_obj = first.get("message", {})
                content = message_obj.get("content") or ""
                tool_calls_raw = message_obj.get("tool_calls") or []
                
                # âš ï¸ å…³é”®è¯Šæ–­ç‚¹ï¼šfinish_reason
                self.logger.info(f"  â­ Finish Reason: {finish_reason}")
                if finish_reason == "length":
                    self.logger.warning("  âŒ å“åº”è¢«æˆªæ–­ï¼åŸå› : max_tokens é™åˆ¶")
                    self.logger.warning("  ğŸ’¡ å»ºè®®: å¢åŠ  max_tokens æˆ–å‡å°‘è¾“å…¥é•¿åº¦")
                elif finish_reason == "stop":
                    self.logger.info("  âœ… å“åº”å®Œæ•´ï¼ˆæ­£å¸¸ç»“æŸï¼‰")
                elif finish_reason == "tool_calls":
                    self.logger.info("  ğŸ”§ å“åº”ç±»å‹: å·¥å…·è°ƒç”¨")
                
                self.logger.info(f"  Content Length: {len(content)} å­—ç¬¦")
                self.logger.info(f"  Tool Calls Count: {len(tool_calls_raw)}")
                
                # æ˜¾ç¤º usage ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                usage = data.get("usage", {})
                if usage:
                    self.logger.info(f"  Token Usage:")
                    self.logger.info(f"    - Prompt Tokens: {usage.get('prompt_tokens', 'N/A')}")
                    self.logger.info(f"    - Completion Tokens: {usage.get('completion_tokens', 'N/A')}")
                    self.logger.info(f"    - Total Tokens: {usage.get('total_tokens', 'N/A')}")
                
                # æ˜¾ç¤ºå“åº”å†…å®¹å‰ 200 å­—ç¬¦ï¼ˆç”¨äºéªŒè¯ï¼‰
                if content:
                    preview = content[:200] + ("..." if len(content) > 200 else "")
                    self.logger.info(f"  Content Preview: {preview}")
                
                self.logger.info("=" * 60)
                
                # è§„èŒƒåŒ–å·¥å…·è°ƒç”¨æ ¼å¼
                tool_calls = []
                for tc in tool_calls_raw:
                    if tc.get("type") == "function":
                        fn = tc.get("function", {})
                        tool_calls.append({
                            "id": tc.get("id") or f"tool_{int(datetime.now().timestamp())}",
                            "type": "function",
                            "function": {
                                "name": fn.get("name"),
                                "arguments": fn.get("arguments", "{}")
                            }
                        })
                return {"content": content, "tool_calls": tool_calls}

            # å“åº”ç»“æ„å¼‚å¸¸æ—¶çš„ fallback
            return {"content": content if 'content' in locals() else "", "tool_calls": []}

        except Exception as e:
            self.logger.error(f"LLM çœŸå®è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼ fallback: {e}", exc_info=True)
            self.logger.error(f"LLM é…ç½® - Base URL: {self.config.llm.base_url}")
            self.logger.error(f"LLM é…ç½® - Model: {config.get('model', self.config.llm.models.default)}")
            self.logger.error(f"LLM é…ç½® - API Key å·²è®¾ç½®: {bool(self.config.llm.api_key)}")

        # ==== Fallback å¯å‘å¼é€»è¾‘ ====
        if "search" in user_message.lower() or "æœç´¢" in user_message:
            return {
                "content": "",
                "tool_calls": [
                    {
                        "id": "search_1",
                        "type": "function",
                        "function": {
                            "name": "search_tool",
                            "arguments": json.dumps({"query": user_message})
                        }
                    }
                ]
            }
        if "calculate" in user_message.lower() or "è®¡ç®—" in user_message or any(char in user_message for char in "+-*/"):
            return {
                "content": "",
                "tool_calls": [
                    {
                        "id": "calc_1",
                        "type": "function",
                        "function": {
                            "name": "calculator",
                            "arguments": json.dumps({"expression": user_message})
                        }
                    }
                ]
            }
        
        # é»˜è®¤ fallback å“åº”
        return {
            "content": f"æˆ‘ç†è§£æ‚¨è¯´çš„æ˜¯ï¼š'{user_message}'",
            "tool_calls": []
        }

    async def stream_llm_call(self, messages: List[Dict[str, str]], config: Dict[str, Any], session_id: Optional[str] = None):
        """æµå¼è°ƒç”¨ LLM å¹¶ç”Ÿæˆå¢é‡å“åº”äº‹ä»¶ã€‚

        ä»¥æµå¼æ–¹å¼è°ƒç”¨ LLM API,é€æ­¥ç”Ÿæˆå“åº”å†…å®¹ã€‚è¿”å›ç‰ˆæœ¬åŒ–äº‹ä»¶,åŒ…å«äº‹ä»¶ IDã€æ—¶é—´æˆ³ç­‰å…ƒæ•°æ®ã€‚
        å¦‚æœæµå¼è°ƒç”¨å¤±è´¥,è‡ªåŠ¨ fallback åˆ°éæµå¼è°ƒç”¨ã€‚

        Args:
            messages: æ¶ˆæ¯å†å²åˆ—è¡¨
            config: LLM é…ç½®(model, temperature, max_tokens ç­‰)
            session_id: ä¼šè¯ ID(å¯é€‰)

        Yields:
            äº‹ä»¶å­—å…¸,åŒ…å« version, id, timestamp, type ç­‰å­—æ®µ:
            - start: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'start', model: '...'}
            - delta: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'delta', content: str}
            - tool_calls: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'tool_calls', tool_calls: [...]}
            - end: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'end', content: full_text}
            - error: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'error', error: msg}
        """
        # ğŸ†• æ€è€ƒé˜¶æ®µï¼šå‡†å¤‡è°ƒç”¨ LLM
        if self.trace and session_id:
            yield self.trace.thinking_phase("å‡†å¤‡ LLM æµå¼è°ƒç”¨", "call_llm", session_id)
        
        # å¯¼å…¥äº‹ä»¶å·¥å…·å‡½æ•°
        try:
            from api.event_utils import (
                create_start_event, create_delta_event, create_end_event,
                create_error_event, create_tool_calls_event
            )
        except ImportError:
            # å¦‚æœ event_utils ä¸å¯ç”¨,ä½¿ç”¨ fallback æ ¼å¼
            from datetime import datetime
            import uuid
            def create_start_event(sid=None, model=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "start"}
                if model: evt["model"] = model
                if sid: evt["session_id"] = sid
                return evt
            def create_delta_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "delta", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_end_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "end", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_error_event(error, sid=None, error_code=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "error", "error": error}
                if sid: evt["session_id"] = sid
                if error_code: evt["error_code"] = error_code
                return evt
            def create_tool_calls_event(tool_calls, sid=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "tool_calls", "tool_calls": tool_calls}
                if sid: evt["session_id"] = sid
                return evt
        
        user_message = messages[-1]["content"] if messages else ""
        full_text = []
        yielded_tool_calls = False
        model = config.get("model", self.config.llm.models.default)
        
        # å°è¯•æµå¼è°ƒç”¨
        try:
            # ç¡®ä¿ HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            await self._ensure_http_client()

            # è·å–å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰
            self.logger.info(f"ğŸ” [Stream] Loading tools schema for streaming mode...")
            tools_schema = self._get_tools_schema()
            self.logger.info(f"ğŸ” [Stream] Tools schema loaded: {len(tools_schema) if tools_schema else 0} tools")

            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                stream=True,
                tools=tools_schema if tools_schema else None  # ä¼ é€’å·¥å…·å®šä¹‰
            )
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - æµå¼è¯·æ±‚å‚æ•°
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¤ [STREAM] LLM API è¯·æ±‚å‚æ•°:")
            self.logger.info(f"  Model: {payload.get('model')}")
            self.logger.info(f"  Max Tokens: {payload.get('max_tokens') or payload.get('max_completion_tokens')}")
            self.logger.info(f"  Temperature: {payload.get('temperature', 'N/A')}")
            self.logger.info(f"  Messages Count: {len(messages)}")
            self.logger.info(f"  Tools Count: {len(tools_schema) if tools_schema else 0}")
            self.logger.info(f"  Stream Mode: True")
            
            # ä¼°ç®—è¾“å…¥ token
            total_chars = sum(len(str(m.get('content', ''))) for m in messages)
            estimated_input_tokens = int(total_chars / 2)
            self.logger.info(f"  ä¼°ç®—è¾“å…¥ Tokens: ~{estimated_input_tokens}")
            self.logger.info("=" * 60)
            
            # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ  tool_choice
            if tools_schema:
                payload["tool_choice"] = "auto"
                self.logger.info(f"ğŸ”§ [Stream] Added {len(tools_schema)} tools to streaming LLM request")
            else:
                self.logger.warning(f"âš ï¸ [Stream] No tools available for streaming LLM request")
            
            # ä½¿ç”¨æå–çš„ URL æ„å»ºæ–¹æ³•
            url = self._build_llm_url()
            
            self.logger.debug(f"LLM æµå¼è°ƒç”¨ç›®æ ‡: {url}")
            
            yield create_start_event(session_id=session_id, model=model)
            
            # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆæµå¼è¿”å›æ—¶å¯èƒ½åˆ†æ•£åœ¨å¤šä¸ª delta ä¸­ï¼‰
            collected_tool_calls = []
            
            async with self._http_client.stream('POST', url, json=payload) as resp:
                if resp.status_code >= 400:
                    text = await resp.aread()
                    raise RuntimeError(f"æµå¼ HTTP è¯·æ±‚å¤±è´¥ {resp.status_code}: {text[:200]}")
                async for line in resp.aiter_lines():
                    if not line:
                        continue
                    if line.startswith('data:'):
                        data_part = line[5:].strip()
                        if data_part == '[DONE]':
                            break
                        try:
                            data_json = json.loads(data_part)
                        except json.JSONDecodeError:
                            continue
                        for choice in data_json.get('choices', []):
                            delta = choice.get('delta', {})
                            
                            # å¤„ç†æ–‡æœ¬å†…å®¹
                            if 'content' in delta and delta['content']:
                                piece = delta['content']
                                full_text.append(piece)
                                yield create_delta_event(content=piece, session_id=session_id)
                            
                            # æ”¶é›†å·¥å…·è°ƒç”¨ï¼ˆOpenAI æµå¼æ ¼å¼ï¼‰
                            if 'tool_calls' in delta:
                                for tc_delta in delta['tool_calls']:
                                    idx = tc_delta.get('index', 0)
                                    # ç¡®ä¿ list è¶³å¤Ÿé•¿
                                    while len(collected_tool_calls) <= idx:
                                        collected_tool_calls.append({
                                            'id': None,
                                            'type': 'function',
                                            'function': {'name': '', 'arguments': ''}
                                        })
                                    
                                    # ç´¯ç§¯ id
                                    if 'id' in tc_delta:
                                        collected_tool_calls[idx]['id'] = tc_delta['id']
                                    
                                    # ç´¯ç§¯ function name
                                    if 'function' in tc_delta:
                                        fn = tc_delta['function']
                                        if 'name' in fn:
                                            collected_tool_calls[idx]['function']['name'] += fn['name']
                                        if 'arguments' in fn:
                                            collected_tool_calls[idx]['function']['arguments'] += fn['arguments']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if collected_tool_calls:
                self.logger.info(f"ğŸ”§ [Stream] Detected {len(collected_tool_calls)} tool call(s), executing...")
                
                # ğŸ†• æ€è€ƒé˜¶æ®µï¼šæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨
                if self.trace and session_id:
                    yield self.trace.llm_streaming("æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨", session_id, f"å…± {len(collected_tool_calls)} ä¸ªå·¥å…·")
                
                # é€šçŸ¥å‰ç«¯å·¥å…·è°ƒç”¨å¼€å§‹
                yield create_tool_calls_event(tool_calls=collected_tool_calls, session_id=session_id)
                
                # æ‰§è¡Œæ‰€æœ‰å·¥å…·
                tool_results = []
                for tc in collected_tool_calls:
                    try:
                        # è½¬æ¢ä¸º ToolCall å¯¹è±¡
                        tool_call = ToolCall(
                            id=tc.get('id') or f"tool_{int(datetime.now().timestamp())}",
                            name=tc['function']['name'],
                            arguments=json.loads(tc['function']['arguments']) if tc['function']['arguments'] else {}
                        )
                        
                        # ğŸ†• å·¥å…·è°ƒç”¨æ’é˜Ÿäº‹ä»¶
                        if self.trace and session_id:
                            yield self.trace.tool_call_pending(tool_call.name, tool_call.arguments, session_id)
                        
                        # ğŸ†• å·¥å…·æ‰§è¡Œä¸­äº‹ä»¶
                        if self.trace and session_id:
                            yield self.trace.tool_executing(tool_call.name, session_id)
                        
                        import time
                        tool_start_time = time.time()
                        
                        # æ‰§è¡Œå·¥å…·
                        result = await self._execute_tool_call(tool_call)
                        
                        tool_duration = (time.time() - tool_start_time) * 1000
                        
                        # æ ¼å¼åŒ–å·¥å…·ç»“æœå†…å®¹ï¼ˆä½¿ç”¨ result å±æ€§ï¼Œä¸æ˜¯ dataï¼‰
                        if result.success:
                            # result.result å¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²æˆ–å…¶ä»–ç±»å‹
                            if isinstance(result.result, str):
                                result_content = result.result
                            elif isinstance(result.result, (dict, list)):
                                result_content = json.dumps(result.result, ensure_ascii=False)
                            else:
                                result_content = str(result.result)
                        else:
                            result_content = f"Error: {result.error}"
                        
                        # ğŸ†• å·¥å…·ç»“æœäº‹ä»¶
                        if self.trace and session_id:
                            summary = result_content[:100] + "..." if len(result_content) > 100 else result_content
                            yield self.trace.tool_result(
                                tool_call.name, 
                                result.success, 
                                summary, 
                                session_id,
                                tool_duration
                            )
                        
                        tool_results.append({
                            'tool_call_id': tool_call.id,
                            'role': 'tool',
                            'name': tool_call.name,
                            'content': result_content
                        })
                        
                        self.logger.info(f"âœ… [Stream] Tool '{tool_call.name}' executed successfully, result length: {len(result_content)}")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ [Stream] Tool execution failed: {e}")
                        
                        # ğŸ†• å·¥å…·å¤±è´¥äº‹ä»¶
                        if self.trace and session_id:
                            yield self.trace.tool_result(
                                tc['function']['name'],
                                False,
                                f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                                session_id
                            )
                        
                        tool_results.append({
                            'tool_call_id': tc.get('id', 'unknown'),
                            'role': 'tool',
                            'name': tc['function']['name'],
                            'content': f"Error: {str(e)}"
                        })
                
                # ğŸ†• æ€è€ƒé˜¶æ®µï¼šåŸºäºå·¥å…·ç»“æœå†æ¬¡è°ƒç”¨ LLM
                if self.trace and session_id:
                    yield self.trace.llm_streaming("åŸºäºå·¥å…·ç»“æœé‡æ–°æ€è€ƒ", session_id)
                
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼Œå†æ¬¡è°ƒç”¨ LLMï¼ˆæµå¼ï¼‰
                self.logger.info(f"ğŸ”„ [Stream] Calling LLM again with tool results...")
                
                # æ„å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
                new_messages = messages + [
                    {
                        'role': 'assistant',
                        'content': None,
                        'tool_calls': collected_tool_calls
                    }
                ] + tool_results
                
                # è°ƒè¯•ï¼šæ‰“å°æ¶ˆæ¯ç»“æ„
                self.logger.info(f"ğŸ“‹ [Stream] Final message count: {len(new_messages)}")
                self.logger.info(f"ğŸ“‹ [Stream] Last 3 messages roles: {[m.get('role', 'unknown') for m in new_messages[-3:]]}")
                
                # é€’å½’è°ƒç”¨è‡ªå·±ï¼Œä½†ä¸ä¼ é€’å·¥å…·ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
                config_no_tools = config.copy()
                
                # é‡æ–°è°ƒç”¨ï¼ˆè¿™æ¬¡æ˜¯æµå¼è¿”å›å·¥å…·å¤„ç†åçš„ç»“æœï¼‰
                async for event in self._stream_llm_with_tool_results(new_messages, config_no_tools, session_id):
                    yield event
                
                return
            
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸ç»“æŸ
            final_content = ''.join(full_text)
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - æµå¼å“åº”æ€»ç»“
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¥ [STREAM] LLM æµå¼å“åº”æ€»ç»“:")
            self.logger.info(f"  Total Content Length: {len(final_content)} å­—ç¬¦")
            self.logger.info(f"  Delta Events Count: {len(full_text)}")
            self.logger.info(f"  Tool Calls: {len(collected_tool_calls)}")
            
            # æ˜¾ç¤ºå†…å®¹å‰ 200 å­—ç¬¦
            if final_content:
                preview = final_content[:200] + ("..." if len(final_content) > 200 else "")
                self.logger.info(f"  Content Preview: {preview}")
            
            # âš ï¸ æ³¨æ„ï¼šæµå¼å“åº”é€šå¸¸ä¸è¿”å› finish_reason
            # å¦‚æœå†…å®¹çœ‹èµ·æ¥è¢«æˆªæ–­ï¼ˆçªç„¶ç»“æŸï¼‰ï¼Œå¯èƒ½æ˜¯ max_tokens é™åˆ¶
            if len(final_content) > 5000:
                self.logger.warning("  âš ï¸ å“åº”å†…å®¹è¾ƒé•¿ï¼Œå¦‚æœçœ‹èµ·æ¥ä¸å®Œæ•´ï¼Œå¯èƒ½æ˜¯ max_tokens é™åˆ¶")
            
            self.logger.info("=" * 60)
            
            yield create_end_event(content=final_content, session_id=session_id)
            return
        except Exception as e:
            self.logger.warning(f"æµå¼è°ƒç”¨å¤±è´¥,å›é€€åˆ°éæµå¼: {e}")
            if not full_text:
                # å›é€€åˆ°æ™®é€šè°ƒç”¨
                try:
                    result = await self._make_llm_call(messages, config)
                    content = result.get('content', '')
                    yield create_delta_event(content=content, session_id=session_id)
                    if result.get('tool_calls'):
                        yield create_tool_calls_event(tool_calls=result['tool_calls'], session_id=session_id)
                    yield create_end_event(content=content, session_id=session_id)
                    return
                except Exception as e2:
                    yield create_error_event(error=str(e2), session_id=session_id)
                    return
            else:
                yield create_end_event(content=''.join(full_text), session_id=session_id)
    
    async def _stream_llm_with_tool_results(self, messages: List[Dict], config: Dict, session_id: Optional[str] = None):
        """åœ¨å·¥å…·è°ƒç”¨åç»§ç»­æµå¼è¿”å› LLM çš„æœ€ç»ˆå“åº”ï¼ˆä¸å†ä¼ é€’å·¥å…·ï¼‰"""
        self.logger.info(f"ğŸ¯ [Stream] Starting _stream_llm_with_tool_results with {len(messages)} messages")
        
        try:
            from api.event_utils import create_delta_event, create_end_event, create_error_event
        except ImportError:
            from datetime import datetime
            import uuid
            def create_delta_event(content, sid=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "delta", "content": content}
                if sid: evt["session_id"] = sid
                return evt
            def create_end_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "end", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_error_event(error, sid=None, error_code=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "error", "error": error}
                if sid: evt["session_id"] = sid
                if error_code: evt["error_code"] = error_code
                return evt
        
        try:
            await self._ensure_http_client()
            
            # ä¸å†ä¼ é€’å·¥å…·ï¼Œé¿å…æ— é™é€’å½’
            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                stream=True
                # tools=None  # æ˜ç¡®ä¸ä¼ é€’å·¥å…·
            )
            
            url = self._build_llm_url()
            full_response = []
            
            self.logger.info(f"ğŸŒ [Stream] Calling LLM API for tool result processing...")
            
            async with self._http_client.stream('POST', url, json=payload) as resp:
                if resp.status_code >= 400:
                    text = await resp.aread()
                    raise RuntimeError(f"æµå¼ HTTP è¯·æ±‚å¤±è´¥ {resp.status_code}: {text[:200]}")
                
                async for line in resp.aiter_lines():
                    if not line or not line.startswith('data:'):
                        continue
                    
                    data_part = line[5:].strip()
                    if data_part == '[DONE]':
                        break
                    
                    try:
                        data_json = json.loads(data_part)
                    except json.JSONDecodeError:
                        continue
                    
                    for choice in data_json.get('choices', []):
                        delta = choice.get('delta', {})
                        if 'content' in delta and delta['content']:
                            piece = delta['content']
                            full_response.append(piece)
                            self.logger.debug(f"ğŸ“¤ [Stream] Yielding delta: {piece[:50]}...")
                            yield create_delta_event(content=piece, session_id=session_id)
            
            final_content = ''.join(full_response)
            self.logger.info(f"âœ… [Stream] Tool result processing complete, total length: {len(final_content)}")
            yield create_end_event(content=final_content, session_id=session_id)
            
        except Exception as e:
            self.logger.error(f"å·¥å…·ç»“æœæµå¼è°ƒç”¨å¤±è´¥: {e}")
            yield create_error_event(error=str(e), session_id=session_id)
    
    def _has_tool_calls(self, response: Dict[str, Any]) -> bool:
        """Check if LLM response contains tool calls."""
        return bool(response.get("tool_calls"))
    
    def _extract_tool_calls(self, response: Dict[str, Any]) -> List[ToolCall]:
        """Extract tool calls from LLM response."""
        tool_calls = []
        
        for call_data in response.get("tool_calls", []):
            if call_data.get("type") == "function":
                function_data = call_data.get("function", {})
                try:
                    arguments = json.loads(function_data.get("arguments", "{}"))
                except json.JSONDecodeError:
                    arguments = {}
                
                tool_call = ToolCall(
                    id=call_data.get("id", f"tool_{int(datetime.now().timestamp())}"),
                    name=function_data.get("name", "unknown"),
                    arguments=arguments
                )
                tool_calls.append(tool_call)
        
        return tool_calls
    
    def _is_simple_greeting(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºç®€å•é—®å€™è¯­
        
        ç”¨äºå¿«é€Ÿå“åº”ä¼˜åŒ–ï¼Œè·³è¿‡ LLM è°ƒç”¨ä»¥é™ä½å»¶è¿Ÿã€‚
        
        Args:
            text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºç®€å•é—®å€™
        """
        text_lower = text.lower().strip()
        
        # ç®€å•é—®å€™å…³é”®è¯åˆ—è¡¨
        simple_greetings = [
            # è‹±æ–‡
            "hi", "hello", "hey", "hola", "yo",
            # ä¸­æ–‡
            "ä½ å¥½", "æ‚¨å¥½", "å—¨", "å“ˆå–½", "å˜¿",
            "æ—©", "æ—©ä¸Šå¥½", "ä¸­åˆå¥½", "ä¸‹åˆå¥½", "æ™šä¸Šå¥½",
            "æ™šå®‰", "hi~", "hello~", "å—¨~"
        ]
        
        # ç²¾ç¡®åŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
        clean_text = text_lower.strip("!ï¼?ï¼Ÿ.ã€‚,ï¼Œ~")
        return clean_text in simple_greetings
    
    def _get_greeting_response(self) -> str:
        """è·å–é—®å€™å“åº”
        
        Returns:
            éšæœºé—®å€™å“åº”
        """
        import random
        
        responses = [
            "ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©çš„å—ï¼ŸğŸ˜Š",
            "å—¨ï¼æˆ‘æ˜¯ä½ çš„ AI åŠ©æ‰‹ï¼Œéšæ—¶ä¸ºä½ æœåŠ¡ï¼",
            "æ‚¨å¥½ï¼è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°æ‚¨çš„å—ï¼Ÿ",
            "Hiï¼å¾ˆé«˜å…´èƒ½å¸®åˆ°ä½ ï¼âœ¨",
            "ä½ å¥½å‘€ï¼æœ‰ä»€ä¹ˆé—®é¢˜å°½ç®¡é—®æˆ‘ï½"
        ]
        
        return random.choice(responses)
    
    async def _execute_tool_call(self, tool_call: ToolCall) -> ToolResult:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰
        
        æ£€æŸ¥ç¼“å­˜ä»¥é¿å…é‡å¤æ‰§è¡Œç›¸åŒçš„å·¥å…·è°ƒç”¨ï¼Œæå‡æ€§èƒ½å¹¶é™ä½å¤–éƒ¨ API è´¹ç”¨ã€‚
        
        Args:
            tool_call: å·¥å…·è°ƒç”¨å¯¹è±¡
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            # ğŸ†• ç¼“å­˜ä¼˜åŒ–ï¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜ç»“æœ
            cache_key = self._generate_tool_cache_key(tool_call)
            
            if cache_key in self._tool_cache:
                cached_result, cached_time = self._tool_cache[cache_key]
                cache_age = time.time() - cached_time
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                if cache_age < self._cache_ttl:
                    self.logger.info(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜çš„å·¥å…·ç»“æœ: {tool_call.name} (ç¼“å­˜ {int(cache_age)}ç§’å‰)")
                    # åˆ›å»ºæ–°çš„ ToolResult å¯¹è±¡ï¼Œä½¿ç”¨å½“å‰çš„ call_id
                    return ToolResult(
                        call_id=tool_call.id,
                        success=cached_result.success,
                        result=cached_result.result,
                        error=cached_result.error
                    )
                else:
                    # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                    del self._tool_cache[cache_key]
                    self.logger.debug(f"ç¼“å­˜å·²è¿‡æœŸï¼Œé‡æ–°æ‰§è¡Œå·¥å…·: {tool_call.name}")
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆæ— ç¼“å­˜æˆ–ç¼“å­˜è¿‡æœŸï¼‰
            result = await self._execute_tool_call_uncached(tool_call)
            
            # ğŸ†• ç¼“å­˜æˆåŠŸçš„ç»“æœ
            if result.success:
                self._tool_cache[cache_key] = (result, time.time())
                self.logger.debug(f"å·¥å…·ç»“æœå·²ç¼“å­˜: {tool_call.name}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Tool execution error: {e}", exc_info=True)
            return ToolResult(
                call_id=tool_call.id,
                success=False,
                result=None,
                error=str(e)
            )
    
    def _generate_tool_cache_key(self, tool_call: ToolCall) -> str:
        """ç”Ÿæˆå·¥å…·è°ƒç”¨çš„ç¼“å­˜é”®
        
        åŸºäºå·¥å…·åç§°å’Œå‚æ•°ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®ã€‚
        
        Args:
            tool_call: å·¥å…·è°ƒç”¨å¯¹è±¡
            
        Returns:
            ç¼“å­˜é”®å­—ç¬¦ä¸²
        """
        # å°†å‚æ•°æ’åºååºåˆ—åŒ–ï¼Œç¡®ä¿ç›¸åŒå‚æ•°ç”Ÿæˆç›¸åŒçš„é”®
        args_str = json.dumps(tool_call.arguments, sort_keys=True, ensure_ascii=False)
        return f"{tool_call.name}:{args_str}"
    
    async def _execute_tool_call_uncached(self, tool_call: ToolCall) -> ToolResult:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼Œå®é™…æ‰§è¡Œï¼‰
        
        Args:
            tool_call: å·¥å…·è°ƒç”¨å¯¹è±¡
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            # Try to use MCP tool registry
            try:
                from mcp import get_tool_registry
                
                registry = get_tool_registry()
                
                # Map common LLM tool names to MCP tool names
                tool_name_mapping = {
                    "search_tool": "web_search",
                    "web_search": "web_search",
                    "calculator": "calculator",
                    "time_tool": "get_time",
                    "get_time": "get_time",
                    "weather": "get_weather",
                    "get_weather": "get_weather",
                }
                
                mcp_tool_name = tool_name_mapping.get(tool_call.name, tool_call.name)
                
                # Execute through registry
                result_dict = await registry.execute(mcp_tool_name, **tool_call.arguments)
                
                # Convert MCP ToolResult to Agent ToolResult
                if result_dict.get("success"):
                    result_str = json.dumps(result_dict.get("data", {}), ensure_ascii=False)
                else:
                    result_str = None
                
                return ToolResult(
                    call_id=tool_call.id,
                    success=result_dict.get("success", False),
                    result=result_str,
                    error=result_dict.get("error")
                )
            
            except ImportError:
                self.logger.warning("MCP tools not available, using fallback implementation")
                # Fall back to placeholder implementation
                pass
            
            # Fallback placeholder implementation
            if tool_call.name in ["search_tool", "web_search"]:
                result = f"Search results for: {tool_call.arguments.get('query', 'unknown')}"
            elif tool_call.name == "calculator":
                expression = tool_call.arguments.get('expression', '')
                try:
                    result = f"The result is: {expression} (calculation placeholder)"
                except:
                    result = "Could not calculate the expression"
            elif tool_call.name in ["time_tool", "get_time"]:
                result = f"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            else:
                result = f"Tool '{tool_call.name}' executed with arguments: {tool_call.arguments}"
            
            return ToolResult(
                call_id=tool_call.id,
                success=True,
                result=result
            )
            
        except Exception as e:
            self.logger.error(f"Tool execution error in uncached call: {e}", exc_info=True)
            return ToolResult(
                call_id=tool_call.id,
                success=False,
                result=None,
                error=str(e)
            )