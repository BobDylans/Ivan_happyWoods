"""
Agent Nodes Implementation

This module contains the core LangGraph nodes that handle different stages
of conversation processing, including input processing, LLM calls, tool handling,
and response formatting.
"""

import json
import logging
from typing import Dict, Any, List, Optional, Union
import asyncio
import httpx
from datetime import datetime

from .state import AgentState, ConversationMessage, MessageRole, ToolCall, ToolResult

# å¯¼å…¥ LLM å…¼å®¹æ€§å·¥å…·
try:
    from utils.llm_compat import prepare_llm_params
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªç®€å•çš„å…¼å®¹å‡½æ•°
    def prepare_llm_params(model, messages, temperature=0.7, max_tokens=16384, **kwargs):  # ğŸ”§ ä¿®å¤é»˜è®¤å€¼
        params = {
            "model": model,
            "messages": messages,
        }
        # GPT-5 ç³»åˆ—ä¸ä¼  temperatureï¼Œä½¿ç”¨ API é»˜è®¤å€¼
        if not model.startswith("gpt-5"):
            params["temperature"] = temperature
        # GPT-5 ç³»åˆ—ä½¿ç”¨ max_completion_tokens
        if model.startswith("gpt-5"):
            params["max_completion_tokens"] = max_tokens
        else:
            params["max_tokens"] = max_tokens
        params.update(kwargs)
        return params


class DateTimeJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder that handles datetime objects."""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)

try:
    from config.models import VoiceAgentConfig
except ImportError:
    # Fallback for when running as script
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from config.models import VoiceAgentConfig


logger = logging.getLogger(__name__)


class AgentNodes:
    """LangGraph å¯¹è¯å¤„ç†èŠ‚ç‚¹é›†åˆ
    
    è´Ÿè´£å¯¹è¯æµç¨‹ä¸­çš„å„ä¸ªå¤„ç†é˜¶æ®µï¼š
    - è¾“å…¥å¤„ç†å’ŒéªŒè¯
    - LLM è°ƒç”¨ï¼ˆåŒæ­¥/æµå¼ï¼‰
    - å·¥å…·è°ƒç”¨å¤„ç†
    - å“åº”æ ¼å¼åŒ–
    """
    
    def __init__(self, config: VoiceAgentConfig, trace=None):
        """åˆå§‹åŒ–èŠ‚ç‚¹é…ç½®
        
        Args:
            config: è¯­éŸ³åŠ©æ‰‹é…ç½®å¯¹è±¡
            trace: å¯é€‰çš„ TraceEmitter å®ä¾‹ï¼ˆç”¨äºå¯è§†åŒ–äº‹ä»¶ï¼‰
        """
        self.config = config
        self.logger = logger
        self._http_client: Optional[httpx.AsyncClient] = None
        self._client_lock = asyncio.Lock()
        
        # æ¥æ”¶ trace å®ä¾‹
        self.trace = trace
    
    async def _ensure_http_client(self):
        """ç¡®ä¿ HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼ˆæ‡’åŠ è½½ï¼‰
        
        ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ç¡®ä¿çº¿ç¨‹å®‰å…¨çš„å•ä¾‹åˆå§‹åŒ–ã€‚
        """
        if self._http_client is None:
            async with self._client_lock:
                if self._http_client is None:  # åŒé‡æ£€æŸ¥
                    timeout = httpx.Timeout(self.config.llm.timeout, connect=10)
                    self._http_client = httpx.AsyncClient(
                        timeout=timeout,
                        headers={
                            "Authorization": f"Bearer {self.config.llm.api_key}",
                            "Content-Type": "application/json"
                        }
                    )
                    self.logger.debug("HTTP å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    def _build_llm_url(self, endpoint: str = "chat/completions") -> str:
        """æ„å»º LLM API å®Œæ•´ URL
        
        è‡ªåŠ¨å¤„ç† base_url ä¸­æ˜¯å¦åŒ…å« /v1 çš„æƒ…å†µã€‚
        
        Args:
            endpoint: API ç«¯ç‚¹è·¯å¾„ï¼Œé»˜è®¤ä¸º "chat/completions"
        
        Returns:
            å®Œæ•´çš„ API URL
        
        Examples:
            >>> # base_url = "https://api.openai-proxy.org/v1"
            >>> self._build_llm_url()
            "https://api.openai-proxy.org/v1/chat/completions"
            
            >>> # base_url = "https://api.openai-proxy.org"
            >>> self._build_llm_url()
            "https://api.openai-proxy.org/v1/chat/completions"
        """
        base = self.config.llm.base_url.rstrip('/')
        
        # ä»…åœ¨ base_url ä¸åŒ…å« /v1 æ—¶æ·»åŠ 
        if not base.endswith('/v1'):
            base = base + '/v1'
        
        url = f"{base}/{endpoint}"
        return url
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº
        
        å…³é—­ HTTP å®¢æˆ·ç«¯è¿æ¥ï¼Œé‡Šæ”¾èµ„æºã€‚
        åº”åœ¨ç¨‹åºé€€å‡ºæˆ–æœåŠ¡åœæ­¢æ—¶è°ƒç”¨ã€‚
        """
        if self._http_client:
            try:
                await self._http_client.aclose()
                self.logger.debug("HTTP å®¢æˆ·ç«¯å·²å…³é—­")
            except Exception as e:
                self.logger.warning(f"å…³é—­ HTTP å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
            finally:
                self._http_client = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ï¼Œè‡ªåŠ¨æ¸…ç†èµ„æº"""
        await self.cleanup()
    async def process_input(self, state: AgentState) -> AgentState:
        """å¤„ç†å’ŒéªŒè¯ç”¨æˆ·è¾“å…¥
        
        è¿™æ˜¯å¯¹è¯å¤„ç†æµç¨‹çš„å…¥å£èŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. éªŒè¯è¾“å…¥ä¸ä¸ºç©º
        2. åˆ›å»ºç”¨æˆ·æ¶ˆæ¯å¯¹è±¡
        3. åˆæ­¥è¯†åˆ«ç”¨æˆ·æ„å›¾
        4. æ›´æ–°çŠ¶æ€å‡†å¤‡è°ƒç”¨ LLM
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        """
        session_id = state.get('session_id', 'unknown')
        
        try:
            self.logger.debug(f"å¤„ç†ä¼šè¯ {session_id} çš„è¾“å…¥")
            
            # ğŸ†• æ€è€ƒé˜¶æ®µï¼šéªŒè¯è¾“å…¥
            if self.trace:
                # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ yieldï¼Œå› ä¸º process_input æ˜¯åŒæ­¥è¿”å› state çš„
                # æˆ‘ä»¬åªè®°å½•æ—¥å¿—ï¼ŒçœŸæ­£çš„äº‹ä»¶åœ¨ Graph å±‚å‘å°„
                self.logger.debug(f"[Trace] process_input: éªŒè¯ç”¨æˆ·è¾“å…¥")
            
            # æ›´æ–°æ—¶é—´æˆ³
            state["last_activity"] = datetime.now()
            
            # è§„èŒƒåŒ–è¾“å…¥ï¼Œç¡®ä¿ä¸ä¸ºç©º
            user_input = state["user_input"].strip()
            if not user_input:
                state["error_state"] = "empty_input"
                state["should_continue"] = False
                state["agent_response"] = "æˆ‘æ²¡æœ‰æ”¶åˆ°ä»»ä½•è¾“å…¥ï¼Œè¯·è¯´ç‚¹ä»€ä¹ˆå§ã€‚"
                return state
            
            # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²
            user_message = ConversationMessage(
                id=f"user_{len(state['messages']) + 1}_{int(datetime.now().timestamp())}",
                role=MessageRole.USER,
                content=user_input,
                metadata={"processed_at": datetime.now().isoformat()}
            )
            state["messages"].append(user_message)
            
            # é€šè¿‡å…³é”®è¯åˆæ­¥è¯†åˆ«ç”¨æˆ·æ„å›¾
            state["current_intent"] = self._analyze_intent(user_input)
            
            # è®¾ç½®ä¸‹ä¸€æ­¥åŠ¨ä½œï¼šè°ƒç”¨ LLM
            state["next_action"] = "call_llm"
            
            self.logger.debug(f"è¾“å…¥å¤„ç†å®Œæˆï¼Œæ„å›¾: {state['current_intent']}")
            return state
            
        except Exception as e:
            self.logger.error(f"è¾“å…¥å¤„ç†é”™è¯¯: {e}")
            state["error_state"] = f"input_processing_error: {str(e)}"
            state["should_continue"] = False
            return state
    
    async def call_llm(self, state: AgentState) -> AgentState:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå“åº”
        
        è¿™æ˜¯æ ¸å¿ƒå¤„ç†èŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. å‡†å¤‡å¯¹è¯å†å²æ¶ˆæ¯
        2. é…ç½®æ¨¡å‹å‚æ•°
        3. è°ƒç”¨ LLM API
        4. å¤„ç†å“åº”ï¼ˆæ–‡æœ¬æˆ–å·¥å…·è°ƒç”¨ï¼‰
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        """
        try:
            self.logger.debug(f"ä¸ºä¼šè¯ {state['session_id']} è°ƒç”¨ LLM")
            
            # å‡†å¤‡ LLM æ¶ˆæ¯ï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰
            # å¦‚æœ state ä¸­æœ‰ external_historyï¼Œä¼ é€’ç»™ _prepare_llm_messages
            external_history = state.get("external_history")
            if external_history is not None:
                self.logger.info(f"ğŸ” Found external_history in state: {len(external_history)} messages")
            else:
                self.logger.warning(f"âš ï¸ No external_history found in state for session {state['session_id']}")
            messages = self._prepare_llm_messages(state, external_history=external_history)
            
            # é…ç½®æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨å…¼å®¹å±‚å¤„ç†ä¸åŒæ¨¡å‹çš„å‚æ•°å·®å¼‚ï¼‰
            model = state["model_config"].get("model", self.config.llm.models.default)
            max_tokens = state.get("max_tokens", self.config.llm.max_tokens)
            temperature = state.get("temperature", self.config.llm.temperature)
            
            llm_config = prepare_llm_params(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # è°ƒç”¨ LLM API è·å–å“åº”
            response = await self._make_llm_call(messages, llm_config)
            
            # åˆ¤æ–­å“åº”ç±»å‹ï¼šå·¥å…·è°ƒç”¨ or ç›´æ¥å›å¤
            if self._has_tool_calls(response):
                state["next_action"] = "handle_tools"
                # æå–å·¥å…·è°ƒç”¨è¯·æ±‚å¹¶åŠ å…¥å¾…å¤„ç†é˜Ÿåˆ—
                tool_calls = self._extract_tool_calls(response)
                state["pending_tool_calls"].extend(tool_calls)
            else:
                state["next_action"] = "format_response"
                state["agent_response"] = response.get("content", "")
            
            self.logger.debug("LLM è°ƒç”¨å®Œæˆ")
            return state
            
        except Exception as e:
            self.logger.error(f"LLM è°ƒç”¨é”™è¯¯: {e}")
            state["error_state"] = f"llm_call_error: {str(e)}"
            state["agent_response"] = "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
            state["next_action"] = "format_response"
            return state
    
    async def handle_tools(self, state: AgentState) -> AgentState:
        """å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚
        
        ğŸ†• ä¼˜åŒ–ç‰ˆ: æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
        
        å½“ LLM éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œæ­¤èŠ‚ç‚¹è´Ÿè´£ï¼š
        1. æ‰§è¡Œæ‰€æœ‰å¾…å¤„ç†çš„å·¥å…·è°ƒç”¨
        2. æ”¶é›†å·¥å…·æ‰§è¡Œç»“æœ
        3. å°†ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
        4. ğŸ†• å¢åŠ å·¥å…·è°ƒç”¨è®¡æ•°å™¨
        5. å‡†å¤‡å†æ¬¡è°ƒç”¨ LLMï¼ˆè®©å®ƒåŸºäºå·¥å…·ç»“æœé‡æ–°æ€è€ƒï¼‰
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        """
        try:
            self.logger.debug(f"å¤„ç†ä¼šè¯ {state['session_id']} çš„å·¥å…·è°ƒç”¨")
            
            if not state["pending_tool_calls"]:
                self.logger.warning("æ²¡æœ‰å¾…å¤„ç†çš„å·¥å…·è°ƒç”¨")
                state["next_action"] = "call_llm"
                return state
            
            # ğŸ†• å¢åŠ å·¥å…·è°ƒç”¨è®¡æ•°
            state["tool_call_count"] = state.get("tool_call_count", 0) + 1
            current_iteration = state["tool_call_count"]
            
            self.logger.info(f"ğŸ”§ ç¬¬ {current_iteration} è½®å·¥å…·è°ƒç”¨ï¼Œå¾…æ‰§è¡Œå·¥å…·æ•°: {len(state['pending_tool_calls'])}")
            
            # é€ä¸ªæ‰§è¡Œå·¥å…·è°ƒç”¨
            for tool_call in state["pending_tool_calls"]:
                result = await self._execute_tool_call(tool_call)
                state["tool_results"].append(result)
                state["tool_calls"].append(tool_call)
                
                self.logger.info(f"  âœ… å·¥å…· '{tool_call.name}' æ‰§è¡Œå®Œæˆ: {result.success}")
            
            # æ¸…ç©ºå¾…å¤„ç†é˜Ÿåˆ—
            state["pending_tool_calls"] = []
            
            # å°†å·¥å…·æ‰§è¡Œç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
            for result in state["tool_results"][-len(state["tool_calls"]):]:
                tool_message = ConversationMessage(
                    id=f"tool_{result.call_id}_{int(datetime.now().timestamp())}",
                    role=MessageRole.TOOL,
                    content=json.dumps(result.dict(), cls=DateTimeJSONEncoder),
                    metadata={"tool_call_id": result.call_id, "success": result.success}
                )
                state["messages"].append(tool_message)
            
            # ğŸ†• æ ¸å¿ƒæ”¹åŠ¨: å·¥å…·è°ƒç”¨åè¿”å› LLM è¿›è¡Œé‡æ–°æ€è€ƒ
            # LLM ä¼šåŸºäºå·¥å…·ç»“æœåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´å¤šå·¥å…·æˆ–ç›´æ¥ç”Ÿæˆå›å¤
            state["next_action"] = "call_llm"
            
            self.logger.info(f"ğŸ”„ ç¬¬ {current_iteration} è½®å·¥å…·è°ƒç”¨å®Œæˆï¼Œè¿”å› LLM é‡æ–°è¯„ä¼°")
            return state
            
        except Exception as e:
            self.logger.error(f"å·¥å…·å¤„ç†é”™è¯¯: {e}")
            state["error_state"] = f"tool_handling_error: {str(e)}"
            state["agent_response"] = "æŠ±æ­‰ï¼Œåœ¨ä½¿ç”¨å·¥å…·æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œè®©æˆ‘æ¢ä¸ªæ–¹å¼å¸®æ‚¨ã€‚"
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¿”å› LLM è®©å®ƒç”Ÿæˆ fallback å›å¤
            state["next_action"] = "call_llm"
            return state
    
    async def format_response(self, state: AgentState) -> AgentState:
        """æ ¼å¼åŒ–æœ€ç»ˆå“åº”
        
        è¿™æ˜¯æµç¨‹çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. ç¡®ä¿æœ‰å“åº”å†…å®¹
        2. åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å¯¹è±¡
        3. æ·»åŠ å…ƒæ•°æ®
        4. æ ‡è®°å¯¹è¯å›åˆç»“æŸ
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æœ€ç»ˆçš„å¯¹è¯çŠ¶æ€
        """
        try:
            self.logger.debug(f"æ ¼å¼åŒ–ä¼šè¯ {state['session_id']} çš„å“åº”")
            
            # ç¡®ä¿æœ‰å“åº”å†…å®¹
            if not state["agent_response"]:
                if state["error_state"]:
                    state["agent_response"] = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ã€‚"
                else:
                    state["agent_response"] = "æˆ‘ä¸å¤ªç¡®å®šå¦‚ä½•å›ç­”ï¼Œè¯·æ¢ä¸ªæ–¹å¼é—®æˆ‘å§ã€‚"
            
            # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å¹¶æ·»åŠ åˆ°å†å²
            assistant_message = ConversationMessage(
                id=f"assistant_{len(state['messages']) + 1}_{int(datetime.now().timestamp())}",
                role=MessageRole.ASSISTANT,
                content=state["agent_response"],
                metadata={
                    "generated_at": datetime.now().isoformat(),
                    "model": state["model_config"].get("model", "unknown"),
                    "intent": state.get("current_intent"),
                    "tool_calls_count": len(state["tool_calls"])
                }
            )
            state["messages"].append(assistant_message)
            
            # æ›´æ–°æ´»åŠ¨æ—¶é—´æˆ³
            state["last_activity"] = datetime.now()
            
            # æ ‡è®°å¯¹è¯å›åˆå®Œæˆ
            state["should_continue"] = False
            state["next_action"] = None
            
            self.logger.debug("å“åº”æ ¼å¼åŒ–å®Œæˆ")
            return state
            
        except Exception as e:
            self.logger.error(f"å“åº”æ ¼å¼åŒ–é”™è¯¯: {e}")
            state["error_state"] = f"response_formatting_error: {str(e)}"
            state["agent_response"] = "æŠ±æ­‰ï¼Œå“åº”æ ¼å¼åŒ–æ—¶å‡ºç°äº†é—®é¢˜ã€‚"
            state["should_continue"] = False
            return state
    
    def _analyze_intent(self, user_input: str) -> Optional[str]:
        """åˆ†æç”¨æˆ·æ„å›¾ï¼ˆåŸºäºå…³é”®è¯çš„ç®€å•å®ç°ï¼‰
        
        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„æ„å›¾è¯†åˆ«ï¼Œä»…ç”¨äºåŸºç¡€åˆ†ç±»ã€‚
        ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ NLU æ¨¡å‹æˆ– LLM è¿›è¡Œæ„å›¾è¯†åˆ«ã€‚
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        
        Returns:
            è¯†åˆ«çš„æ„å›¾æ ‡ç­¾ï¼Œå¦‚ "search", "calculation" ç­‰
        """
        input_lower = user_input.lower()
        
        # åŸºäºå…³é”®è¯çš„ç®€å•æ„å›¾æ£€æµ‹
        if any(word in input_lower for word in ["search", "find", "look", "æœç´¢", "æŸ¥æ‰¾"]):
            return "search"
        elif any(word in input_lower for word in ["calculate", "math", "compute", "è®¡ç®—"]):
            return "calculation"
        elif any(word in input_lower for word in ["time", "date", "when", "æ—¶é—´", "æ—¥æœŸ"]):
            return "time_query"
        elif any(word in input_lower for word in ["image", "picture", "generate", "create", "å›¾ç‰‡", "ç”Ÿæˆ"]):
            return "image_generation"
        elif any(word in input_lower for word in ["help", "what", "how", "å¸®åŠ©", "æ€ä¹ˆ"]):
            return "help_request"
        else:
            return "general_conversation"
    
    def _prepare_llm_messages(self, state: AgentState, external_history: List[Dict] = None) -> List[Dict[str, str]]:
        """å‡†å¤‡ LLM API è°ƒç”¨çš„æ¶ˆæ¯åˆ—è¡¨
        
        åŒ…å«ä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯å’Œå†å²å¯¹è¯ã€‚ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„å†å²è®°å½•ã€‚
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
            external_history: å¤–éƒ¨ä¼ å…¥çš„å†å²æ¶ˆæ¯åˆ—è¡¨ (å¯é€‰)
        
        Returns:
            æ ¼å¼åŒ–çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œç¬¦åˆ OpenAI API æ ¼å¼
        """
        messages = []
        
        # æ„å»ºä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯
        system_prompt = self._build_optimized_system_prompt(state)
        system_message = {
            "role": "system",
            "content": system_prompt
        }
        messages.append(system_message)
        
        # ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨å†å²ï¼ˆä» SessionHistoryManagerï¼‰
        if external_history is not None:
            # é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡ï¼ˆæœ€è¿‘ 10 æ¡ï¼‰
            MAX_HISTORY_MESSAGES = 10
            recent_history = external_history[-MAX_HISTORY_MESSAGES:] if external_history else []
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            self.logger.info(f"âœ… Loaded {len(recent_history)} messages from external history for LLM")
            
            # é‡è¦ï¼šæ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ¥è‡ª state["messages"] çš„æœ€åä¸€æ¡ï¼‰
            if state["messages"] and state["messages"][-1].role == MessageRole.USER:
                current_user_msg = state["messages"][-1]
                messages.append({
                    "role": "user",
                    "content": current_user_msg.content
                })
                self.logger.info(f"âœ… Added current user message to LLM input")
        else:
            # å›é€€åˆ° state ä¸­çš„æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            self.logger.info("âš ï¸ No external history provided, using state messages")
            MAX_HISTORY_MESSAGES = 10
            recent_messages = state["messages"][-MAX_HISTORY_MESSAGES:]
            for msg in recent_messages:
                if msg.role in [MessageRole.USER, MessageRole.ASSISTANT]:
                    messages.append({
                        "role": msg.role.value,
                        "content": msg.content
                    })
            self.logger.info(f"ğŸ“ Using {len(recent_messages)} messages from state")
        
        return messages
    
    def _build_optimized_system_prompt(self, state: AgentState) -> str:
        """æ„å»ºä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯ï¼Œæå‡æ™ºèƒ½æ€§å’Œæ•ˆç‡
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æ˜ç¡®è§’è‰²å®šä½å’Œèƒ½åŠ›è¾¹ç•Œ
        2. æä¾›æ¸…æ™°çš„å·¥å…·ä½¿ç”¨æŒ‡å—
        3. å¼ºè°ƒæ•ˆç‡å’Œå‡†ç¡®æ€§
        4. åŒ…å«ä»»åŠ¡åˆ†è§£å’Œæ¨ç†æ¡†æ¶
        5. æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æç¤ºè¯
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            ä¼˜åŒ–åçš„ç³»ç»Ÿæç¤ºè¯å­—ç¬¦ä¸²
        """
        # åŸºç¡€èº«ä»½å®šä¹‰
        base_identity = """# è§’è‰²å®šä½
ä½ æ˜¯ä¸€ä¸ªé«˜æ•ˆã€æ™ºèƒ½çš„å¤šåŠŸèƒ½ AI åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š
- è‡ªç„¶æµç•…çš„ä¸­è‹±æ–‡å¯¹è¯
- æ™ºèƒ½å·¥å…·è°ƒç”¨å’Œä»»åŠ¡ç¼–æ’
- ç»“æ„åŒ–é—®é¢˜åˆ†æå’Œè§£å†³
- ä¸Šä¸‹æ–‡ç†è§£å’Œè®°å¿†ä¿æŒ

# æ ¸å¿ƒåŸåˆ™
1. **æ•ˆç‡ä¼˜å…ˆ**: ç”¨æœ€å°‘çš„æ­¥éª¤è¾¾æˆç›®æ ‡ï¼Œé¿å…å†—ä½™æ“ä½œ
2. **å‡†ç¡®è‡³ä¸Š**: ä¼˜å…ˆä¿è¯ä¿¡æ¯å‡†ç¡®æ€§ï¼Œä¸ç¡®å®šæ—¶æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
3. **ä¸»åŠ¨æ€è€ƒ**: ç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¿…è¦æ—¶ä¸»åŠ¨æ¾„æ¸…éœ€æ±‚
4. **å·¥å…·æ™ºç”¨**: åˆç†åˆ¤æ–­ä½•æ—¶éœ€è¦å·¥å…·ï¼Œé¿å…ä¸å¿…è¦çš„è°ƒç”¨

# ğŸ“ å›å¤æ ¼å¼è§„èŒƒï¼ˆé‡è¦ï¼‰
**è¯·åŠ¡å¿…ä½¿ç”¨ Markdown æ ¼å¼ç»„ç»‡ä½ çš„å›å¤ï¼Œæå‡å¯è¯»æ€§ï¼š**

1. **ä½¿ç”¨æ ‡é¢˜åˆ†å±‚**: ç”¨ `##` æˆ– `###` æ ‡æ³¨æ®µè½ä¸»é¢˜
2. **åˆ—è¡¨å‘ˆç°è¦ç‚¹**: ç”¨ `-` æˆ– `1.` åˆ—ä¸¾ä¿¡æ¯
3. **å¼ºè°ƒå…³é”®ä¿¡æ¯**: ç”¨ `**ç²—ä½“**` çªå‡ºé‡ç‚¹
4. **ä»£ç å—**: æŠ€æœ¯å†…å®¹ç”¨ ` ```è¯­è¨€ ``` ` åŒ…è£¹
5. **å¼•ç”¨æ¥æº**: æœç´¢ç»“æœç”¨ `> å¼•ç”¨` æ ¼å¼
6. **é“¾æ¥æ ¼å¼**: ç”¨ `[æ ‡é¢˜](URL)` å±•ç¤ºé“¾æ¥

**ç¤ºä¾‹å›å¤æ ¼å¼**:
```
## ğŸ“Š æœç´¢ç»“æœ

æ ¹æ®æœ€æ–°ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯å…³äº [ä¸»é¢˜] çš„è¦ç‚¹ï¼š

### 1. [ç¬¬ä¸€ä¸ªè¦ç‚¹]
- **å…³é”®ä¿¡æ¯**: xxx
- **æ—¶é—´**: xxx
- **æ¥æº**: [æ–°é—»æ ‡é¢˜](é“¾æ¥)

### 2. [ç¬¬äºŒä¸ªè¦ç‚¹]
...

---
ğŸ’¡ **æ€»ç»“**: ç®€çŸ­æ€»ç»“å…³é”®ä¿¡æ¯
```

**å¯¹äºæœç´¢ç»“æœï¼Œç‰¹åˆ«æ³¨æ„**ï¼š
- ç”¨æ¸…æ™°çš„æ ‡é¢˜å’Œåºå·ç»„ç»‡
- æ¯æ¡æ–°é—»åŒ…å«ï¼šæ ‡é¢˜ã€æ‘˜è¦ã€æ¥æºé“¾æ¥
- ç”¨åˆ†éš”çº¿ `---` åŒºåˆ†ä¸åŒéƒ¨åˆ†
- é¿å…é•¿æ®µè½ï¼Œå¤šç”¨åˆ—è¡¨"""

        # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
        available_tools = self._format_available_tools()
        
        tools_guide = f"""

# å¯ç”¨å·¥å…·
{available_tools}

# å·¥å…·ä½¿ç”¨ç­–ç•¥
**ä½•æ—¶ä½¿ç”¨å·¥å…·**:
- éœ€è¦å®æ—¶ä¿¡æ¯ï¼ˆå¤©æ°”ã€æ—¶é—´ã€æœç´¢ï¼‰æ—¶ â†’ å¿…é¡»ä½¿ç”¨å·¥å…·
- éœ€è¦å¤æ‚è®¡ç®—æˆ–æ•°æ®å¤„ç†æ—¶ â†’ ä½¿ç”¨è®¡ç®—å™¨å·¥å…·
- ç”¨æˆ·æ˜ç¡®è¦æ±‚æ‰§è¡Œç‰¹å®šæ“ä½œæ—¶ â†’ ä½¿ç”¨å¯¹åº”å·¥å…·

**ä½•æ—¶ä¸ä½¿ç”¨å·¥å…·**:
- å›ç­”å¸¸è¯†æ€§é—®é¢˜æˆ–ä¸€èˆ¬æ€§å¯¹è¯ â†’ ç›´æ¥å›ç­”
- ç®€å•çš„å¿ƒç®—æˆ–é€»è¾‘æ¨ç† â†’ ç›´æ¥å›ç­”
- éœ€è¦åˆ›æ„æˆ–å»ºè®®æ—¶ â†’ ç›´æ¥å›ç­”

**å·¥å…·è°ƒç”¨åŸåˆ™**:
1. ä¸€æ¬¡åªè°ƒç”¨çœŸæ­£éœ€è¦çš„å·¥å…·
2. ä¼˜å…ˆä½¿ç”¨æœ€åˆé€‚çš„å•ä¸ªå·¥å…·ï¼Œè€Œéå¤šä¸ªå·¥å…·
3. å·¥å…·è°ƒç”¨åï¼ŒåŸºäºç»“æœç»™å‡ºæ¸…æ™°ã€æœ‰ä»·å€¼çš„å›ç­”"""

        # ä»»åŠ¡å¤„ç†æ¡†æ¶
        task_framework = """

# ä»»åŠ¡å¤„ç†æ¡†æ¶
å¯¹äºå¤æ‚è¯·æ±‚ï¼Œéµå¾ªä»¥ä¸‹æ€ç»´æµç¨‹ï¼š
1. **ç†è§£**: å‡†ç¡®è¯†åˆ«ç”¨æˆ·çœŸå®éœ€æ±‚å’Œæ„å›¾
2. **è§„åˆ’**: ç¡®å®šæ˜¯å¦éœ€è¦å·¥å…·ï¼Œéœ€è¦å“ªäº›å·¥å…·
3. **æ‰§è¡Œ**: é«˜æ•ˆè°ƒç”¨å¿…è¦çš„å·¥å…·è·å–ä¿¡æ¯
4. **ç»¼åˆ**: æ•´åˆå·¥å…·ç»“æœï¼Œæä¾›æœ‰ä»·å€¼çš„å›ç­”
5. **éªŒè¯**: ç¡®ä¿å›ç­”å®Œæ•´ã€å‡†ç¡®åœ°è§£å†³äº†ç”¨æˆ·é—®é¢˜

# å“åº”è´¨é‡æ ‡å‡†
âœ… ä¼˜è´¨å›ç­”åº”è¯¥ï¼š
- ç›´æ¥é’ˆå¯¹ç”¨æˆ·é—®é¢˜ï¼Œé¿å…å•°å—¦
- ç»“æ„æ¸…æ™°ï¼ˆå¿…è¦æ—¶ä½¿ç”¨åˆ—è¡¨ã€åˆ†ç‚¹ï¼‰
- ä¿¡æ¯å‡†ç¡®ï¼Œæ¥æºå¯é 
- è¯­æ°”å‹å¥½ã€ä¸“ä¸š

âŒ é¿å…ï¼š
- è¿‡åº¦å†—é•¿æˆ–é‡å¤çš„è§£é‡Š
- ä¸å¿…è¦çš„é“æ­‰æˆ–è°¦é€Šè¡¨è¾¾
- æ¨¡ç³Šä¸æ¸…çš„å›ç­”
- è°ƒç”¨ä¸ç›¸å…³çš„å·¥å…·"""

        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¼˜åŒ–
        context_optimization = self._build_context_aware_addition(state)
        
        # ç»„åˆå®Œæ•´æç¤ºè¯
        full_prompt = base_identity + tools_guide + task_framework
        
        if context_optimization:
            full_prompt += "\n\n" + context_optimization
        
        return full_prompt
    
    def _get_tools_schema(self) -> List[Dict]:
        """è·å–å·¥å…·çš„ OpenAI Function Calling æ ¼å¼å®šä¹‰
        
        Returns:
            å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ŒOpenAI tools æ ¼å¼
        """
        try:
            from mcp import get_tool_registry
            registry = get_tool_registry()
            tools = registry.list_tools()
            
            if not tools:
                return []
            
            # è½¬æ¢ä¸º OpenAI Function Calling æ ¼å¼
            tools_schema = []
            for tool in tools:
                schema = tool.to_openai_schema()
                tools_schema.append(schema)
            
            self.logger.info(f"âœ… Loaded {len(tools_schema)} tools for LLM")
            return tools_schema
        except Exception as e:
            self.logger.error(f"âŒ Failed to load tools schema: {e}", exc_info=True)
            return []
    
    def _format_available_tools(self) -> str:
        """æ ¼å¼åŒ–å¯ç”¨å·¥å…·åˆ—è¡¨ä¸ºæ˜“è¯»çš„æ–‡æœ¬
        
        Returns:
            æ ¼å¼åŒ–çš„å·¥å…·åˆ—è¡¨å­—ç¬¦ä¸²
        """
        try:
            from mcp import get_tool_registry
            registry = get_tool_registry()
            tools = registry.list_tools()
            
            if not tools:
                return "å½“å‰æš‚æ— å¯ç”¨å·¥å…·ã€‚"
            
            tool_descriptions = []
            for tool in tools:
                name = tool.name
                desc = tool.description
                # ç®€åŒ–æè¿°ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯
                short_desc = desc.split('.')[0] if desc else "æ— æè¿°"
                tool_descriptions.append(f"- **{name}**: {short_desc}")
            
            return "\n".join(tool_descriptions)
        
        except Exception as e:
            self.logger.warning(f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
            return "- **calculator**: æ‰§è¡Œæ•°å­¦è®¡ç®—\n- **get_time**: è·å–å½“å‰æ—¶é—´\n- **get_weather**: æŸ¥è¯¢å¤©æ°”ä¿¡æ¯\n- **web_search**: æœç´¢ç½‘ç»œä¿¡æ¯"
    
    def _build_context_aware_addition(self, state: AgentState) -> str:
        """æ ¹æ®å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡æ„å»ºé¢å¤–çš„æç¤ºè¯å¢å¼º
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            ä¸Šä¸‹æ–‡ç›¸å…³çš„é¢å¤–æç¤ºè¯ï¼Œå¦‚æœä¸éœ€è¦åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        additions = []
        
        # 1. å¦‚æœæœ‰å·¥å…·è°ƒç”¨å†å²ï¼Œæé†’åŸºäºç»“æœå›ç­”
        if state.get("tool_calls") and len(state["tool_calls"]) > 0:
            additions.append(
                "# å½“å‰çŠ¶æ€\n"
                "ä½ åˆšåˆšè°ƒç”¨äº†å·¥å…·å¹¶è·å¾—äº†ç»“æœã€‚è¯·åŸºäºå·¥å…·è¿”å›çš„å®é™…æ•°æ®å›ç­”ç”¨æˆ·ï¼Œ"
                "ä¸è¦ç¼–é€ æˆ–çŒœæµ‹ä¿¡æ¯ã€‚å¦‚æœå·¥å…·ç»“æœä¸å®Œæ•´ï¼Œå¯ä»¥æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ã€‚"
            )
        
        # 2. å¦‚æœå¯¹è¯è½®æ¬¡è¾ƒå¤šï¼Œæé†’ä¿æŒè¿è´¯æ€§
        message_count = len(state.get("messages", []))
        if message_count > 6:
            additions.append(
                "# å¯¹è¯è¿è´¯æ€§\n"
                "å½“å‰å¯¹è¯å·²è¿›è¡Œå¤šè½®ï¼Œè¯·ä¿æŒå¯¹è¯è¿è´¯æ€§å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚"
                "å¦‚æœç”¨æˆ·æåˆ°'å®ƒ'ã€'è¿™ä¸ª'ç­‰ä»£è¯ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£æ‰€æŒ‡å¯¹è±¡ã€‚"
            )
        
        # 3. å¦‚æœæ£€æµ‹åˆ°ç‰¹å®šæ„å›¾ï¼Œç»™å‡ºé’ˆå¯¹æ€§æŒ‡å¯¼
        intent = state.get("current_intent")
        if intent == "search":
            additions.append(
                "# æœç´¢ä»»åŠ¡ä¼˜åŒ–\n"
                "ç”¨æˆ·éœ€è¦æœç´¢ä¿¡æ¯ã€‚ä½¿ç”¨ web_search å·¥å…·åï¼Œè¯·ï¼š\n"
                "1. æ€»ç»“å…³é”®ä¿¡æ¯ï¼Œè€Œéç®€å•ç½—åˆ—ç»“æœ\n"
                "2. å¦‚æœæœ‰ AI ç”Ÿæˆçš„æ‘˜è¦ï¼Œä¼˜å…ˆä½¿ç”¨\n"
                "3. æä¾› 1-2 ä¸ªæœ€ç›¸å…³çš„é“¾æ¥ä¾›ç”¨æˆ·å‚è€ƒ"
            )
        elif intent == "calculation":
            additions.append(
                "# è®¡ç®—ä»»åŠ¡ä¼˜åŒ–\n"
                "ç”¨æˆ·éœ€è¦è¿›è¡Œè®¡ç®—ã€‚å¯¹äºå¤æ‚è¡¨è¾¾å¼æˆ–å¤šæ­¥è®¡ç®—ï¼Œè¯·ä½¿ç”¨ calculator å·¥å…·ã€‚\n"
                "ç®€å•ç®—æœ¯ï¼ˆå¦‚ 2+2ï¼‰å¯ç›´æ¥å›ç­”ï¼Œä½†æ¶‰åŠå°æ•°ã€å¹‚æ¬¡ã€ä¸‰è§’å‡½æ•°ç­‰åº”ä½¿ç”¨å·¥å…·ç¡®ä¿ç²¾åº¦ã€‚"
            )
        
        return "\n\n".join(additions) if additions else ""
    
    async def _make_llm_call(self, messages: List[Dict[str, str]], config: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨ LLM APIï¼ˆOpenAI å…¼å®¹ï¼‰
        
        å¦‚æœçœŸå® HTTP è°ƒç”¨å¤±è´¥ï¼Œä¼šä½¿ç”¨åŸºäºå…³é”®è¯çš„å¯å‘å¼ fallbackã€‚
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            config: LLM é…ç½®å‚æ•°
        
        Returns:
            åŒ…å« content (str) å’Œ tool_calls (list) çš„å­—å…¸
        """
        user_message = messages[-1]["content"] if messages else ""

        # ==== ä¸»è¦è·¯å¾„ï¼šçœŸå® HTTP è°ƒç”¨ ====
        try:
            # ç¡®ä¿ HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            await self._ensure_http_client()

            # è·å–å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰
            self.logger.info(f"ğŸ” Attempting to load tools schema...")
            tools_schema = self._get_tools_schema()
            self.logger.info(f"ğŸ” Tools schema loaded: {len(tools_schema) if tools_schema else 0} tools")
            
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                tools=tools_schema if tools_schema else None  # ä¼ é€’å·¥å…·å®šä¹‰
            )
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - LLM è¯·æ±‚å‚æ•°
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¤ LLM API è¯·æ±‚å‚æ•°:")
            self.logger.info(f"  Model: {payload.get('model')}")
            self.logger.info(f"  Max Tokens: {payload.get('max_tokens') or payload.get('max_completion_tokens')}")
            self.logger.info(f"  Temperature: {payload.get('temperature', 'N/A (æ¨¡å‹é»˜è®¤)')}")
            self.logger.info(f"  Messages Count: {len(messages)}")
            self.logger.info(f"  Tools Count: {len(tools_schema) if tools_schema else 0}")
            
            # ä¼°ç®—è¾“å…¥ token æ•°ï¼ˆç²—ç•¥ä¼°è®¡ï¼šä¸­æ–‡ ~1.5 å­—ç¬¦/tokenï¼Œè‹±æ–‡ ~4 å­—ç¬¦/tokenï¼‰
            total_chars = sum(len(str(m.get('content', ''))) for m in messages)
            estimated_input_tokens = int(total_chars / 2)  # ä¿å®ˆä¼°è®¡
            self.logger.info(f"  ä¼°ç®—è¾“å…¥ Tokens: ~{estimated_input_tokens}")
            self.logger.info("=" * 60)
            
            # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ  tool_choice
            if tools_schema:
                payload["tool_choice"] = "auto"  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
                self.logger.info(f"ğŸ”§ Added {len(tools_schema)} tools to LLM request")
            else:
                self.logger.warning(f"âš ï¸ No tools available for LLM request")

            # æ„å»ºå®Œæ•´ URL
            url = self._build_llm_url()
            self.logger.debug(f"LLM è°ƒç”¨: {url}")

            # å‘é€è¯·æ±‚
            resp = await self._http_client.post(url, json=payload)
            if resp.status_code >= 400:
                error_text = resp.text[:500]
                self.logger.error(f"LLM HTTP {resp.status_code}: {error_text}")
                raise RuntimeError(f"LLM HTTP {resp.status_code}: {error_text}")
            
            data = resp.json()
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - LLM å“åº”ä¿¡æ¯
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¥ LLM API å“åº”:")
            self.logger.info(f"  Choices Count: {len(data.get('choices', []))}")
            
            # æå–å…³é”®ä¿¡æ¯
            choices = data.get("choices", [])
            if choices:
                first = choices[0]
                finish_reason = first.get("finish_reason", "unknown")
                message_obj = first.get("message", {})
                content = message_obj.get("content") or ""
                tool_calls_raw = message_obj.get("tool_calls") or []
                
                # âš ï¸ å…³é”®è¯Šæ–­ç‚¹ï¼šfinish_reason
                self.logger.info(f"  â­ Finish Reason: {finish_reason}")
                if finish_reason == "length":
                    self.logger.warning("  âŒ å“åº”è¢«æˆªæ–­ï¼åŸå› : max_tokens é™åˆ¶")
                    self.logger.warning("  ğŸ’¡ å»ºè®®: å¢åŠ  max_tokens æˆ–å‡å°‘è¾“å…¥é•¿åº¦")
                elif finish_reason == "stop":
                    self.logger.info("  âœ… å“åº”å®Œæ•´ï¼ˆæ­£å¸¸ç»“æŸï¼‰")
                elif finish_reason == "tool_calls":
                    self.logger.info("  ğŸ”§ å“åº”ç±»å‹: å·¥å…·è°ƒç”¨")
                
                self.logger.info(f"  Content Length: {len(content)} å­—ç¬¦")
                self.logger.info(f"  Tool Calls Count: {len(tool_calls_raw)}")
                
                # æ˜¾ç¤º usage ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                usage = data.get("usage", {})
                if usage:
                    self.logger.info(f"  Token Usage:")
                    self.logger.info(f"    - Prompt Tokens: {usage.get('prompt_tokens', 'N/A')}")
                    self.logger.info(f"    - Completion Tokens: {usage.get('completion_tokens', 'N/A')}")
                    self.logger.info(f"    - Total Tokens: {usage.get('total_tokens', 'N/A')}")
                
                # æ˜¾ç¤ºå“åº”å†…å®¹å‰ 200 å­—ç¬¦ï¼ˆç”¨äºéªŒè¯ï¼‰
                if content:
                    preview = content[:200] + ("..." if len(content) > 200 else "")
                    self.logger.info(f"  Content Preview: {preview}")
                
                self.logger.info("=" * 60)
                
                # è§„èŒƒåŒ–å·¥å…·è°ƒç”¨æ ¼å¼
                tool_calls = []
                for tc in tool_calls_raw:
                    if tc.get("type") == "function":
                        fn = tc.get("function", {})
                        tool_calls.append({
                            "id": tc.get("id") or f"tool_{int(datetime.now().timestamp())}",
                            "type": "function",
                            "function": {
                                "name": fn.get("name"),
                                "arguments": fn.get("arguments", "{}")
                            }
                        })
                return {"content": content, "tool_calls": tool_calls}

            # å“åº”ç»“æ„å¼‚å¸¸æ—¶çš„ fallback
            return {"content": content if 'content' in locals() else "", "tool_calls": []}

        except Exception as e:
            self.logger.error(f"LLM çœŸå®è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼ fallback: {e}", exc_info=True)
            self.logger.error(f"LLM é…ç½® - Base URL: {self.config.llm.base_url}")
            self.logger.error(f"LLM é…ç½® - Model: {config.get('model', self.config.llm.models.default)}")
            self.logger.error(f"LLM é…ç½® - API Key å·²è®¾ç½®: {bool(self.config.llm.api_key)}")

        # ==== Fallback å¯å‘å¼é€»è¾‘ ====
        if "search" in user_message.lower() or "æœç´¢" in user_message:
            return {
                "content": "",
                "tool_calls": [
                    {
                        "id": "search_1",
                        "type": "function",
                        "function": {
                            "name": "search_tool",
                            "arguments": json.dumps({"query": user_message})
                        }
                    }
                ]
            }
        if "calculate" in user_message.lower() or "è®¡ç®—" in user_message or any(char in user_message for char in "+-*/"):
            return {
                "content": "",
                "tool_calls": [
                    {
                        "id": "calc_1",
                        "type": "function",
                        "function": {
                            "name": "calculator",
                            "arguments": json.dumps({"expression": user_message})
                        }
                    }
                ]
            }
        
        # é»˜è®¤ fallback å“åº”
        return {
            "content": f"æˆ‘ç†è§£æ‚¨è¯´çš„æ˜¯ï¼š'{user_message}'",
            "tool_calls": []
        }

    async def stream_llm_call(self, messages: List[Dict[str, str]], config: Dict[str, Any], session_id: Optional[str] = None):
        """æµå¼è°ƒç”¨ LLM å¹¶ç”Ÿæˆå¢é‡å“åº”äº‹ä»¶ã€‚

        ä»¥æµå¼æ–¹å¼è°ƒç”¨ LLM API,é€æ­¥ç”Ÿæˆå“åº”å†…å®¹ã€‚è¿”å›ç‰ˆæœ¬åŒ–äº‹ä»¶,åŒ…å«äº‹ä»¶ IDã€æ—¶é—´æˆ³ç­‰å…ƒæ•°æ®ã€‚
        å¦‚æœæµå¼è°ƒç”¨å¤±è´¥,è‡ªåŠ¨ fallback åˆ°éæµå¼è°ƒç”¨ã€‚

        Args:
            messages: æ¶ˆæ¯å†å²åˆ—è¡¨
            config: LLM é…ç½®(model, temperature, max_tokens ç­‰)
            session_id: ä¼šè¯ ID(å¯é€‰)

        Yields:
            äº‹ä»¶å­—å…¸,åŒ…å« version, id, timestamp, type ç­‰å­—æ®µ:
            - start: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'start', model: '...'}
            - delta: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'delta', content: str}
            - tool_calls: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'tool_calls', tool_calls: [...]}
            - end: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'end', content: full_text}
            - error: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'error', error: msg}
        """
        # ğŸ†• æ€è€ƒé˜¶æ®µï¼šå‡†å¤‡è°ƒç”¨ LLM
        if self.trace and session_id:
            yield self.trace.thinking_phase("å‡†å¤‡ LLM æµå¼è°ƒç”¨", "call_llm", session_id)
        
        # å¯¼å…¥äº‹ä»¶å·¥å…·å‡½æ•°
        try:
            from api.event_utils import (
                create_start_event, create_delta_event, create_end_event,
                create_error_event, create_tool_calls_event
            )
        except ImportError:
            # å¦‚æœ event_utils ä¸å¯ç”¨,ä½¿ç”¨ fallback æ ¼å¼
            from datetime import datetime
            import uuid
            def create_start_event(sid=None, model=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "start"}
                if model: evt["model"] = model
                if sid: evt["session_id"] = sid
                return evt
            def create_delta_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "delta", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_end_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "end", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_error_event(error, sid=None, error_code=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "error", "error": error}
                if sid: evt["session_id"] = sid
                if error_code: evt["error_code"] = error_code
                return evt
            def create_tool_calls_event(tool_calls, sid=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "tool_calls", "tool_calls": tool_calls}
                if sid: evt["session_id"] = sid
                return evt
        
        user_message = messages[-1]["content"] if messages else ""
        full_text = []
        yielded_tool_calls = False
        model = config.get("model", self.config.llm.models.default)
        
        # å°è¯•æµå¼è°ƒç”¨
        try:
            # ç¡®ä¿ HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            await self._ensure_http_client()

            # è·å–å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰
            self.logger.info(f"ğŸ” [Stream] Loading tools schema for streaming mode...")
            tools_schema = self._get_tools_schema()
            self.logger.info(f"ğŸ” [Stream] Tools schema loaded: {len(tools_schema) if tools_schema else 0} tools")

            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                stream=True,
                tools=tools_schema if tools_schema else None  # ä¼ é€’å·¥å…·å®šä¹‰
            )
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - æµå¼è¯·æ±‚å‚æ•°
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¤ [STREAM] LLM API è¯·æ±‚å‚æ•°:")
            self.logger.info(f"  Model: {payload.get('model')}")
            self.logger.info(f"  Max Tokens: {payload.get('max_tokens') or payload.get('max_completion_tokens')}")
            self.logger.info(f"  Temperature: {payload.get('temperature', 'N/A')}")
            self.logger.info(f"  Messages Count: {len(messages)}")
            self.logger.info(f"  Tools Count: {len(tools_schema) if tools_schema else 0}")
            self.logger.info(f"  Stream Mode: True")
            
            # ä¼°ç®—è¾“å…¥ token
            total_chars = sum(len(str(m.get('content', ''))) for m in messages)
            estimated_input_tokens = int(total_chars / 2)
            self.logger.info(f"  ä¼°ç®—è¾“å…¥ Tokens: ~{estimated_input_tokens}")
            self.logger.info("=" * 60)
            
            # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ  tool_choice
            if tools_schema:
                payload["tool_choice"] = "auto"
                self.logger.info(f"ğŸ”§ [Stream] Added {len(tools_schema)} tools to streaming LLM request")
            else:
                self.logger.warning(f"âš ï¸ [Stream] No tools available for streaming LLM request")
            
            # ä½¿ç”¨æå–çš„ URL æ„å»ºæ–¹æ³•
            url = self._build_llm_url()
            
            self.logger.debug(f"LLM æµå¼è°ƒç”¨ç›®æ ‡: {url}")
            
            yield create_start_event(session_id=session_id, model=model)
            
            # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆæµå¼è¿”å›æ—¶å¯èƒ½åˆ†æ•£åœ¨å¤šä¸ª delta ä¸­ï¼‰
            collected_tool_calls = []
            
            async with self._http_client.stream('POST', url, json=payload) as resp:
                if resp.status_code >= 400:
                    text = await resp.aread()
                    raise RuntimeError(f"æµå¼ HTTP è¯·æ±‚å¤±è´¥ {resp.status_code}: {text[:200]}")
                async for line in resp.aiter_lines():
                    if not line:
                        continue
                    if line.startswith('data:'):
                        data_part = line[5:].strip()
                        if data_part == '[DONE]':
                            break
                        try:
                            data_json = json.loads(data_part)
                        except json.JSONDecodeError:
                            continue
                        for choice in data_json.get('choices', []):
                            delta = choice.get('delta', {})
                            
                            # å¤„ç†æ–‡æœ¬å†…å®¹
                            if 'content' in delta and delta['content']:
                                piece = delta['content']
                                full_text.append(piece)
                                yield create_delta_event(content=piece, session_id=session_id)
                            
                            # æ”¶é›†å·¥å…·è°ƒç”¨ï¼ˆOpenAI æµå¼æ ¼å¼ï¼‰
                            if 'tool_calls' in delta:
                                for tc_delta in delta['tool_calls']:
                                    idx = tc_delta.get('index', 0)
                                    # ç¡®ä¿ list è¶³å¤Ÿé•¿
                                    while len(collected_tool_calls) <= idx:
                                        collected_tool_calls.append({
                                            'id': None,
                                            'type': 'function',
                                            'function': {'name': '', 'arguments': ''}
                                        })
                                    
                                    # ç´¯ç§¯ id
                                    if 'id' in tc_delta:
                                        collected_tool_calls[idx]['id'] = tc_delta['id']
                                    
                                    # ç´¯ç§¯ function name
                                    if 'function' in tc_delta:
                                        fn = tc_delta['function']
                                        if 'name' in fn:
                                            collected_tool_calls[idx]['function']['name'] += fn['name']
                                        if 'arguments' in fn:
                                            collected_tool_calls[idx]['function']['arguments'] += fn['arguments']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if collected_tool_calls:
                self.logger.info(f"ğŸ”§ [Stream] Detected {len(collected_tool_calls)} tool call(s), executing...")
                
                # ğŸ†• æ€è€ƒé˜¶æ®µï¼šæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨
                if self.trace and session_id:
                    yield self.trace.llm_streaming("æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨", session_id, f"å…± {len(collected_tool_calls)} ä¸ªå·¥å…·")
                
                # é€šçŸ¥å‰ç«¯å·¥å…·è°ƒç”¨å¼€å§‹
                yield create_tool_calls_event(tool_calls=collected_tool_calls, session_id=session_id)
                
                # æ‰§è¡Œæ‰€æœ‰å·¥å…·
                tool_results = []
                for tc in collected_tool_calls:
                    try:
                        # è½¬æ¢ä¸º ToolCall å¯¹è±¡
                        tool_call = ToolCall(
                            id=tc.get('id') or f"tool_{int(datetime.now().timestamp())}",
                            name=tc['function']['name'],
                            arguments=json.loads(tc['function']['arguments']) if tc['function']['arguments'] else {}
                        )
                        
                        # ğŸ†• å·¥å…·è°ƒç”¨æ’é˜Ÿäº‹ä»¶
                        if self.trace and session_id:
                            yield self.trace.tool_call_pending(tool_call.name, tool_call.arguments, session_id)
                        
                        # ğŸ†• å·¥å…·æ‰§è¡Œä¸­äº‹ä»¶
                        if self.trace and session_id:
                            yield self.trace.tool_executing(tool_call.name, session_id)
                        
                        import time
                        tool_start_time = time.time()
                        
                        # æ‰§è¡Œå·¥å…·
                        result = await self._execute_tool_call(tool_call)
                        
                        tool_duration = (time.time() - tool_start_time) * 1000
                        
                        # æ ¼å¼åŒ–å·¥å…·ç»“æœå†…å®¹ï¼ˆä½¿ç”¨ result å±æ€§ï¼Œä¸æ˜¯ dataï¼‰
                        if result.success:
                            # result.result å¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²æˆ–å…¶ä»–ç±»å‹
                            if isinstance(result.result, str):
                                result_content = result.result
                            elif isinstance(result.result, (dict, list)):
                                result_content = json.dumps(result.result, ensure_ascii=False)
                            else:
                                result_content = str(result.result)
                        else:
                            result_content = f"Error: {result.error}"
                        
                        # ğŸ†• å·¥å…·ç»“æœäº‹ä»¶
                        if self.trace and session_id:
                            summary = result_content[:100] + "..." if len(result_content) > 100 else result_content
                            yield self.trace.tool_result(
                                tool_call.name, 
                                result.success, 
                                summary, 
                                session_id,
                                tool_duration
                            )
                        
                        tool_results.append({
                            'tool_call_id': tool_call.id,
                            'role': 'tool',
                            'name': tool_call.name,
                            'content': result_content
                        })
                        
                        self.logger.info(f"âœ… [Stream] Tool '{tool_call.name}' executed successfully, result length: {len(result_content)}")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ [Stream] Tool execution failed: {e}")
                        
                        # ğŸ†• å·¥å…·å¤±è´¥äº‹ä»¶
                        if self.trace and session_id:
                            yield self.trace.tool_result(
                                tc['function']['name'],
                                False,
                                f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                                session_id
                            )
                        
                        tool_results.append({
                            'tool_call_id': tc.get('id', 'unknown'),
                            'role': 'tool',
                            'name': tc['function']['name'],
                            'content': f"Error: {str(e)}"
                        })
                
                # ğŸ†• æ€è€ƒé˜¶æ®µï¼šåŸºäºå·¥å…·ç»“æœå†æ¬¡è°ƒç”¨ LLM
                if self.trace and session_id:
                    yield self.trace.llm_streaming("åŸºäºå·¥å…·ç»“æœé‡æ–°æ€è€ƒ", session_id)
                
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼Œå†æ¬¡è°ƒç”¨ LLMï¼ˆæµå¼ï¼‰
                self.logger.info(f"ğŸ”„ [Stream] Calling LLM again with tool results...")
                
                # æ„å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
                new_messages = messages + [
                    {
                        'role': 'assistant',
                        'content': None,
                        'tool_calls': collected_tool_calls
                    }
                ] + tool_results
                
                # è°ƒè¯•ï¼šæ‰“å°æ¶ˆæ¯ç»“æ„
                self.logger.info(f"ğŸ“‹ [Stream] Final message count: {len(new_messages)}")
                self.logger.info(f"ğŸ“‹ [Stream] Last 3 messages roles: {[m.get('role', 'unknown') for m in new_messages[-3:]]}")
                
                # é€’å½’è°ƒç”¨è‡ªå·±ï¼Œä½†ä¸ä¼ é€’å·¥å…·ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
                config_no_tools = config.copy()
                
                # é‡æ–°è°ƒç”¨ï¼ˆè¿™æ¬¡æ˜¯æµå¼è¿”å›å·¥å…·å¤„ç†åçš„ç»“æœï¼‰
                async for event in self._stream_llm_with_tool_results(new_messages, config_no_tools, session_id):
                    yield event
                
                return
            
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸ç»“æŸ
            final_content = ''.join(full_text)
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - æµå¼å“åº”æ€»ç»“
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¥ [STREAM] LLM æµå¼å“åº”æ€»ç»“:")
            self.logger.info(f"  Total Content Length: {len(final_content)} å­—ç¬¦")
            self.logger.info(f"  Delta Events Count: {len(full_text)}")
            self.logger.info(f"  Tool Calls: {len(collected_tool_calls)}")
            
            # æ˜¾ç¤ºå†…å®¹å‰ 200 å­—ç¬¦
            if final_content:
                preview = final_content[:200] + ("..." if len(final_content) > 200 else "")
                self.logger.info(f"  Content Preview: {preview}")
            
            # âš ï¸ æ³¨æ„ï¼šæµå¼å“åº”é€šå¸¸ä¸è¿”å› finish_reason
            # å¦‚æœå†…å®¹çœ‹èµ·æ¥è¢«æˆªæ–­ï¼ˆçªç„¶ç»“æŸï¼‰ï¼Œå¯èƒ½æ˜¯ max_tokens é™åˆ¶
            if len(final_content) > 5000:
                self.logger.warning("  âš ï¸ å“åº”å†…å®¹è¾ƒé•¿ï¼Œå¦‚æœçœ‹èµ·æ¥ä¸å®Œæ•´ï¼Œå¯èƒ½æ˜¯ max_tokens é™åˆ¶")
            
            self.logger.info("=" * 60)
            
            yield create_end_event(content=final_content, session_id=session_id)
            return
        except Exception as e:
            self.logger.warning(f"æµå¼è°ƒç”¨å¤±è´¥,å›é€€åˆ°éæµå¼: {e}")
            if not full_text:
                # å›é€€åˆ°æ™®é€šè°ƒç”¨
                try:
                    result = await self._make_llm_call(messages, config)
                    content = result.get('content', '')
                    yield create_delta_event(content=content, session_id=session_id)
                    if result.get('tool_calls'):
                        yield create_tool_calls_event(tool_calls=result['tool_calls'], session_id=session_id)
                    yield create_end_event(content=content, session_id=session_id)
                    return
                except Exception as e2:
                    yield create_error_event(error=str(e2), session_id=session_id)
                    return
            else:
                yield create_end_event(content=''.join(full_text), session_id=session_id)
    
    async def _stream_llm_with_tool_results(self, messages: List[Dict], config: Dict, session_id: Optional[str] = None):
        """åœ¨å·¥å…·è°ƒç”¨åç»§ç»­æµå¼è¿”å› LLM çš„æœ€ç»ˆå“åº”ï¼ˆä¸å†ä¼ é€’å·¥å…·ï¼‰"""
        self.logger.info(f"ğŸ¯ [Stream] Starting _stream_llm_with_tool_results with {len(messages)} messages")
        
        try:
            from api.event_utils import create_delta_event, create_end_event, create_error_event
        except ImportError:
            from datetime import datetime
            import uuid
            def create_delta_event(content, sid=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "delta", "content": content}
                if sid: evt["session_id"] = sid
                return evt
            def create_end_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "end", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_error_event(error, sid=None, error_code=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "error", "error": error}
                if sid: evt["session_id"] = sid
                if error_code: evt["error_code"] = error_code
                return evt
        
        try:
            await self._ensure_http_client()
            
            # ä¸å†ä¼ é€’å·¥å…·ï¼Œé¿å…æ— é™é€’å½’
            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                stream=True
                # tools=None  # æ˜ç¡®ä¸ä¼ é€’å·¥å…·
            )
            
            url = self._build_llm_url()
            full_response = []
            
            self.logger.info(f"ğŸŒ [Stream] Calling LLM API for tool result processing...")
            
            async with self._http_client.stream('POST', url, json=payload) as resp:
                if resp.status_code >= 400:
                    text = await resp.aread()
                    raise RuntimeError(f"æµå¼ HTTP è¯·æ±‚å¤±è´¥ {resp.status_code}: {text[:200]}")
                
                async for line in resp.aiter_lines():
                    if not line or not line.startswith('data:'):
                        continue
                    
                    data_part = line[5:].strip()
                    if data_part == '[DONE]':
                        break
                    
                    try:
                        data_json = json.loads(data_part)
                    except json.JSONDecodeError:
                        continue
                    
                    for choice in data_json.get('choices', []):
                        delta = choice.get('delta', {})
                        if 'content' in delta and delta['content']:
                            piece = delta['content']
                            full_response.append(piece)
                            self.logger.debug(f"ğŸ“¤ [Stream] Yielding delta: {piece[:50]}...")
                            yield create_delta_event(content=piece, session_id=session_id)
            
            final_content = ''.join(full_response)
            self.logger.info(f"âœ… [Stream] Tool result processing complete, total length: {len(final_content)}")
            yield create_end_event(content=final_content, session_id=session_id)
            
        except Exception as e:
            self.logger.error(f"å·¥å…·ç»“æœæµå¼è°ƒç”¨å¤±è´¥: {e}")
            yield create_error_event(error=str(e), session_id=session_id)
    
    def _has_tool_calls(self, response: Dict[str, Any]) -> bool:
        """Check if LLM response contains tool calls."""
        return bool(response.get("tool_calls"))
    
    def _extract_tool_calls(self, response: Dict[str, Any]) -> List[ToolCall]:
        """Extract tool calls from LLM response."""
        tool_calls = []
        
        for call_data in response.get("tool_calls", []):
            if call_data.get("type") == "function":
                function_data = call_data.get("function", {})
                try:
                    arguments = json.loads(function_data.get("arguments", "{}"))
                except json.JSONDecodeError:
                    arguments = {}
                
                tool_call = ToolCall(
                    id=call_data.get("id", f"tool_{int(datetime.now().timestamp())}"),
                    name=function_data.get("name", "unknown"),
                    arguments=arguments
                )
                tool_calls.append(tool_call)
        
        return tool_calls
    
    async def _execute_tool_call(self, tool_call: ToolCall) -> ToolResult:
        """Execute a tool call using MCP tool registry."""
        try:
            # Try to use MCP tool registry
            try:
                from mcp import get_tool_registry
                
                registry = get_tool_registry()
                
                # Map common LLM tool names to MCP tool names
                tool_name_mapping = {
                    "search_tool": "web_search",
                    "web_search": "web_search",
                    "calculator": "calculator",
                    "time_tool": "get_time",
                    "get_time": "get_time",
                    "weather": "get_weather",
                    "get_weather": "get_weather",
                }
                
                mcp_tool_name = tool_name_mapping.get(tool_call.name, tool_call.name)
                
                # Execute through registry
                result_dict = await registry.execute(mcp_tool_name, **tool_call.arguments)
                
                # Convert MCP ToolResult to Agent ToolResult
                if result_dict.get("success"):
                    result_str = json.dumps(result_dict.get("data", {}), ensure_ascii=False)
                else:
                    result_str = None
                
                return ToolResult(
                    call_id=tool_call.id,
                    success=result_dict.get("success", False),
                    result=result_str,
                    error=result_dict.get("error")
                )
            
            except ImportError:
                self.logger.warning("MCP tools not available, using fallback implementation")
                # Fall back to placeholder implementation
                pass
            
            # Fallback placeholder implementation
            if tool_call.name in ["search_tool", "web_search"]:
                result = f"Search results for: {tool_call.arguments.get('query', 'unknown')}"
            elif tool_call.name == "calculator":
                expression = tool_call.arguments.get('expression', '')
                try:
                    result = f"The result is: {expression} (calculation placeholder)"
                except:
                    result = "Could not calculate the expression"
            elif tool_call.name in ["time_tool", "get_time"]:
                result = f"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            else:
                result = f"Tool '{tool_call.name}' executed with arguments: {tool_call.arguments}"
            
            return ToolResult(
                call_id=tool_call.id,
                success=True,
                result=result
            )
            
        except Exception as e:
            self.logger.error(f"Tool execution error: {e}", exc_info=True)
            return ToolResult(
                call_id=tool_call.id,
                success=False,
                result=None,
                error=str(e)
            )