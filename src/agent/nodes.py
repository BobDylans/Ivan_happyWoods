"""
Agent Nodes Implementation

This module contains the core LangGraph nodes that handle different stages
of conversation processing, including input processing, LLM calls, tool handling,
and response formatting.
"""

import json
import logging
from typing import Dict, Any, List, Optional, Union
import asyncio
import httpx
from datetime import datetime

from .state import AgentState, ConversationMessage, MessageRole, ToolCall, ToolResult

# ÂØºÂÖ• LLM ÂÖºÂÆπÊÄßÂ∑•ÂÖ∑
try:
    from utils.llm_compat import prepare_llm_params
except ImportError:
    # Â¶ÇÊûúÂØºÂÖ•Â§±Ë¥•ÔºåÊèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂÖºÂÆπÂáΩÊï∞
    def prepare_llm_params(model, messages, temperature=0.7, max_tokens=2048, **kwargs):
        params = {
            "model": model,
            "messages": messages,
        }
        # GPT-5 Á≥ªÂàó‰∏ç‰º† temperatureÔºå‰ΩøÁî® API ÈªòËÆ§ÂÄº
        if not model.startswith("gpt-5"):
            params["temperature"] = temperature
        # GPT-5 Á≥ªÂàó‰ΩøÁî® max_completion_tokens
        if model.startswith("gpt-5"):
            params["max_completion_tokens"] = max_tokens
        else:
            params["max_tokens"] = max_tokens
        params.update(kwargs)
        return params


class DateTimeJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder that handles datetime objects."""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)

try:
    from config.models import VoiceAgentConfig
except ImportError:
    # Fallback for when running as script
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from config.models import VoiceAgentConfig


logger = logging.getLogger(__name__)


class AgentNodes:
    """LangGraph ÂØπËØùÂ§ÑÁêÜËäÇÁÇπÈõÜÂêà
    
    Ë¥üË¥£ÂØπËØùÊµÅÁ®ã‰∏≠ÁöÑÂêÑ‰∏™Â§ÑÁêÜÈò∂ÊÆµÔºö
    - ËæìÂÖ•Â§ÑÁêÜÂíåÈ™åËØÅ
    - LLM Ë∞ÉÁî®ÔºàÂêåÊ≠•/ÊµÅÂºèÔºâ
    - Â∑•ÂÖ∑Ë∞ÉÁî®Â§ÑÁêÜ
    - ÂìçÂ∫îÊ†ºÂºèÂåñ
    """
    
    def __init__(self, config: VoiceAgentConfig):
        """ÂàùÂßãÂåñËäÇÁÇπÈÖçÁΩÆ
        
        Args:
            config: ËØ≠Èü≥Âä©ÊâãÈÖçÁΩÆÂØπË±°
        """
        self.config = config
        self.logger = logger
        self._http_client: Optional[httpx.AsyncClient] = None
        self._client_lock = asyncio.Lock()
    
    async def _ensure_http_client(self):
        """Á°Æ‰øù HTTP ÂÆ¢Êà∑Á´ØÂ∑≤ÂàùÂßãÂåñÔºàÊáíÂä†ËΩΩÔºâ
        
        ‰ΩøÁî®ÂèåÈáçÊ£ÄÊü•ÈîÅÂÆöÊ®°ÂºèÁ°Æ‰øùÁ∫øÁ®ãÂÆâÂÖ®ÁöÑÂçï‰æãÂàùÂßãÂåñ„ÄÇ
        """
        if self._http_client is None:
            async with self._client_lock:
                if self._http_client is None:  # ÂèåÈáçÊ£ÄÊü•
                    timeout = httpx.Timeout(self.config.llm.timeout, connect=10)
                    self._http_client = httpx.AsyncClient(
                        timeout=timeout,
                        headers={
                            "Authorization": f"Bearer {self.config.llm.api_key}",
                            "Content-Type": "application/json"
                        }
                    )
                    self.logger.debug("HTTP ÂÆ¢Êà∑Á´ØÂàùÂßãÂåñÊàêÂäü")
    
    def _build_llm_url(self, endpoint: str = "chat/completions") -> str:
        """ÊûÑÂª∫ LLM API ÂÆåÊï¥ URL
        
        Ëá™Âä®Â§ÑÁêÜ base_url ‰∏≠ÊòØÂê¶ÂåÖÂê´ /v1 ÁöÑÊÉÖÂÜµ„ÄÇ
        
        Args:
            endpoint: API Á´ØÁÇπË∑ØÂæÑÔºåÈªòËÆ§‰∏∫ "chat/completions"
        
        Returns:
            ÂÆåÊï¥ÁöÑ API URL
        
        Examples:
            >>> # base_url = "https://api.openai-proxy.org/v1"
            >>> self._build_llm_url()
            "https://api.openai-proxy.org/v1/chat/completions"
            
            >>> # base_url = "https://api.openai-proxy.org"
            >>> self._build_llm_url()
            "https://api.openai-proxy.org/v1/chat/completions"
        """
        base = self.config.llm.base_url.rstrip('/')
        
        # ‰ªÖÂú® base_url ‰∏çÂåÖÂê´ /v1 Êó∂Ê∑ªÂä†
        if not base.endswith('/v1'):
            base = base + '/v1'
        
        url = f"{base}/{endpoint}"
        return url
    
    async def cleanup(self):
        """Ê∏ÖÁêÜËµÑÊ∫ê
        
        ÂÖ≥Èó≠ HTTP ÂÆ¢Êà∑Á´ØËøûÊé•ÔºåÈáäÊîæËµÑÊ∫ê„ÄÇ
        Â∫îÂú®Á®ãÂ∫èÈÄÄÂá∫ÊàñÊúçÂä°ÂÅúÊ≠¢Êó∂Ë∞ÉÁî®„ÄÇ
        """
        if self._http_client:
            try:
                await self._http_client.aclose()
                self.logger.debug("HTTP ÂÆ¢Êà∑Á´ØÂ∑≤ÂÖ≥Èó≠")
            except Exception as e:
                self.logger.warning(f"ÂÖ≥Èó≠ HTTP ÂÆ¢Êà∑Á´ØÊó∂Âá∫Èîô: {e}")
            finally:
                self._http_client = None
    
    async def __aenter__(self):
        """ÂºÇÊ≠•‰∏ä‰∏ãÊñáÁÆ°ÁêÜÂô®ÂÖ•Âè£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ÂºÇÊ≠•‰∏ä‰∏ãÊñáÁÆ°ÁêÜÂô®Âá∫Âè£ÔºåËá™Âä®Ê∏ÖÁêÜËµÑÊ∫ê"""
        await self.cleanup()
    async def process_input(self, state: AgentState) -> AgentState:
        """Â§ÑÁêÜÂíåÈ™åËØÅÁî®Êà∑ËæìÂÖ•
        
        ËøôÊòØÂØπËØùÂ§ÑÁêÜÊµÅÁ®ãÁöÑÂÖ•Âè£ËäÇÁÇπÔºåË¥üË¥£Ôºö
        1. È™åËØÅËæìÂÖ•‰∏ç‰∏∫Á©∫
        2. ÂàõÂª∫Áî®Êà∑Ê∂àÊÅØÂØπË±°
        3. ÂàùÊ≠•ËØÜÂà´Áî®Êà∑ÊÑèÂõæ
        4. Êõ¥Êñ∞Áä∂ÊÄÅÂáÜÂ§áË∞ÉÁî® LLM
        
        Args:
            state: ÂΩìÂâçÂØπËØùÁä∂ÊÄÅ
        
        Returns:
            Êõ¥Êñ∞ÂêéÁöÑÂØπËØùÁä∂ÊÄÅ
        """
        try:
            self.logger.debug(f"Â§ÑÁêÜ‰ºöËØù {state['session_id']} ÁöÑËæìÂÖ•")
            
            # Êõ¥Êñ∞Êó∂Èó¥Êà≥
            state["last_activity"] = datetime.now()
            
            # ËßÑËåÉÂåñËæìÂÖ•ÔºåÁ°Æ‰øù‰∏ç‰∏∫Á©∫
            user_input = state["user_input"].strip()
            if not user_input:
                state["error_state"] = "empty_input"
                state["should_continue"] = False
                state["agent_response"] = "ÊàëÊ≤°ÊúâÊî∂Âà∞‰ªª‰ΩïËæìÂÖ•ÔºåËØ∑ËØ¥ÁÇπ‰ªÄ‰πàÂêß„ÄÇ"
                return state
            
            # Â∞ÜÁî®Êà∑Ê∂àÊÅØÊ∑ªÂä†Âà∞ÂØπËØùÂéÜÂè≤
            user_message = ConversationMessage(
                id=f"user_{len(state['messages']) + 1}_{int(datetime.now().timestamp())}",
                role=MessageRole.USER,
                content=user_input,
                metadata={"processed_at": datetime.now().isoformat()}
            )
            state["messages"].append(user_message)
            
            # ÈÄöËøáÂÖ≥ÈîÆËØçÂàùÊ≠•ËØÜÂà´Áî®Êà∑ÊÑèÂõæ
            state["current_intent"] = self._analyze_intent(user_input)
            
            # ËÆæÁΩÆ‰∏ã‰∏ÄÊ≠•Âä®‰ΩúÔºöË∞ÉÁî® LLM
            state["next_action"] = "call_llm"
            
            self.logger.debug(f"ËæìÂÖ•Â§ÑÁêÜÂÆåÊàêÔºåÊÑèÂõæ: {state['current_intent']}")
            return state
            
        except Exception as e:
            self.logger.error(f"ËæìÂÖ•Â§ÑÁêÜÈîôËØØ: {e}")
            state["error_state"] = f"input_processing_error: {str(e)}"
            state["should_continue"] = False
            return state
    
    async def call_llm(self, state: AgentState) -> AgentState:
        """Ë∞ÉÁî®Â§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂìçÂ∫î
        
        ËøôÊòØÊ†∏ÂøÉÂ§ÑÁêÜËäÇÁÇπÔºåË¥üË¥£Ôºö
        1. ÂáÜÂ§áÂØπËØùÂéÜÂè≤Ê∂àÊÅØ
        2. ÈÖçÁΩÆÊ®°ÂûãÂèÇÊï∞
        3. Ë∞ÉÁî® LLM API
        4. Â§ÑÁêÜÂìçÂ∫îÔºàÊñáÊú¨ÊàñÂ∑•ÂÖ∑Ë∞ÉÁî®Ôºâ
        
        Args:
            state: ÂΩìÂâçÂØπËØùÁä∂ÊÄÅ
        
        Returns:
            Êõ¥Êñ∞ÂêéÁöÑÂØπËØùÁä∂ÊÄÅ
        """
        try:
            self.logger.debug(f"‰∏∫‰ºöËØù {state['session_id']} Ë∞ÉÁî® LLM")
            
            # ÂáÜÂ§á LLM Ê∂àÊÅØÔºàÂåÖÂê´ÂØπËØùÂéÜÂè≤Ôºâ
            # Â¶ÇÊûú state ‰∏≠Êúâ external_historyÔºå‰º†ÈÄíÁªô _prepare_llm_messages
            external_history = state.get("external_history")
            if external_history is not None:
                self.logger.info(f"üîç Found external_history in state: {len(external_history)} messages")
            else:
                self.logger.warning(f"‚ö†Ô∏è No external_history found in state for session {state['session_id']}")
            messages = self._prepare_llm_messages(state, external_history=external_history)
            
            # ÈÖçÁΩÆÊ®°ÂûãÂèÇÊï∞Ôºà‰ΩøÁî®ÂÖºÂÆπÂ±ÇÂ§ÑÁêÜ‰∏çÂêåÊ®°ÂûãÁöÑÂèÇÊï∞Â∑ÆÂºÇÔºâ
            model = state["model_config"].get("model", self.config.llm.models.default)
            max_tokens = state.get("max_tokens", self.config.llm.max_tokens)
            temperature = state.get("temperature", self.config.llm.temperature)
            
            llm_config = prepare_llm_params(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # Ë∞ÉÁî® LLM API Ëé∑ÂèñÂìçÂ∫î
            response = await self._make_llm_call(messages, llm_config)
            
            # Âà§Êñ≠ÂìçÂ∫îÁ±ªÂûãÔºöÂ∑•ÂÖ∑Ë∞ÉÁî® or Áõ¥Êé•ÂõûÂ§ç
            if self._has_tool_calls(response):
                state["next_action"] = "handle_tools"
                # ÊèêÂèñÂ∑•ÂÖ∑Ë∞ÉÁî®ËØ∑Ê±ÇÂπ∂Âä†ÂÖ•ÂæÖÂ§ÑÁêÜÈòüÂàó
                tool_calls = self._extract_tool_calls(response)
                state["pending_tool_calls"].extend(tool_calls)
            else:
                state["next_action"] = "format_response"
                state["agent_response"] = response.get("content", "")
            
            self.logger.debug("LLM Ë∞ÉÁî®ÂÆåÊàê")
            return state
            
        except Exception as e:
            self.logger.error(f"LLM Ë∞ÉÁî®ÈîôËØØ: {e}")
            state["error_state"] = f"llm_call_error: {str(e)}"
            state["agent_response"] = "Êä±Ê≠âÔºåÊàëÂú®Â§ÑÁêÜÊÇ®ÁöÑËØ∑Ê±ÇÊó∂ÈÅáÂà∞‰∫ÜÈóÆÈ¢òÔºåËØ∑Á®çÂêéÂÜçËØï„ÄÇ"
            state["next_action"] = "format_response"
            return state
    
    async def handle_tools(self, state: AgentState) -> AgentState:
        """Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ËØ∑Ê±Ç
        
        ÂΩì LLM ÈúÄË¶Å‰ΩøÁî®Â∑•ÂÖ∑Êó∂ÔºåÊ≠§ËäÇÁÇπË¥üË¥£Ôºö
        1. ÊâßË°åÊâÄÊúâÂæÖÂ§ÑÁêÜÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®
        2. Êî∂ÈõÜÂ∑•ÂÖ∑ÊâßË°åÁªìÊûú
        3. Â∞ÜÁªìÊûúÊ∑ªÂä†Âà∞ÂØπËØùÂéÜÂè≤
        4. ÂáÜÂ§áÂÜçÊ¨°Ë∞ÉÁî® LLMÔºàËÆ©ÂÆÉÂ§ÑÁêÜÂ∑•ÂÖ∑ÁªìÊûúÔºâ
        
        Args:
            state: ÂΩìÂâçÂØπËØùÁä∂ÊÄÅ
        
        Returns:
            Êõ¥Êñ∞ÂêéÁöÑÂØπËØùÁä∂ÊÄÅ
        """
        try:
            self.logger.debug(f"Â§ÑÁêÜ‰ºöËØù {state['session_id']} ÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®")
            
            if not state["pending_tool_calls"]:
                self.logger.warning("Ê≤°ÊúâÂæÖÂ§ÑÁêÜÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®")
                state["next_action"] = "call_llm"
                return state
            
            # ÈÄê‰∏™ÊâßË°åÂ∑•ÂÖ∑Ë∞ÉÁî®
            for tool_call in state["pending_tool_calls"]:
                result = await self._execute_tool_call(tool_call)
                state["tool_results"].append(result)
                state["tool_calls"].append(tool_call)
            
            # Ê∏ÖÁ©∫ÂæÖÂ§ÑÁêÜÈòüÂàó
            state["pending_tool_calls"] = []
            
            # Â∞ÜÂ∑•ÂÖ∑ÊâßË°åÁªìÊûúÊ∑ªÂä†Âà∞ÂØπËØùÂéÜÂè≤
            for result in state["tool_results"][-len(state["tool_calls"]):]:
                tool_message = ConversationMessage(
                    id=f"tool_{result.call_id}_{int(datetime.now().timestamp())}",
                    role=MessageRole.TOOL,
                    content=json.dumps(result.dict(), cls=DateTimeJSONEncoder),
                    metadata={"tool_call_id": result.call_id, "success": result.success}
                )
                state["messages"].append(tool_message)
            
            # ÁªßÁª≠Ë∞ÉÁî® LLM Â§ÑÁêÜÂ∑•ÂÖ∑ÁªìÊûú
            state["next_action"] = "call_llm"
            
            self.logger.debug(f"Â∑≤Â§ÑÁêÜ {len(state['tool_calls'])} ‰∏™Â∑•ÂÖ∑Ë∞ÉÁî®")
            return state
            
        except Exception as e:
            self.logger.error(f"Â∑•ÂÖ∑Â§ÑÁêÜÈîôËØØ: {e}")
            state["error_state"] = f"tool_handling_error: {str(e)}"
            state["agent_response"] = "Êä±Ê≠âÔºåÂú®‰ΩøÁî®Â∑•ÂÖ∑Êó∂ÈÅáÂà∞‰∫ÜÈóÆÈ¢òÔºåËÆ©ÊàëÊç¢‰∏™ÊñπÂºèÂ∏ÆÊÇ®„ÄÇ"
            state["next_action"] = "format_response"
            return state
    
    async def format_response(self, state: AgentState) -> AgentState:
        """Ê†ºÂºèÂåñÊúÄÁªàÂìçÂ∫î
        
        ËøôÊòØÊµÅÁ®ãÁöÑÊúÄÂêé‰∏Ä‰∏™ËäÇÁÇπÔºåË¥üË¥£Ôºö
        1. Á°Æ‰øùÊúâÂìçÂ∫îÂÜÖÂÆπ
        2. ÂàõÂª∫Âä©ÊâãÊ∂àÊÅØÂØπË±°
        3. Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
        4. Ê†áËÆ∞ÂØπËØùÂõûÂêàÁªìÊùü
        
        Args:
            state: ÂΩìÂâçÂØπËØùÁä∂ÊÄÅ
        
        Returns:
            ÊúÄÁªàÁöÑÂØπËØùÁä∂ÊÄÅ
        """
        try:
            self.logger.debug(f"Ê†ºÂºèÂåñ‰ºöËØù {state['session_id']} ÁöÑÂìçÂ∫î")
            
            # Á°Æ‰øùÊúâÂìçÂ∫îÂÜÖÂÆπ
            if not state["agent_response"]:
                if state["error_state"]:
                    state["agent_response"] = "Êä±Ê≠âÔºåÂ§ÑÁêÜÊÇ®ÁöÑËØ∑Ê±ÇÊó∂Âá∫Áé∞‰∫ÜÈîôËØØ„ÄÇ"
                else:
                    state["agent_response"] = "Êàë‰∏çÂ§™Á°ÆÂÆöÂ¶Ç‰ΩïÂõûÁ≠îÔºåËØ∑Êç¢‰∏™ÊñπÂºèÈóÆÊàëÂêß„ÄÇ"
            
            # ÂàõÂª∫Âä©ÊâãÊ∂àÊÅØÂπ∂Ê∑ªÂä†Âà∞ÂéÜÂè≤
            assistant_message = ConversationMessage(
                id=f"assistant_{len(state['messages']) + 1}_{int(datetime.now().timestamp())}",
                role=MessageRole.ASSISTANT,
                content=state["agent_response"],
                metadata={
                    "generated_at": datetime.now().isoformat(),
                    "model": state["model_config"].get("model", "unknown"),
                    "intent": state.get("current_intent"),
                    "tool_calls_count": len(state["tool_calls"])
                }
            )
            state["messages"].append(assistant_message)
            
            # Êõ¥Êñ∞Ê¥ªÂä®Êó∂Èó¥Êà≥
            state["last_activity"] = datetime.now()
            
            # Ê†áËÆ∞ÂØπËØùÂõûÂêàÂÆåÊàê
            state["should_continue"] = False
            state["next_action"] = None
            
            self.logger.debug("ÂìçÂ∫îÊ†ºÂºèÂåñÂÆåÊàê")
            return state
            
        except Exception as e:
            self.logger.error(f"ÂìçÂ∫îÊ†ºÂºèÂåñÈîôËØØ: {e}")
            state["error_state"] = f"response_formatting_error: {str(e)}"
            state["agent_response"] = "Êä±Ê≠âÔºåÂìçÂ∫îÊ†ºÂºèÂåñÊó∂Âá∫Áé∞‰∫ÜÈóÆÈ¢ò„ÄÇ"
            state["should_continue"] = False
            return state
    
    def _analyze_intent(self, user_input: str) -> Optional[str]:
        """ÂàÜÊûêÁî®Êà∑ÊÑèÂõæÔºàÂü∫‰∫éÂÖ≥ÈîÆËØçÁöÑÁÆÄÂçïÂÆûÁé∞Ôºâ
        
        Ê≥®ÊÑèÔºöËøôÊòØ‰∏Ä‰∏™ÁÆÄÂåñÁâàÊú¨ÁöÑÊÑèÂõæËØÜÂà´Ôºå‰ªÖÁî®‰∫éÂü∫Á°ÄÂàÜÁ±ª„ÄÇ
        Áîü‰∫ßÁéØÂ¢ÉÂª∫ËÆÆ‰ΩøÁî® NLU Ê®°ÂûãÊàñ LLM ËøõË°åÊÑèÂõæËØÜÂà´„ÄÇ
        
        Args:
            user_input: Áî®Êà∑ËæìÂÖ•ÊñáÊú¨
        
        Returns:
            ËØÜÂà´ÁöÑÊÑèÂõæÊ†áÁ≠æÔºåÂ¶Ç "search", "calculation" Á≠â
        """
        input_lower = user_input.lower()
        
        # Âü∫‰∫éÂÖ≥ÈîÆËØçÁöÑÁÆÄÂçïÊÑèÂõæÊ£ÄÊµã
        if any(word in input_lower for word in ["search", "find", "look", "ÊêúÁ¥¢", "Êü•Êâæ"]):
            return "search"
        elif any(word in input_lower for word in ["calculate", "math", "compute", "ËÆ°ÁÆó"]):
            return "calculation"
        elif any(word in input_lower for word in ["time", "date", "when", "Êó∂Èó¥", "Êó•Êúü"]):
            return "time_query"
        elif any(word in input_lower for word in ["image", "picture", "generate", "create", "ÂõæÁâá", "ÁîüÊàê"]):
            return "image_generation"
        elif any(word in input_lower for word in ["help", "what", "how", "Â∏ÆÂä©", "ÊÄé‰πà"]):
            return "help_request"
        else:
            return "general_conversation"
    
    def _prepare_llm_messages(self, state: AgentState, external_history: List[Dict] = None) -> List[Dict[str, str]]:
        """ÂáÜÂ§á LLM API Ë∞ÉÁî®ÁöÑÊ∂àÊÅØÂàóË°®
        
        ÂåÖÂê´‰ºòÂåñÁöÑÁ≥ªÁªüÊèêÁ§∫ËØçÂíåÂéÜÂè≤ÂØπËØù„ÄÇ‰ºòÂÖà‰ΩøÁî®Â§ñÈÉ®‰º†ÂÖ•ÁöÑÂéÜÂè≤ËÆ∞ÂΩï„ÄÇ
        
        Args:
            state: ÂΩìÂâçÂØπËØùÁä∂ÊÄÅ
            external_history: Â§ñÈÉ®‰º†ÂÖ•ÁöÑÂéÜÂè≤Ê∂àÊÅØÂàóË°® (ÂèØÈÄâ)
        
        Returns:
            Ê†ºÂºèÂåñÁöÑÊ∂àÊÅØÂàóË°®ÔºåÁ¨¶Âêà OpenAI API Ê†ºÂºè
        """
        messages = []
        
        # ÊûÑÂª∫‰ºòÂåñÁöÑÁ≥ªÁªüÊèêÁ§∫ËØç
        system_prompt = self._build_optimized_system_prompt(state)
        system_message = {
            "role": "system",
            "content": system_prompt
        }
        messages.append(system_message)
        
        # ‰ºòÂÖà‰ΩøÁî®Â§ñÈÉ®ÂéÜÂè≤Ôºà‰ªé SessionHistoryManagerÔºâ
        if external_history is not None:
            # ÈôêÂà∂ÂéÜÂè≤Ê∂àÊÅØÊï∞ÈáèÔºàÊúÄËøë 10 Êù°Ôºâ
            MAX_HISTORY_MESSAGES = 10
            recent_history = external_history[-MAX_HISTORY_MESSAGES:] if external_history else []
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            self.logger.info(f"‚úÖ Loaded {len(recent_history)} messages from external history for LLM")
            
            # ÈáçË¶ÅÔºöÊ∑ªÂä†ÂΩìÂâçÁî®Êà∑Ê∂àÊÅØÔºàÊù•Ëá™ state["messages"] ÁöÑÊúÄÂêé‰∏ÄÊù°Ôºâ
            if state["messages"] and state["messages"][-1].role == MessageRole.USER:
                current_user_msg = state["messages"][-1]
                messages.append({
                    "role": "user",
                    "content": current_user_msg.content
                })
                self.logger.info(f"‚úÖ Added current user message to LLM input")
        else:
            # ÂõûÈÄÄÂà∞ state ‰∏≠ÁöÑÊ∂àÊÅØÔºàÂ¶ÇÊûúÊúâÔºâ
            self.logger.info("‚ö†Ô∏è No external history provided, using state messages")
            MAX_HISTORY_MESSAGES = 10
            recent_messages = state["messages"][-MAX_HISTORY_MESSAGES:]
            for msg in recent_messages:
                if msg.role in [MessageRole.USER, MessageRole.ASSISTANT]:
                    messages.append({
                        "role": msg.role.value,
                        "content": msg.content
                    })
            self.logger.info(f"üìù Using {len(recent_messages)} messages from state")
        
        return messages
    
    def _build_optimized_system_prompt(self, state: AgentState) -> str:
        """ÊûÑÂª∫‰ºòÂåñÁöÑÁ≥ªÁªüÊèêÁ§∫ËØçÔºåÊèêÂçáÊô∫ËÉΩÊÄßÂíåÊïàÁéá
        
        ‰ºòÂåñÁ≠ñÁï•Ôºö
        1. ÊòéÁ°ÆËßíËâ≤ÂÆö‰ΩçÂíåËÉΩÂäõËæπÁïå
        2. Êèê‰æõÊ∏ÖÊô∞ÁöÑÂ∑•ÂÖ∑‰ΩøÁî®ÊåáÂçó
        3. Âº∫Ë∞ÉÊïàÁéáÂíåÂáÜÁ°ÆÊÄß
        4. ÂåÖÂê´‰ªªÂä°ÂàÜËß£ÂíåÊé®ÁêÜÊ°ÜÊû∂
        5. Ê†πÊçÆ‰∏ä‰∏ãÊñáÂä®ÊÄÅË∞ÉÊï¥ÊèêÁ§∫ËØç
        
        Args:
            state: ÂΩìÂâçÂØπËØùÁä∂ÊÄÅ
        
        Returns:
            ‰ºòÂåñÂêéÁöÑÁ≥ªÁªüÊèêÁ§∫ËØçÂ≠óÁ¨¶‰∏≤
        """
        # Âü∫Á°ÄË∫´‰ªΩÂÆö‰πâ
        base_identity = """# ËßíËâ≤ÂÆö‰Ωç
‰Ω†ÊòØ‰∏Ä‰∏™È´òÊïà„ÄÅÊô∫ËÉΩÁöÑÂ§öÂäüËÉΩ AI Âä©ÊâãÔºåÂÖ∑Â§á‰ª•‰∏ãÊ†∏ÂøÉËÉΩÂäõÔºö
- Ëá™ÁÑ∂ÊµÅÁïÖÁöÑ‰∏≠Ëã±ÊñáÂØπËØù
- Êô∫ËÉΩÂ∑•ÂÖ∑Ë∞ÉÁî®Âíå‰ªªÂä°ÁºñÊéí
- ÁªìÊûÑÂåñÈóÆÈ¢òÂàÜÊûêÂíåËß£ÂÜ≥
- ‰∏ä‰∏ãÊñáÁêÜËß£ÂíåËÆ∞ÂøÜ‰øùÊåÅ

# Ê†∏ÂøÉÂéüÂàô
1. **ÊïàÁéá‰ºòÂÖà**: Áî®ÊúÄÂ∞ëÁöÑÊ≠•È™§ËææÊàêÁõÆÊ†áÔºåÈÅøÂÖçÂÜó‰ΩôÊìç‰Ωú
2. **ÂáÜÁ°ÆËá≥‰∏ä**: ‰ºòÂÖà‰øùËØÅ‰ø°ÊÅØÂáÜÁ°ÆÊÄßÔºå‰∏çÁ°ÆÂÆöÊó∂ÊòéÁ°ÆÂëäÁü•Áî®Êà∑
3. **‰∏ªÂä®ÊÄùËÄÉ**: ÁêÜËß£Áî®Êà∑ÊÑèÂõæÔºåÂøÖË¶ÅÊó∂‰∏ªÂä®ÊæÑÊ∏ÖÈúÄÊ±Ç
4. **Â∑•ÂÖ∑Êô∫Áî®**: ÂêàÁêÜÂà§Êñ≠‰ΩïÊó∂ÈúÄË¶ÅÂ∑•ÂÖ∑ÔºåÈÅøÂÖç‰∏çÂøÖË¶ÅÁöÑË∞ÉÁî®

# üìù ÂõûÂ§çÊ†ºÂºèËßÑËåÉÔºàÈáçË¶ÅÔºâ
**ËØ∑Âä°ÂøÖ‰ΩøÁî® Markdown Ê†ºÂºèÁªÑÁªá‰Ω†ÁöÑÂõûÂ§çÔºåÊèêÂçáÂèØËØªÊÄßÔºö**

1. **‰ΩøÁî®Ê†áÈ¢òÂàÜÂ±Ç**: Áî® `##` Êàñ `###` Ê†áÊ≥®ÊÆµËêΩ‰∏ªÈ¢ò
2. **ÂàóË°®ÂëàÁé∞Ë¶ÅÁÇπ**: Áî® `-` Êàñ `1.` Âàó‰∏æ‰ø°ÊÅØ
3. **Âº∫Ë∞ÉÂÖ≥ÈîÆ‰ø°ÊÅØ**: Áî® `**Á≤ó‰Ωì**` Á™ÅÂá∫ÈáçÁÇπ
4. **‰ª£Á†ÅÂùó**: ÊäÄÊúØÂÜÖÂÆπÁî® ` ```ËØ≠Ë®Ä ``` ` ÂåÖË£π
5. **ÂºïÁî®Êù•Ê∫ê**: ÊêúÁ¥¢ÁªìÊûúÁî® `> ÂºïÁî®` Ê†ºÂºè
6. **ÈìæÊé•Ê†ºÂºè**: Áî® `[Ê†áÈ¢ò](URL)` Â±ïÁ§∫ÈìæÊé•

**Á§∫‰æãÂõûÂ§çÊ†ºÂºè**:
```
## üìä ÊêúÁ¥¢ÁªìÊûú

Ê†πÊçÆÊúÄÊñ∞‰ø°ÊÅØÔºå‰ª•‰∏ãÊòØÂÖ≥‰∫é [‰∏ªÈ¢ò] ÁöÑË¶ÅÁÇπÔºö

### 1. [Á¨¨‰∏Ä‰∏™Ë¶ÅÁÇπ]
- **ÂÖ≥ÈîÆ‰ø°ÊÅØ**: xxx
- **Êó∂Èó¥**: xxx
- **Êù•Ê∫ê**: [Êñ∞ÈóªÊ†áÈ¢ò](ÈìæÊé•)

### 2. [Á¨¨‰∫å‰∏™Ë¶ÅÁÇπ]
...

---
üí° **ÊÄªÁªì**: ÁÆÄÁü≠ÊÄªÁªìÂÖ≥ÈîÆ‰ø°ÊÅØ
```

**ÂØπ‰∫éÊêúÁ¥¢ÁªìÊûúÔºåÁâπÂà´Ê≥®ÊÑè**Ôºö
- Áî®Ê∏ÖÊô∞ÁöÑÊ†áÈ¢òÂíåÂ∫èÂè∑ÁªÑÁªá
- ÊØèÊù°Êñ∞ÈóªÂåÖÂê´ÔºöÊ†áÈ¢ò„ÄÅÊëòË¶Å„ÄÅÊù•Ê∫êÈìæÊé•
- Áî®ÂàÜÈöîÁ∫ø `---` Âå∫ÂàÜ‰∏çÂêåÈÉ®ÂàÜ
- ÈÅøÂÖçÈïøÊÆµËêΩÔºåÂ§öÁî®ÂàóË°®"""

        # Ëé∑ÂèñÂèØÁî®Â∑•ÂÖ∑ÂàóË°®
        available_tools = self._format_available_tools()
        
        tools_guide = f"""

# ÂèØÁî®Â∑•ÂÖ∑
{available_tools}

# Â∑•ÂÖ∑‰ΩøÁî®Á≠ñÁï•
**‰ΩïÊó∂‰ΩøÁî®Â∑•ÂÖ∑**:
- ÈúÄË¶ÅÂÆûÊó∂‰ø°ÊÅØÔºàÂ§©Ê∞î„ÄÅÊó∂Èó¥„ÄÅÊêúÁ¥¢ÔºâÊó∂ ‚Üí ÂøÖÈ°ª‰ΩøÁî®Â∑•ÂÖ∑
- ÈúÄË¶ÅÂ§çÊùÇËÆ°ÁÆóÊàñÊï∞ÊçÆÂ§ÑÁêÜÊó∂ ‚Üí ‰ΩøÁî®ËÆ°ÁÆóÂô®Â∑•ÂÖ∑
- Áî®Êà∑ÊòéÁ°ÆË¶ÅÊ±ÇÊâßË°åÁâπÂÆöÊìç‰ΩúÊó∂ ‚Üí ‰ΩøÁî®ÂØπÂ∫îÂ∑•ÂÖ∑

**‰ΩïÊó∂‰∏ç‰ΩøÁî®Â∑•ÂÖ∑**:
- ÂõûÁ≠îÂ∏∏ËØÜÊÄßÈóÆÈ¢òÊàñ‰∏ÄËà¨ÊÄßÂØπËØù ‚Üí Áõ¥Êé•ÂõûÁ≠î
- ÁÆÄÂçïÁöÑÂøÉÁÆóÊàñÈÄªËæëÊé®ÁêÜ ‚Üí Áõ¥Êé•ÂõûÁ≠î
- ÈúÄË¶ÅÂàõÊÑèÊàñÂª∫ËÆÆÊó∂ ‚Üí Áõ¥Êé•ÂõûÁ≠î

**Â∑•ÂÖ∑Ë∞ÉÁî®ÂéüÂàô**:
1. ‰∏ÄÊ¨°Âè™Ë∞ÉÁî®ÁúüÊ≠£ÈúÄË¶ÅÁöÑÂ∑•ÂÖ∑
2. ‰ºòÂÖà‰ΩøÁî®ÊúÄÂêàÈÄÇÁöÑÂçï‰∏™Â∑•ÂÖ∑ÔºåËÄåÈùûÂ§ö‰∏™Â∑•ÂÖ∑
3. Â∑•ÂÖ∑Ë∞ÉÁî®ÂêéÔºåÂü∫‰∫éÁªìÊûúÁªôÂá∫Ê∏ÖÊô∞„ÄÅÊúâ‰ª∑ÂÄºÁöÑÂõûÁ≠î"""

        # ‰ªªÂä°Â§ÑÁêÜÊ°ÜÊû∂
        task_framework = """

# ‰ªªÂä°Â§ÑÁêÜÊ°ÜÊû∂
ÂØπ‰∫éÂ§çÊùÇËØ∑Ê±ÇÔºåÈÅµÂæ™‰ª•‰∏ãÊÄùÁª¥ÊµÅÁ®ãÔºö
1. **ÁêÜËß£**: ÂáÜÁ°ÆËØÜÂà´Áî®Êà∑ÁúüÂÆûÈúÄÊ±ÇÂíåÊÑèÂõæ
2. **ËßÑÂàí**: Á°ÆÂÆöÊòØÂê¶ÈúÄË¶ÅÂ∑•ÂÖ∑ÔºåÈúÄË¶ÅÂì™‰∫õÂ∑•ÂÖ∑
3. **ÊâßË°å**: È´òÊïàË∞ÉÁî®ÂøÖË¶ÅÁöÑÂ∑•ÂÖ∑Ëé∑Âèñ‰ø°ÊÅØ
4. **ÁªºÂêà**: Êï¥ÂêàÂ∑•ÂÖ∑ÁªìÊûúÔºåÊèê‰æõÊúâ‰ª∑ÂÄºÁöÑÂõûÁ≠î
5. **È™åËØÅ**: Á°Æ‰øùÂõûÁ≠îÂÆåÊï¥„ÄÅÂáÜÁ°ÆÂú∞Ëß£ÂÜ≥‰∫ÜÁî®Êà∑ÈóÆÈ¢ò

# ÂìçÂ∫îË¥®ÈáèÊ†áÂáÜ
‚úÖ ‰ºòË¥®ÂõûÁ≠îÂ∫îËØ•Ôºö
- Áõ¥Êé•ÈíàÂØπÁî®Êà∑ÈóÆÈ¢òÔºåÈÅøÂÖçÂï∞Âó¶
- ÁªìÊûÑÊ∏ÖÊô∞ÔºàÂøÖË¶ÅÊó∂‰ΩøÁî®ÂàóË°®„ÄÅÂàÜÁÇπÔºâ
- ‰ø°ÊÅØÂáÜÁ°ÆÔºåÊù•Ê∫êÂèØÈù†
- ËØ≠Ê∞îÂèãÂ•Ω„ÄÅ‰∏ì‰∏ö

‚ùå ÈÅøÂÖçÔºö
- ËøáÂ∫¶ÂÜóÈïøÊàñÈáçÂ§çÁöÑËß£Èáä
- ‰∏çÂøÖË¶ÅÁöÑÈÅìÊ≠âÊàñË∞¶ÈÄäË°®Ëææ
- Ê®°Á≥ä‰∏çÊ∏ÖÁöÑÂõûÁ≠î
- Ë∞ÉÁî®‰∏çÁõ∏ÂÖ≥ÁöÑÂ∑•ÂÖ∑"""

        # ‰∏ä‰∏ãÊñáÊÑüÁü•‰ºòÂåñ
        context_optimization = self._build_context_aware_addition(state)
        
        # ÁªÑÂêàÂÆåÊï¥ÊèêÁ§∫ËØç
        full_prompt = base_identity + tools_guide + task_framework
        
        if context_optimization:
            full_prompt += "\n\n" + context_optimization
        
        return full_prompt
    
    def _get_tools_schema(self) -> List[Dict]:
        """Ëé∑ÂèñÂ∑•ÂÖ∑ÁöÑ OpenAI Function Calling Ê†ºÂºèÂÆö‰πâ
        
        Returns:
            Â∑•ÂÖ∑ÂÆö‰πâÂàóË°®ÔºåOpenAI tools Ê†ºÂºè
        """
        try:
            from mcp import get_tool_registry
            registry = get_tool_registry()
            tools = registry.list_tools()
            
            if not tools:
                return []
            
            # ËΩ¨Êç¢‰∏∫ OpenAI Function Calling Ê†ºÂºè
            tools_schema = []
            for tool in tools:
                schema = tool.to_openai_schema()
                tools_schema.append(schema)
            
            self.logger.info(f"‚úÖ Loaded {len(tools_schema)} tools for LLM")
            return tools_schema
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load tools schema: {e}", exc_info=True)
            return []
    
    def _format_available_tools(self) -> str:
        """Ê†ºÂºèÂåñÂèØÁî®Â∑•ÂÖ∑ÂàóË°®‰∏∫ÊòìËØªÁöÑÊñáÊú¨
        
        Returns:
            Ê†ºÂºèÂåñÁöÑÂ∑•ÂÖ∑ÂàóË°®Â≠óÁ¨¶‰∏≤
        """
        try:
            from mcp import get_tool_registry
            registry = get_tool_registry()
            tools = registry.list_tools()
            
            if not tools:
                return "ÂΩìÂâçÊöÇÊó†ÂèØÁî®Â∑•ÂÖ∑„ÄÇ"
            
            tool_descriptions = []
            for tool in tools:
                name = tool.name
                desc = tool.description
                # ÁÆÄÂåñÊèèËø∞ÔºåÂè™‰øùÁïôÂÖ≥ÈîÆ‰ø°ÊÅØ
                short_desc = desc.split('.')[0] if desc else "Êó†ÊèèËø∞"
                tool_descriptions.append(f"- **{name}**: {short_desc}")
            
            return "\n".join(tool_descriptions)
        
        except Exception as e:
            self.logger.warning(f"Ëé∑ÂèñÂ∑•ÂÖ∑ÂàóË°®Â§±Ë¥•: {e}")
            return "- **calculator**: ÊâßË°åÊï∞Â≠¶ËÆ°ÁÆó\n- **get_time**: Ëé∑ÂèñÂΩìÂâçÊó∂Èó¥\n- **get_weather**: Êü•ËØ¢Â§©Ê∞î‰ø°ÊÅØ\n- **web_search**: ÊêúÁ¥¢ÁΩëÁªú‰ø°ÊÅØ"
    
    def _build_context_aware_addition(self, state: AgentState) -> str:
        """Ê†πÊçÆÂΩìÂâçÂØπËØù‰∏ä‰∏ãÊñáÊûÑÂª∫È¢ùÂ§ñÁöÑÊèêÁ§∫ËØçÂ¢ûÂº∫
        
        Args:
            state: ÂΩìÂâçÂØπËØùÁä∂ÊÄÅ
        
        Returns:
            ‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÁöÑÈ¢ùÂ§ñÊèêÁ§∫ËØçÔºåÂ¶ÇÊûú‰∏çÈúÄË¶ÅÂàôËøîÂõûÁ©∫Â≠óÁ¨¶‰∏≤
        """
        additions = []
        
        # 1. Â¶ÇÊûúÊúâÂ∑•ÂÖ∑Ë∞ÉÁî®ÂéÜÂè≤ÔºåÊèêÈÜíÂü∫‰∫éÁªìÊûúÂõûÁ≠î
        if state.get("tool_calls") and len(state["tool_calls"]) > 0:
            additions.append(
                "# ÂΩìÂâçÁä∂ÊÄÅ\n"
                "‰Ω†ÂàöÂàöË∞ÉÁî®‰∫ÜÂ∑•ÂÖ∑Âπ∂Ëé∑Âæó‰∫ÜÁªìÊûú„ÄÇËØ∑Âü∫‰∫éÂ∑•ÂÖ∑ËøîÂõûÁöÑÂÆûÈôÖÊï∞ÊçÆÂõûÁ≠îÁî®Êà∑Ôºå"
                "‰∏çË¶ÅÁºñÈÄ†ÊàñÁåúÊµã‰ø°ÊÅØ„ÄÇÂ¶ÇÊûúÂ∑•ÂÖ∑ÁªìÊûú‰∏çÂÆåÊï¥ÔºåÂèØ‰ª•ÊòéÁ°ÆÂëäÁü•Áî®Êà∑„ÄÇ"
            )
        
        # 2. Â¶ÇÊûúÂØπËØùËΩÆÊ¨°ËæÉÂ§öÔºåÊèêÈÜí‰øùÊåÅËøûË¥ØÊÄß
        message_count = len(state.get("messages", []))
        if message_count > 6:
            additions.append(
                "# ÂØπËØùËøûË¥ØÊÄß\n"
                "ÂΩìÂâçÂØπËØùÂ∑≤ËøõË°åÂ§öËΩÆÔºåËØ∑‰øùÊåÅÂØπËØùËøûË¥ØÊÄßÂíå‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄß„ÄÇ"
                "Â¶ÇÊûúÁî®Êà∑ÊèêÂà∞'ÂÆÉ'„ÄÅ'Ëøô‰∏™'Á≠â‰ª£ËØçÔºåËØ∑ÁªìÂêà‰∏ä‰∏ãÊñáÁêÜËß£ÊâÄÊåáÂØπË±°„ÄÇ"
            )
        
        # 3. Â¶ÇÊûúÊ£ÄÊµãÂà∞ÁâπÂÆöÊÑèÂõæÔºåÁªôÂá∫ÈíàÂØπÊÄßÊåáÂØº
        intent = state.get("current_intent")
        if intent == "search":
            additions.append(
                "# ÊêúÁ¥¢‰ªªÂä°‰ºòÂåñ\n"
                "Áî®Êà∑ÈúÄË¶ÅÊêúÁ¥¢‰ø°ÊÅØ„ÄÇ‰ΩøÁî® web_search Â∑•ÂÖ∑ÂêéÔºåËØ∑Ôºö\n"
                "1. ÊÄªÁªìÂÖ≥ÈîÆ‰ø°ÊÅØÔºåËÄåÈùûÁÆÄÂçïÁΩóÂàóÁªìÊûú\n"
                "2. Â¶ÇÊûúÊúâ AI ÁîüÊàêÁöÑÊëòË¶ÅÔºå‰ºòÂÖà‰ΩøÁî®\n"
                "3. Êèê‰æõ 1-2 ‰∏™ÊúÄÁõ∏ÂÖ≥ÁöÑÈìæÊé•‰æõÁî®Êà∑ÂèÇËÄÉ"
            )
        elif intent == "calculation":
            additions.append(
                "# ËÆ°ÁÆó‰ªªÂä°‰ºòÂåñ\n"
                "Áî®Êà∑ÈúÄË¶ÅËøõË°åËÆ°ÁÆó„ÄÇÂØπ‰∫éÂ§çÊùÇË°®ËææÂºèÊàñÂ§öÊ≠•ËÆ°ÁÆóÔºåËØ∑‰ΩøÁî® calculator Â∑•ÂÖ∑„ÄÇ\n"
                "ÁÆÄÂçïÁÆóÊúØÔºàÂ¶Ç 2+2ÔºâÂèØÁõ¥Êé•ÂõûÁ≠îÔºå‰ΩÜÊ∂âÂèäÂ∞èÊï∞„ÄÅÂπÇÊ¨°„ÄÅ‰∏âËßíÂáΩÊï∞Á≠âÂ∫î‰ΩøÁî®Â∑•ÂÖ∑Á°Æ‰øùÁ≤æÂ∫¶„ÄÇ"
            )
        
        return "\n\n".join(additions) if additions else ""
    
    async def _make_llm_call(self, messages: List[Dict[str, str]], config: Dict[str, Any]) -> Dict[str, Any]:
        """Ë∞ÉÁî® LLM APIÔºàOpenAI ÂÖºÂÆπÔºâ
        
        Â¶ÇÊûúÁúüÂÆû HTTP Ë∞ÉÁî®Â§±Ë¥•Ôºå‰ºö‰ΩøÁî®Âü∫‰∫éÂÖ≥ÈîÆËØçÁöÑÂêØÂèëÂºè fallback„ÄÇ
        
        Args:
            messages: ÂØπËØùÊ∂àÊÅØÂàóË°®
            config: LLM ÈÖçÁΩÆÂèÇÊï∞
        
        Returns:
            ÂåÖÂê´ content (str) Âíå tool_calls (list) ÁöÑÂ≠óÂÖ∏
        """
        user_message = messages[-1]["content"] if messages else ""

        # ==== ‰∏ªË¶ÅË∑ØÂæÑÔºöÁúüÂÆû HTTP Ë∞ÉÁî® ====
        try:
            # Á°Æ‰øù HTTP ÂÆ¢Êà∑Á´ØÂ∑≤ÂàùÂßãÂåñ
            await self._ensure_http_client()

            # Ëé∑ÂèñÂ∑•ÂÖ∑ÂÆö‰πâÔºàOpenAI Ê†ºÂºèÔºâ
            self.logger.info(f"üîç Attempting to load tools schema...")
            tools_schema = self._get_tools_schema()
            self.logger.info(f"üîç Tools schema loaded: {len(tools_schema) if tools_schema else 0} tools")
            
            # ÂáÜÂ§áËØ∑Ê±ÇÂèÇÊï∞
            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                tools=tools_schema if tools_schema else None  # ‰º†ÈÄíÂ∑•ÂÖ∑ÂÆö‰πâ
            )
            
            # Â¶ÇÊûúÊúâÂ∑•ÂÖ∑ÔºåÊ∑ªÂä† tool_choice
            if tools_schema:
                payload["tool_choice"] = "auto"  # ËÆ©Ê®°ÂûãËá™Âä®ÂÜ≥ÂÆöÊòØÂê¶Ë∞ÉÁî®Â∑•ÂÖ∑
                self.logger.info(f"üîß Added {len(tools_schema)} tools to LLM request")
            else:
                self.logger.warning(f"‚ö†Ô∏è No tools available for LLM request")

            # ÊûÑÂª∫ÂÆåÊï¥ URL
            url = self._build_llm_url()
            self.logger.debug(f"LLM Ë∞ÉÁî®: {url}")

            # ÂèëÈÄÅËØ∑Ê±Ç
            resp = await self._http_client.post(url, json=payload)
            if resp.status_code >= 400:
                error_text = resp.text[:500]
                self.logger.error(f"LLM HTTP {resp.status_code}: {error_text}")
                raise RuntimeError(f"LLM HTTP {resp.status_code}: {error_text}")
            
            data = resp.json()
            self.logger.debug(f"LLM ÂìçÂ∫î: {len(data.get('choices', []))} ‰∏™ÈÄâÊã©")

            # Ëß£Êûê OpenAI È£éÊ†ºÁöÑÂìçÂ∫îÁªìÊûÑ
            choices = data.get("choices", [])
            if choices:
                first = choices[0]
                message_obj = first.get("message", {})
                content = message_obj.get("content") or ""
                tool_calls_raw = message_obj.get("tool_calls") or []
                
                # ËßÑËåÉÂåñÂ∑•ÂÖ∑Ë∞ÉÁî®Ê†ºÂºè
                tool_calls = []
                for tc in tool_calls_raw:
                    if tc.get("type") == "function":
                        fn = tc.get("function", {})
                        tool_calls.append({
                            "id": tc.get("id") or f"tool_{int(datetime.now().timestamp())}",
                            "type": "function",
                            "function": {
                                "name": fn.get("name"),
                                "arguments": fn.get("arguments", "{}")
                            }
                        })
                return {"content": content, "tool_calls": tool_calls}

            # ÂìçÂ∫îÁªìÊûÑÂºÇÂ∏∏Êó∂ÁöÑ fallback
            return {"content": content if 'content' in locals() else "", "tool_calls": []}

        except Exception as e:
            self.logger.error(f"LLM ÁúüÂÆûË∞ÉÁî®Â§±Ë¥•Ôºå‰ΩøÁî®ÂêØÂèëÂºè fallback: {e}", exc_info=True)
            self.logger.error(f"LLM ÈÖçÁΩÆ - Base URL: {self.config.llm.base_url}")
            self.logger.error(f"LLM ÈÖçÁΩÆ - Model: {config.get('model', self.config.llm.models.default)}")
            self.logger.error(f"LLM ÈÖçÁΩÆ - API Key Â∑≤ËÆæÁΩÆ: {bool(self.config.llm.api_key)}")

        # ==== Fallback ÂêØÂèëÂºèÈÄªËæë ====
        if "search" in user_message.lower() or "ÊêúÁ¥¢" in user_message:
            return {
                "content": "",
                "tool_calls": [
                    {
                        "id": "search_1",
                        "type": "function",
                        "function": {
                            "name": "search_tool",
                            "arguments": json.dumps({"query": user_message})
                        }
                    }
                ]
            }
        if "calculate" in user_message.lower() or "ËÆ°ÁÆó" in user_message or any(char in user_message for char in "+-*/"):
            return {
                "content": "",
                "tool_calls": [
                    {
                        "id": "calc_1",
                        "type": "function",
                        "function": {
                            "name": "calculator",
                            "arguments": json.dumps({"expression": user_message})
                        }
                    }
                ]
            }
        
        # ÈªòËÆ§ fallback ÂìçÂ∫î
        return {
            "content": f"ÊàëÁêÜËß£ÊÇ®ËØ¥ÁöÑÊòØÔºö'{user_message}'",
            "tool_calls": []
        }

    async def stream_llm_call(self, messages: List[Dict[str, str]], config: Dict[str, Any], session_id: Optional[str] = None):
        """ÊµÅÂºèË∞ÉÁî® LLM Âπ∂ÁîüÊàêÂ¢ûÈáèÂìçÂ∫î‰∫ã‰ª∂„ÄÇ

        ‰ª•ÊµÅÂºèÊñπÂºèË∞ÉÁî® LLM API,ÈÄêÊ≠•ÁîüÊàêÂìçÂ∫îÂÜÖÂÆπ„ÄÇËøîÂõûÁâàÊú¨Âåñ‰∫ã‰ª∂,ÂåÖÂê´‰∫ã‰ª∂ ID„ÄÅÊó∂Èó¥Êà≥Á≠âÂÖÉÊï∞ÊçÆ„ÄÇ
        Â¶ÇÊûúÊµÅÂºèË∞ÉÁî®Â§±Ë¥•,Ëá™Âä® fallback Âà∞ÈùûÊµÅÂºèË∞ÉÁî®„ÄÇ

        Args:
            messages: Ê∂àÊÅØÂéÜÂè≤ÂàóË°®
            config: LLM ÈÖçÁΩÆ(model, temperature, max_tokens Á≠â)
            session_id: ‰ºöËØù ID(ÂèØÈÄâ)

        Yields:
            ‰∫ã‰ª∂Â≠óÂÖ∏,ÂåÖÂê´ version, id, timestamp, type Á≠âÂ≠óÊÆµ:
            - start: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'start', model: '...'}
            - delta: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'delta', content: str}
            - tool_calls: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'tool_calls', tool_calls: [...]}
            - end: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'end', content: full_text}
            - error: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'error', error: msg}
        """
        # ÂØºÂÖ•‰∫ã‰ª∂Â∑•ÂÖ∑ÂáΩÊï∞
        try:
            from api.event_utils import (
                create_start_event, create_delta_event, create_end_event,
                create_error_event, create_tool_calls_event
            )
        except ImportError:
            # Â¶ÇÊûú event_utils ‰∏çÂèØÁî®,‰ΩøÁî® fallback Ê†ºÂºè
            from datetime import datetime
            import uuid
            def create_start_event(sid=None, model=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "start"}
                if model: evt["model"] = model
                if sid: evt["session_id"] = sid
                return evt
            def create_delta_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "delta", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_end_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "end", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_error_event(error, sid=None, error_code=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "error", "error": error}
                if sid: evt["session_id"] = sid
                if error_code: evt["error_code"] = error_code
                return evt
            def create_tool_calls_event(tool_calls, sid=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "tool_calls", "tool_calls": tool_calls}
                if sid: evt["session_id"] = sid
                return evt
        
        user_message = messages[-1]["content"] if messages else ""
        full_text = []
        yielded_tool_calls = False
        model = config.get("model", self.config.llm.models.default)
        
        # Â∞ùËØïÊµÅÂºèË∞ÉÁî®
        try:
            # Á°Æ‰øù HTTP ÂÆ¢Êà∑Á´ØÂ∑≤ÂàùÂßãÂåñ
            await self._ensure_http_client()

            # Ëé∑ÂèñÂ∑•ÂÖ∑ÂÆö‰πâÔºàOpenAI Ê†ºÂºèÔºâ
            self.logger.info(f"üîç [Stream] Loading tools schema for streaming mode...")
            tools_schema = self._get_tools_schema()
            self.logger.info(f"üîç [Stream] Tools schema loaded: {len(tools_schema) if tools_schema else 0} tools")

            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                stream=True,
                tools=tools_schema if tools_schema else None  # ‰º†ÈÄíÂ∑•ÂÖ∑ÂÆö‰πâ
            )
            
            # Â¶ÇÊûúÊúâÂ∑•ÂÖ∑ÔºåÊ∑ªÂä† tool_choice
            if tools_schema:
                payload["tool_choice"] = "auto"
                self.logger.info(f"üîß [Stream] Added {len(tools_schema)} tools to streaming LLM request")
            else:
                self.logger.warning(f"‚ö†Ô∏è [Stream] No tools available for streaming LLM request")
            
            # ‰ΩøÁî®ÊèêÂèñÁöÑ URL ÊûÑÂª∫ÊñπÊ≥ï
            url = self._build_llm_url()
            
            self.logger.debug(f"LLM ÊµÅÂºèË∞ÉÁî®ÁõÆÊ†á: {url}")
            
            yield create_start_event(session_id=session_id, model=model)
            
            # Êî∂ÈõÜÂ∑•ÂÖ∑Ë∞ÉÁî®‰ø°ÊÅØÔºàÊµÅÂºèËøîÂõûÊó∂ÂèØËÉΩÂàÜÊï£Âú®Â§ö‰∏™ delta ‰∏≠Ôºâ
            collected_tool_calls = []
            
            async with self._http_client.stream('POST', url, json=payload) as resp:
                if resp.status_code >= 400:
                    text = await resp.aread()
                    raise RuntimeError(f"ÊµÅÂºè HTTP ËØ∑Ê±ÇÂ§±Ë¥• {resp.status_code}: {text[:200]}")
                async for line in resp.aiter_lines():
                    if not line:
                        continue
                    if line.startswith('data:'):
                        data_part = line[5:].strip()
                        if data_part == '[DONE]':
                            break
                        try:
                            data_json = json.loads(data_part)
                        except json.JSONDecodeError:
                            continue
                        for choice in data_json.get('choices', []):
                            delta = choice.get('delta', {})
                            
                            # Â§ÑÁêÜÊñáÊú¨ÂÜÖÂÆπ
                            if 'content' in delta and delta['content']:
                                piece = delta['content']
                                full_text.append(piece)
                                yield create_delta_event(content=piece, session_id=session_id)
                            
                            # Êî∂ÈõÜÂ∑•ÂÖ∑Ë∞ÉÁî®ÔºàOpenAI ÊµÅÂºèÊ†ºÂºèÔºâ
                            if 'tool_calls' in delta:
                                for tc_delta in delta['tool_calls']:
                                    idx = tc_delta.get('index', 0)
                                    # Á°Æ‰øù list Ë∂≥Â§üÈïø
                                    while len(collected_tool_calls) <= idx:
                                        collected_tool_calls.append({
                                            'id': None,
                                            'type': 'function',
                                            'function': {'name': '', 'arguments': ''}
                                        })
                                    
                                    # Á¥ØÁßØ id
                                    if 'id' in tc_delta:
                                        collected_tool_calls[idx]['id'] = tc_delta['id']
                                    
                                    # Á¥ØÁßØ function name
                                    if 'function' in tc_delta:
                                        fn = tc_delta['function']
                                        if 'name' in fn:
                                            collected_tool_calls[idx]['function']['name'] += fn['name']
                                        if 'arguments' in fn:
                                            collected_tool_calls[idx]['function']['arguments'] += fn['arguments']
            
            # Ê£ÄÊü•ÊòØÂê¶ÊúâÂ∑•ÂÖ∑Ë∞ÉÁî®
            if collected_tool_calls:
                self.logger.info(f"üîß [Stream] Detected {len(collected_tool_calls)} tool call(s), executing...")
                
                # ÈÄöÁü•ÂâçÁ´ØÂ∑•ÂÖ∑Ë∞ÉÁî®ÂºÄÂßã
                yield create_tool_calls_event(tool_calls=collected_tool_calls, session_id=session_id)
                
                # ÊâßË°åÊâÄÊúâÂ∑•ÂÖ∑
                tool_results = []
                for tc in collected_tool_calls:
                    try:
                        # ËΩ¨Êç¢‰∏∫ ToolCall ÂØπË±°
                        tool_call = ToolCall(
                            id=tc.get('id') or f"tool_{int(datetime.now().timestamp())}",
                            name=tc['function']['name'],
                            arguments=json.loads(tc['function']['arguments']) if tc['function']['arguments'] else {}
                        )
                        
                        # ÊâßË°åÂ∑•ÂÖ∑
                        result = await self._execute_tool_call(tool_call)
                        
                        # Ê†ºÂºèÂåñÂ∑•ÂÖ∑ÁªìÊûúÂÜÖÂÆπÔºà‰ΩøÁî® result Â±ûÊÄßÔºå‰∏çÊòØ dataÔºâ
                        if result.success:
                            # result.result ÂèØËÉΩÊòØ JSON Â≠óÁ¨¶‰∏≤ÊàñÂÖ∂‰ªñÁ±ªÂûã
                            if isinstance(result.result, str):
                                result_content = result.result
                            elif isinstance(result.result, (dict, list)):
                                result_content = json.dumps(result.result, ensure_ascii=False)
                            else:
                                result_content = str(result.result)
                        else:
                            result_content = f"Error: {result.error}"
                        
                        tool_results.append({
                            'tool_call_id': tool_call.id,
                            'role': 'tool',
                            'name': tool_call.name,
                            'content': result_content
                        })
                        
                        self.logger.info(f"‚úÖ [Stream] Tool '{tool_call.name}' executed successfully, result length: {len(result_content)}")
                        
                    except Exception as e:
                        self.logger.error(f"‚ùå [Stream] Tool execution failed: {e}")
                        tool_results.append({
                            'tool_call_id': tc.get('id', 'unknown'),
                            'role': 'tool',
                            'name': tc['function']['name'],
                            'content': f"Error: {str(e)}"
                        })
                
                # Â∞ÜÂ∑•ÂÖ∑ÁªìÊûúÊ∑ªÂä†Âà∞Ê∂àÊÅØÂéÜÂè≤ÔºåÂÜçÊ¨°Ë∞ÉÁî® LLMÔºàÊµÅÂºèÔºâ
                self.logger.info(f"üîÑ [Stream] Calling LLM again with tool results...")
                
                # ÊûÑÂª∫Êñ∞ÁöÑÊ∂àÊÅØÂàóË°®
                new_messages = messages + [
                    {
                        'role': 'assistant',
                        'content': None,
                        'tool_calls': collected_tool_calls
                    }
                ] + tool_results
                
                # Ë∞ÉËØïÔºöÊâìÂç∞Ê∂àÊÅØÁªìÊûÑ
                self.logger.info(f"üìã [Stream] Final message count: {len(new_messages)}")
                self.logger.info(f"üìã [Stream] Last 3 messages roles: {[m.get('role', 'unknown') for m in new_messages[-3:]]}")
                
                # ÈÄíÂΩíË∞ÉÁî®Ëá™Â∑±Ôºå‰ΩÜ‰∏ç‰º†ÈÄíÂ∑•ÂÖ∑ÔºàÈÅøÂÖçÊó†ÈôêÂæ™ÁéØÔºâ
                config_no_tools = config.copy()
                
                # ÈáçÊñ∞Ë∞ÉÁî®ÔºàËøôÊ¨°ÊòØÊµÅÂºèËøîÂõûÂ∑•ÂÖ∑Â§ÑÁêÜÂêéÁöÑÁªìÊûúÔºâ
                async for event in self._stream_llm_with_tool_results(new_messages, config_no_tools, session_id):
                    yield event
                
                return
            
            # Ê≤°ÊúâÂ∑•ÂÖ∑Ë∞ÉÁî®ÔºåÊ≠£Â∏∏ÁªìÊùü
            yield create_end_event(content=''.join(full_text), session_id=session_id)
            return
        except Exception as e:
            self.logger.warning(f"ÊµÅÂºèË∞ÉÁî®Â§±Ë¥•,ÂõûÈÄÄÂà∞ÈùûÊµÅÂºè: {e}")
            if not full_text:
                # ÂõûÈÄÄÂà∞ÊôÆÈÄöË∞ÉÁî®
                try:
                    result = await self._make_llm_call(messages, config)
                    content = result.get('content', '')
                    yield create_delta_event(content=content, session_id=session_id)
                    if result.get('tool_calls'):
                        yield create_tool_calls_event(tool_calls=result['tool_calls'], session_id=session_id)
                    yield create_end_event(content=content, session_id=session_id)
                    return
                except Exception as e2:
                    yield create_error_event(error=str(e2), session_id=session_id)
                    return
            else:
                yield create_end_event(content=''.join(full_text), session_id=session_id)
    
    async def _stream_llm_with_tool_results(self, messages: List[Dict], config: Dict, session_id: Optional[str] = None):
        """Âú®Â∑•ÂÖ∑Ë∞ÉÁî®ÂêéÁªßÁª≠ÊµÅÂºèËøîÂõû LLM ÁöÑÊúÄÁªàÂìçÂ∫îÔºà‰∏çÂÜç‰º†ÈÄíÂ∑•ÂÖ∑Ôºâ"""
        self.logger.info(f"üéØ [Stream] Starting _stream_llm_with_tool_results with {len(messages)} messages")
        
        try:
            from api.event_utils import create_delta_event, create_end_event, create_error_event
        except ImportError:
            from datetime import datetime
            import uuid
            def create_delta_event(content, sid=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "delta", "content": content}
                if sid: evt["session_id"] = sid
                return evt
            def create_end_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "end", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_error_event(error, sid=None, error_code=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "error", "error": error}
                if sid: evt["session_id"] = sid
                if error_code: evt["error_code"] = error_code
                return evt
        
        try:
            await self._ensure_http_client()
            
            # ‰∏çÂÜç‰º†ÈÄíÂ∑•ÂÖ∑ÔºåÈÅøÂÖçÊó†ÈôêÈÄíÂΩí
            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                stream=True
                # tools=None  # ÊòéÁ°Æ‰∏ç‰º†ÈÄíÂ∑•ÂÖ∑
            )
            
            url = self._build_llm_url()
            full_response = []
            
            self.logger.info(f"üåê [Stream] Calling LLM API for tool result processing...")
            
            async with self._http_client.stream('POST', url, json=payload) as resp:
                if resp.status_code >= 400:
                    text = await resp.aread()
                    raise RuntimeError(f"ÊµÅÂºè HTTP ËØ∑Ê±ÇÂ§±Ë¥• {resp.status_code}: {text[:200]}")
                
                async for line in resp.aiter_lines():
                    if not line or not line.startswith('data:'):
                        continue
                    
                    data_part = line[5:].strip()
                    if data_part == '[DONE]':
                        break
                    
                    try:
                        data_json = json.loads(data_part)
                    except json.JSONDecodeError:
                        continue
                    
                    for choice in data_json.get('choices', []):
                        delta = choice.get('delta', {})
                        if 'content' in delta and delta['content']:
                            piece = delta['content']
                            full_response.append(piece)
                            self.logger.debug(f"üì§ [Stream] Yielding delta: {piece[:50]}...")
                            yield create_delta_event(content=piece, session_id=session_id)
            
            final_content = ''.join(full_response)
            self.logger.info(f"‚úÖ [Stream] Tool result processing complete, total length: {len(final_content)}")
            yield create_end_event(content=final_content, session_id=session_id)
            
        except Exception as e:
            self.logger.error(f"Â∑•ÂÖ∑ÁªìÊûúÊµÅÂºèË∞ÉÁî®Â§±Ë¥•: {e}")
            yield create_error_event(error=str(e), session_id=session_id)
    
    def _has_tool_calls(self, response: Dict[str, Any]) -> bool:
        """Check if LLM response contains tool calls."""
        return bool(response.get("tool_calls"))
    
    def _extract_tool_calls(self, response: Dict[str, Any]) -> List[ToolCall]:
        """Extract tool calls from LLM response."""
        tool_calls = []
        
        for call_data in response.get("tool_calls", []):
            if call_data.get("type") == "function":
                function_data = call_data.get("function", {})
                try:
                    arguments = json.loads(function_data.get("arguments", "{}"))
                except json.JSONDecodeError:
                    arguments = {}
                
                tool_call = ToolCall(
                    id=call_data.get("id", f"tool_{int(datetime.now().timestamp())}"),
                    name=function_data.get("name", "unknown"),
                    arguments=arguments
                )
                tool_calls.append(tool_call)
        
        return tool_calls
    
    async def _execute_tool_call(self, tool_call: ToolCall) -> ToolResult:
        """Execute a tool call using MCP tool registry."""
        try:
            # Try to use MCP tool registry
            try:
                from mcp import get_tool_registry
                
                registry = get_tool_registry()
                
                # Map common LLM tool names to MCP tool names
                tool_name_mapping = {
                    "search_tool": "web_search",
                    "web_search": "web_search",
                    "calculator": "calculator",
                    "time_tool": "get_time",
                    "get_time": "get_time",
                    "weather": "get_weather",
                    "get_weather": "get_weather",
                }
                
                mcp_tool_name = tool_name_mapping.get(tool_call.name, tool_call.name)
                
                # Execute through registry
                result_dict = await registry.execute(mcp_tool_name, **tool_call.arguments)
                
                # Convert MCP ToolResult to Agent ToolResult
                if result_dict.get("success"):
                    result_str = json.dumps(result_dict.get("data", {}), ensure_ascii=False)
                else:
                    result_str = None
                
                return ToolResult(
                    call_id=tool_call.id,
                    success=result_dict.get("success", False),
                    result=result_str,
                    error=result_dict.get("error")
                )
            
            except ImportError:
                self.logger.warning("MCP tools not available, using fallback implementation")
                # Fall back to placeholder implementation
                pass
            
            # Fallback placeholder implementation
            if tool_call.name in ["search_tool", "web_search"]:
                result = f"Search results for: {tool_call.arguments.get('query', 'unknown')}"
            elif tool_call.name == "calculator":
                expression = tool_call.arguments.get('expression', '')
                try:
                    result = f"The result is: {expression} (calculation placeholder)"
                except:
                    result = "Could not calculate the expression"
            elif tool_call.name in ["time_tool", "get_time"]:
                result = f"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            else:
                result = f"Tool '{tool_call.name}' executed with arguments: {tool_call.arguments}"
            
            return ToolResult(
                call_id=tool_call.id,
                success=True,
                result=result
            )
            
        except Exception as e:
            self.logger.error(f"Tool execution error: {e}", exc_info=True)
            return ToolResult(
                call_id=tool_call.id,
                success=False,
                result=None,
                error=str(e)
            )