"""
Agent Nodes Implementation

This module contains the core LangGraph nodes that handle different stages
of conversation processing, including input processing, LLM calls, tool handling,
and response formatting.
"""

import json
import logging
from typing import Dict, Any, List, Optional, Union
import asyncio
import httpx
from datetime import datetime

from .state import AgentState, ConversationMessage, MessageRole, ToolCall, ToolResult

# å¯¼å…¥ LLM å…¼å®¹æ€§å·¥å…·
try:
    from utils.llm_compat import prepare_llm_params
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªç®€å•çš„å…¼å®¹å‡½æ•°
    def prepare_llm_params(model, messages, temperature=0.7, max_tokens=16384, **kwargs):  # ğŸ”§ ä¿®å¤é»˜è®¤å€¼
        params = {
            "model": model,
            "messages": messages,
        }
        # GPT-5 ç³»åˆ—ä¸ä¼  temperatureï¼Œä½¿ç”¨ API é»˜è®¤å€¼
        if not model.startswith("gpt-5"):
            params["temperature"] = temperature
        # GPT-5 ç³»åˆ—ä½¿ç”¨ max_completion_tokens
        if model.startswith("gpt-5"):
            params["max_completion_tokens"] = max_tokens
        else:
            params["max_tokens"] = max_tokens
        params.update(kwargs)
        return params


class DateTimeJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder that handles datetime objects."""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)

try:
    from config.models import VoiceAgentConfig
except ImportError:
    # Fallback for when running as script
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from config.models import VoiceAgentConfig

try:
    from rag.service import RAGService
except ImportError:  # pragma: no cover - optional dependency
    RAGService = None  # type: ignore


logger = logging.getLogger(__name__)


class AgentNodes:
    """LangGraph å¯¹è¯å¤„ç†èŠ‚ç‚¹é›†åˆ
    
    è´Ÿè´£å¯¹è¯æµç¨‹ä¸­çš„å„ä¸ªå¤„ç†é˜¶æ®µï¼š
    - è¾“å…¥å¤„ç†å’ŒéªŒè¯
    - LLM è°ƒç”¨ï¼ˆåŒæ­¥/æµå¼ï¼‰
    - å·¥å…·è°ƒç”¨å¤„ç†
    - å“åº”æ ¼å¼åŒ–
    """
    
    def __init__(self, config: VoiceAgentConfig, trace=None):
        """åˆå§‹åŒ–èŠ‚ç‚¹é…ç½®
        
        Args:
            config: è¯­éŸ³åŠ©æ‰‹é…ç½®å¯¹è±¡
            trace: å¯é€‰çš„ TraceEmitter å®ä¾‹ï¼ˆç”¨äºå¯è§†åŒ–äº‹ä»¶ï¼‰
        """
        self.config = config
        self.logger = logger
        self._http_client: Optional[httpx.AsyncClient] = None
        self._client_lock = asyncio.Lock()
        
        # æ¥æ”¶ trace å®ä¾‹
        self.trace = trace

        # Initialize RAG service if enabled
        self._rag_service: Optional[RAGService] = None
        if RAGService and self.config.rag.enabled:
            try:
                self._rag_service = RAGService(config)
                self.logger.info("ğŸ“š RAG service initialized")
            except Exception as exc:  # pragma: no cover - safeguard
                self.logger.warning(f"RAG service initialization failed: {exc}")
                self._rag_service = None
    
    async def _ensure_http_client(self):
        """ç¡®ä¿ HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼ˆæ‡’åŠ è½½ï¼‰
        
        ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ç¡®ä¿çº¿ç¨‹å®‰å…¨çš„å•ä¾‹åˆå§‹åŒ–ã€‚
        """
        if self._http_client is None:
            async with self._client_lock:
                if self._http_client is None:  # åŒé‡æ£€æŸ¥
                    timeout = httpx.Timeout(self.config.llm.timeout, connect=10)
                    self._http_client = httpx.AsyncClient(
                        timeout=timeout,
                        headers={
                            "Authorization": f"Bearer {self.config.llm.api_key}",
                            "Content-Type": "application/json"
                        }
                    )
                    self.logger.debug("HTTP å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    def _build_llm_url(self, endpoint: str = "chat/completions") -> str:
        """æ„å»º LLM API å®Œæ•´ URL
        
        è‡ªåŠ¨å¤„ç† base_url ä¸­æ˜¯å¦åŒ…å« /v1 çš„æƒ…å†µã€‚
        
        Args:
            endpoint: API ç«¯ç‚¹è·¯å¾„ï¼Œé»˜è®¤ä¸º "chat/completions"
        
        Returns:
            å®Œæ•´çš„ API URL
        
        Examples:
            >>> # base_url = "https://api.openai-proxy.org/v1"
            >>> self._build_llm_url()
            "https://api.openai-proxy.org/v1/chat/completions"
            
            >>> # base_url = "https://api.openai-proxy.org"
            >>> self._build_llm_url()
            "https://api.openai-proxy.org/v1/chat/completions"
        """
        # ğŸ” è°ƒè¯•æ—¥å¿— - æ˜¾ç¤ºåŸå§‹é…ç½®
        self.logger.info(f"ğŸ”§ é…ç½®æ£€æŸ¥ - base_url: {self.config.llm.base_url}")
        self.logger.info(f"ğŸ”§ é…ç½®æ£€æŸ¥ - provider: {self.config.llm.provider}")
        self.logger.info(f"ğŸ”§ é…ç½®æ£€æŸ¥ - model: {self.config.llm.models.default}")
        
        base = self.config.llm.base_url.rstrip('/')
        
        # ä»…åœ¨ base_url ä¸åŒ…å« /v1 æ—¶æ·»åŠ 
        if not base.endswith('/v1'):
            base = base + '/v1'
        
        url = f"{base}/{endpoint}"
        self.logger.info(f"ğŸ”§ æœ€ç»ˆ URL: {url}")
        return url

    async def _retrieve_rag_snippets(self, state: AgentState):
        if not self._rag_service or not self._rag_service.enabled:
            state["rag_snippets"] = []
            return []

        query = state.get("user_input", "")
        if not query.strip():
            state["rag_snippets"] = []
            return []

        user_id = state.get("user_id")
        corpus_id = state.get("active_corpus_id")
        if corpus_id is None:
            corpus_id = self.config.rag.default_corpus_name
            state["active_corpus_id"] = corpus_id

        try:
            resolved_collection = self._rag_service.resolve_collection_name(
                user_id=user_id,
                corpus_id=corpus_id,
                collection_name=state.get("rag_collection"),
            )
            state["rag_collection"] = resolved_collection
        except ValueError as exc:
            self.logger.warning(f"RAG collection resolution failed: {exc}")
            state["rag_snippets"] = []
            return []

        try:
            results = await self._rag_service.retrieve(
                query,
                user_id=user_id,
                corpus_id=corpus_id,
                collection_name=resolved_collection,
            )
        except Exception as exc:  # pragma: no cover - tolerate runtime errors
            self.logger.warning(f"RAG æ£€ç´¢å¤±è´¥: {exc}")
            state["rag_snippets"] = []
            return []

        state["rag_snippets"] = [
            {
                "text": item.text,
                "score": round(item.score, 4),
                "source": item.source,
                "metadata": item.metadata,
            }
            for item in results
        ]
        return results
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº
        
        å…³é—­ HTTP å®¢æˆ·ç«¯è¿æ¥ï¼Œé‡Šæ”¾èµ„æºã€‚
        åº”åœ¨ç¨‹åºé€€å‡ºæˆ–æœåŠ¡åœæ­¢æ—¶è°ƒç”¨ã€‚
        """
        if self._http_client:
            try:
                await self._http_client.aclose()
                self.logger.debug("HTTP å®¢æˆ·ç«¯å·²å…³é—­")
            except Exception as e:
                self.logger.warning(f"å…³é—­ HTTP å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
            finally:
                self._http_client = None

        if self._rag_service:
            try:
                await self._rag_service.close()
            except Exception as e:  # pragma: no cover - cleanup safeguard
                self.logger.warning(f"å…³é—­ RAG æœåŠ¡æ—¶å‡ºé”™: {e}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ï¼Œè‡ªåŠ¨æ¸…ç†èµ„æº"""
        await self.cleanup()
    async def process_input(self, state: AgentState) -> AgentState:
        """å¤„ç†å’ŒéªŒè¯ç”¨æˆ·è¾“å…¥
        
        è¿™æ˜¯å¯¹è¯å¤„ç†æµç¨‹çš„å…¥å£èŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. éªŒè¯è¾“å…¥ä¸ä¸ºç©º
        2. åˆ›å»ºç”¨æˆ·æ¶ˆæ¯å¯¹è±¡
        3. åˆæ­¥è¯†åˆ«ç”¨æˆ·æ„å›¾
        4. æ›´æ–°çŠ¶æ€å‡†å¤‡è°ƒç”¨ LLM
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        """
        session_id = state.get('session_id', 'unknown')
        
        try:
            self.logger.debug(f"å¤„ç†ä¼šè¯ {session_id} çš„è¾“å…¥")
            
            # ğŸ†• æ€è€ƒé˜¶æ®µï¼šéªŒè¯è¾“å…¥
            if self.trace:
                # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ yieldï¼Œå› ä¸º process_input æ˜¯åŒæ­¥è¿”å› state çš„
                # æˆ‘ä»¬åªè®°å½•æ—¥å¿—ï¼ŒçœŸæ­£çš„äº‹ä»¶åœ¨ Graph å±‚å‘å°„
                self.logger.debug(f"[Trace] process_input: éªŒè¯ç”¨æˆ·è¾“å…¥")
            
            # æ›´æ–°æ—¶é—´æˆ³
            state["last_activity"] = datetime.now()
            
            # è§„èŒƒåŒ–è¾“å…¥ï¼Œç¡®ä¿ä¸ä¸ºç©º
            user_input = state["user_input"].strip()
            if not user_input:
                state["error_state"] = "empty_input"
                state["should_continue"] = False
                state["agent_response"] = "æˆ‘æ²¡æœ‰æ”¶åˆ°ä»»ä½•è¾“å…¥ï¼Œè¯·è¯´ç‚¹ä»€ä¹ˆå§ã€‚"
                return state
            
            # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²
            user_message = ConversationMessage(
                id=f"user_{len(state['messages']) + 1}_{int(datetime.now().timestamp())}",
                role=MessageRole.USER,
                content=user_input,
                metadata={"processed_at": datetime.now().isoformat()}
            )
            state["messages"].append(user_message)
            
            # é€šè¿‡å…³é”®è¯åˆæ­¥è¯†åˆ«ç”¨æˆ·æ„å›¾
            state["current_intent"] = self._analyze_intent(user_input)
            
            # è®¾ç½®ä¸‹ä¸€æ­¥åŠ¨ä½œï¼šè°ƒç”¨ LLM
            state["next_action"] = "call_llm"
            
            self.logger.debug(f"è¾“å…¥å¤„ç†å®Œæˆï¼Œæ„å›¾: {state['current_intent']}")
            return state
            
        except Exception as e:
            self.logger.error(f"è¾“å…¥å¤„ç†é”™è¯¯: {e}")
            state["error_state"] = f"input_processing_error: {str(e)}"
            state["should_continue"] = False
            return state
    
    async def call_llm(self, state: AgentState) -> AgentState:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå“åº”
        
        è¿™æ˜¯æ ¸å¿ƒå¤„ç†èŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. å‡†å¤‡å¯¹è¯å†å²æ¶ˆæ¯
        2. é…ç½®æ¨¡å‹å‚æ•°
        3. è°ƒç”¨ LLM API
        4. å¤„ç†å“åº”ï¼ˆæ–‡æœ¬æˆ–å·¥å…·è°ƒç”¨ï¼‰
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        """
        try:
            self.logger.debug(f"ä¸ºä¼šè¯ {state['session_id']} è°ƒç”¨ LLM")
            
            # å‡†å¤‡ LLM æ¶ˆæ¯ï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰
            # å¦‚æœ state ä¸­æœ‰ external_historyï¼Œä¼ é€’ç»™ _prepare_llm_messages
            external_history = state.get("external_history")
            if external_history is not None:
                self.logger.info(f"ğŸ” Found external_history in state: {len(external_history)} messages")
            else:
                self.logger.warning(f"âš ï¸ No external_history found in state for session {state['session_id']}")
            rag_results = await self._retrieve_rag_snippets(state)

            messages = self._prepare_llm_messages(state, external_history=external_history)

            if rag_results and self._rag_service:
                rag_prompt = self._rag_service.build_prompt(rag_results)
                if rag_prompt:
                    system_message = {"role": "system", "content": rag_prompt}
                    if messages and messages[-1].get("role") == "user":
                        messages.insert(len(messages) - 1, system_message)
                    else:
                        messages.append(system_message)
            
            # é…ç½®æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨å…¼å®¹å±‚å¤„ç†ä¸åŒæ¨¡å‹çš„å‚æ•°å·®å¼‚ï¼‰
            model = state["model_config"].get("model", self.config.llm.models.default)
            max_tokens = state.get("max_tokens", self.config.llm.max_tokens)
            temperature = state.get("temperature", self.config.llm.temperature)
            
            llm_config = prepare_llm_params(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # è°ƒç”¨ LLM API è·å–å“åº”
            response = await self._make_llm_call(messages, llm_config)
            
            # åˆ¤æ–­å“åº”ç±»å‹ï¼šå·¥å…·è°ƒç”¨ or ç›´æ¥å›å¤
            if self._has_tool_calls(response):
                state["next_action"] = "handle_tools"
                # æå–å·¥å…·è°ƒç”¨è¯·æ±‚å¹¶åŠ å…¥å¾…å¤„ç†é˜Ÿåˆ—
                tool_calls = self._extract_tool_calls(response)
                state["pending_tool_calls"].extend(tool_calls)
            else:
                state["next_action"] = "format_response"
                state["agent_response"] = response.get("content", "")
            
            self.logger.debug("LLM è°ƒç”¨å®Œæˆ")
            return state
            
        except Exception as e:
            self.logger.error(f"LLM è°ƒç”¨é”™è¯¯: {e}")
            state["error_state"] = f"llm_call_error: {str(e)}"
            state["agent_response"] = "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
            state["next_action"] = "format_response"
            return state
    
    async def handle_tools(self, state: AgentState) -> AgentState:
        """å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚
        
        ğŸ†• ä¼˜åŒ–ç‰ˆ: æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
        
        å½“ LLM éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œæ­¤èŠ‚ç‚¹è´Ÿè´£ï¼š
        1. æ‰§è¡Œæ‰€æœ‰å¾…å¤„ç†çš„å·¥å…·è°ƒç”¨
        2. æ”¶é›†å·¥å…·æ‰§è¡Œç»“æœ
        3. å°†ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
        4. ğŸ†• å¢åŠ å·¥å…·è°ƒç”¨è®¡æ•°å™¨
        5. å‡†å¤‡å†æ¬¡è°ƒç”¨ LLMï¼ˆè®©å®ƒåŸºäºå·¥å…·ç»“æœé‡æ–°æ€è€ƒï¼‰
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æ›´æ–°åçš„å¯¹è¯çŠ¶æ€
        """
        try:
            self.logger.debug(f"å¤„ç†ä¼šè¯ {state['session_id']} çš„å·¥å…·è°ƒç”¨")
            
            if not state["pending_tool_calls"]:
                self.logger.warning("æ²¡æœ‰å¾…å¤„ç†çš„å·¥å…·è°ƒç”¨")
                state["next_action"] = "call_llm"
                return state
            
            # ğŸ†• å¢åŠ å·¥å…·è°ƒç”¨è®¡æ•°
            state["tool_call_count"] = state.get("tool_call_count", 0) + 1
            current_iteration = state["tool_call_count"]
            
            self.logger.info(f"ğŸ”§ ç¬¬ {current_iteration} è½®å·¥å…·è°ƒç”¨ï¼Œå¾…æ‰§è¡Œå·¥å…·æ•°: {len(state['pending_tool_calls'])}")
            
            # é€ä¸ªæ‰§è¡Œå·¥å…·è°ƒç”¨
            for tool_call in state["pending_tool_calls"]:
                result = await self._execute_tool_call(tool_call)
                state["tool_results"].append(result)
                state["tool_calls"].append(tool_call)
                
                self.logger.info(f"  âœ… å·¥å…· '{tool_call.name}' æ‰§è¡Œå®Œæˆ: {result.success}")
                
                # âœ… ä¿å­˜ tool_call åˆ°æ•°æ®åº“
                await self._save_tool_call_to_database(
                    session_id=state["session_id"],
                    tool_call=tool_call,
                    result=result
                )
            
            # æ¸…ç©ºå¾…å¤„ç†é˜Ÿåˆ—
            state["pending_tool_calls"] = []
            
            # å°†å·¥å…·æ‰§è¡Œç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
            for result in state["tool_results"][-len(state["tool_calls"]):]:
                tool_message = ConversationMessage(
                    id=f"tool_{result.call_id}_{int(datetime.now().timestamp())}",
                    role=MessageRole.TOOL,
                    content=json.dumps(result.dict(), cls=DateTimeJSONEncoder),
                    metadata={"tool_call_id": result.call_id, "success": result.success}
                )
                state["messages"].append(tool_message)
            
            # ğŸ†• æ ¸å¿ƒæ”¹åŠ¨: å·¥å…·è°ƒç”¨åè¿”å› LLM è¿›è¡Œé‡æ–°æ€è€ƒ
            # LLM ä¼šåŸºäºå·¥å…·ç»“æœåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´å¤šå·¥å…·æˆ–ç›´æ¥ç”Ÿæˆå›å¤
            state["next_action"] = "call_llm"
            
            self.logger.info(f"ğŸ”„ ç¬¬ {current_iteration} è½®å·¥å…·è°ƒç”¨å®Œæˆï¼Œè¿”å› LLM é‡æ–°è¯„ä¼°")
            return state
            
        except Exception as e:
            self.logger.error(f"å·¥å…·å¤„ç†é”™è¯¯: {e}")
            state["error_state"] = f"tool_handling_error: {str(e)}"
            state["agent_response"] = "æŠ±æ­‰ï¼Œåœ¨ä½¿ç”¨å·¥å…·æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œè®©æˆ‘æ¢ä¸ªæ–¹å¼å¸®æ‚¨ã€‚"
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¿”å› LLM è®©å®ƒç”Ÿæˆ fallback å›å¤
            state["next_action"] = "call_llm"
            return state
    
    async def format_response(self, state: AgentState) -> AgentState:
        """æ ¼å¼åŒ–æœ€ç»ˆå“åº”
        
        è¿™æ˜¯æµç¨‹çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. ç¡®ä¿æœ‰å“åº”å†…å®¹
        2. åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å¯¹è±¡
        3. æ·»åŠ å…ƒæ•°æ®
        4. æ ‡è®°å¯¹è¯å›åˆç»“æŸ
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            æœ€ç»ˆçš„å¯¹è¯çŠ¶æ€
        """
        try:
            self.logger.debug(f"æ ¼å¼åŒ–ä¼šè¯ {state['session_id']} çš„å“åº”")
            
            # ç¡®ä¿æœ‰å“åº”å†…å®¹
            if not state["agent_response"]:
                if state["error_state"]:
                    state["agent_response"] = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ã€‚"
                else:
                    state["agent_response"] = "æˆ‘ä¸å¤ªç¡®å®šå¦‚ä½•å›ç­”ï¼Œè¯·æ¢ä¸ªæ–¹å¼é—®æˆ‘å§ã€‚"
            
            # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å¹¶æ·»åŠ åˆ°å†å²
            assistant_message = ConversationMessage(
                id=f"assistant_{len(state['messages']) + 1}_{int(datetime.now().timestamp())}",
                role=MessageRole.ASSISTANT,
                content=state["agent_response"],
                metadata={
                    "generated_at": datetime.now().isoformat(),
                    "model": state["model_config"].get("model", "unknown"),
                    "intent": state.get("current_intent"),
                    "tool_calls_count": len(state["tool_calls"]),
                    "rag_snippets": state.get("rag_snippets", []),
                }
            )
            state["messages"].append(assistant_message)
            
            # æ›´æ–°æ´»åŠ¨æ—¶é—´æˆ³
            state["last_activity"] = datetime.now()
            
            # æ ‡è®°å¯¹è¯å›åˆå®Œæˆ
            state["should_continue"] = False
            state["next_action"] = None
            
            self.logger.debug("å“åº”æ ¼å¼åŒ–å®Œæˆ")
            return state
            
        except Exception as e:
            self.logger.error(f"å“åº”æ ¼å¼åŒ–é”™è¯¯: {e}")
            state["error_state"] = f"response_formatting_error: {str(e)}"
            state["agent_response"] = "æŠ±æ­‰ï¼Œå“åº”æ ¼å¼åŒ–æ—¶å‡ºç°äº†é—®é¢˜ã€‚"
            state["should_continue"] = False
            return state
    
    def _analyze_intent(self, user_input: str) -> Optional[str]:
        """åˆ†æç”¨æˆ·æ„å›¾ï¼ˆåŸºäºå…³é”®è¯çš„ç®€å•å®ç°ï¼‰
        
        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„æ„å›¾è¯†åˆ«ï¼Œä»…ç”¨äºåŸºç¡€åˆ†ç±»ã€‚
        ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ NLU æ¨¡å‹æˆ– LLM è¿›è¡Œæ„å›¾è¯†åˆ«ã€‚
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        
        Returns:
            è¯†åˆ«çš„æ„å›¾æ ‡ç­¾ï¼Œå¦‚ "search", "calculation" ç­‰
        """
        input_lower = user_input.lower()
        
        # åŸºäºå…³é”®è¯çš„ç®€å•æ„å›¾æ£€æµ‹
        if any(word in input_lower for word in ["search", "find", "look", "æœç´¢", "æŸ¥æ‰¾"]):
            return "search"
        elif any(word in input_lower for word in ["calculate", "math", "compute", "è®¡ç®—"]):
            return "calculation"
        elif any(word in input_lower for word in ["time", "date", "when", "æ—¶é—´", "æ—¥æœŸ"]):
            return "time_query"
        elif any(word in input_lower for word in ["image", "picture", "generate", "create", "å›¾ç‰‡", "ç”Ÿæˆ"]):
            return "image_generation"
        elif any(word in input_lower for word in ["help", "what", "how", "å¸®åŠ©", "æ€ä¹ˆ"]):
            return "help_request"
        else:
            return "general_conversation"
    
    def _prepare_llm_messages(self, state: AgentState, external_history: Optional[List[Dict]] = None) -> List[Dict[str, str]]:
        """å‡†å¤‡ LLM API è°ƒç”¨çš„æ¶ˆæ¯åˆ—è¡¨
        
        åŒ…å«ä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯å’Œå†å²å¯¹è¯ã€‚ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„å†å²è®°å½•ã€‚
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
            external_history: å¤–éƒ¨ä¼ å…¥çš„å†å²æ¶ˆæ¯åˆ—è¡¨ (å¯é€‰)
        
        Returns:
            æ ¼å¼åŒ–çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œç¬¦åˆ OpenAI API æ ¼å¼
        """
        messages = []
        
        # æ„å»ºä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯
        system_prompt = self._build_optimized_system_prompt(state)
        system_message = {
            "role": "system",
            "content": system_prompt
        }
        messages.append(system_message)
        
        # ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨å†å²ï¼ˆä» SessionHistoryManagerï¼‰
        if external_history is not None:
            # é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡ï¼ˆæœ€è¿‘ 10 æ¡ï¼‰
            MAX_HISTORY_MESSAGES = 10
            recent_history = external_history[-MAX_HISTORY_MESSAGES:] if external_history else []
            for msg in recent_history:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            self.logger.info(f"âœ… Loaded {len(recent_history)} messages from external history for LLM")
            
            # é‡è¦ï¼šæ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ¥è‡ª state["messages"] çš„æœ€åä¸€æ¡ï¼‰
            if state["messages"] and state["messages"][-1].role == MessageRole.USER:
                current_user_msg = state["messages"][-1]
                messages.append({
                    "role": "user",
                    "content": current_user_msg.content
                })
                self.logger.info(f"âœ… Added current user message to LLM input")
        else:
            # å›é€€åˆ° state ä¸­çš„æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            self.logger.info("âš ï¸ No external history provided, using state messages")
            MAX_HISTORY_MESSAGES = 10
            recent_messages = state["messages"][-MAX_HISTORY_MESSAGES:]
            for msg in recent_messages:
                if msg.role in [MessageRole.USER, MessageRole.ASSISTANT]:
                    messages.append({
                        "role": msg.role.value,
                        "content": msg.content
                    })
            self.logger.info(f"ğŸ“ Using {len(recent_messages)} messages from state")
        
        return messages
    
    def _build_optimized_system_prompt(self, state: AgentState) -> str:
        """æ„å»ºä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯ï¼Œæå‡æ™ºèƒ½æ€§å’Œæ•ˆç‡
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æ˜ç¡®è§’è‰²å®šä½å’Œèƒ½åŠ›è¾¹ç•Œ
        2. æä¾›æ¸…æ™°çš„å·¥å…·ä½¿ç”¨æŒ‡å—
        3. å¼ºè°ƒæ•ˆç‡å’Œå‡†ç¡®æ€§
        4. åŒ…å«ä»»åŠ¡åˆ†è§£å’Œæ¨ç†æ¡†æ¶
        5. æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æç¤ºè¯
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            ä¼˜åŒ–åçš„ç³»ç»Ÿæç¤ºè¯å­—ç¬¦ä¸²
        """
        # åŸºç¡€èº«ä»½å®šä¹‰
        base_identity = """# Role Definition
You are an efficient, intelligent multi-functional AI assistant with the following core capabilities:
- Natural and fluent conversation in both Chinese and English (respond in user's language)
- Intelligent tool invocation and task orchestration
- Structured problem analysis and solving
- Context understanding and memory retention

# Core Principles
1. **Efficiency First**: Achieve goals with minimal steps, avoid redundant operations
2. **Accuracy Above All**: Prioritize information accuracy; clearly inform users when uncertain
3. **Proactive Thinking**: Understand user intent; proactively clarify requirements when needed
4. **Smart Tool Usage**: Judiciously determine when tools are needed; avoid unnecessary calls

# ğŸ“ Response Format Standards (CRITICAL - Frontend Rendering Rules)
**You MUST organize all responses using Markdown format following these exact rules:**

## Basic Markdown Syntax (Frontend-Compatible)

### Headers
- Use `##` for main sections, `###` for subsections
- **MUST have space after #**: `## Title` (NOT `##Title`)
- **MUST have blank line after header**

Example:
```
## Main Section

Content starts here...

### Subsection

More content...
```

### Paragraphs
- Separate paragraphs with **ONE blank line**
- Single newlines within a paragraph will NOT create line breaks
- For explicit line breaks: use `  \n` (two spaces + newline)

### Lists (MOST IMPORTANT)
**Unordered Lists** (Use `-` for consistency):
```
- First item;
- Second item;
- Third item.
```

**Ordered Lists**:
```
1. First step;
2. Second step;
3. Third step.
```

**Critical List Rules**:
1. âœ… **MUST have space after `-` or number**: `- Item` (NOT `-Item`)
2. âœ… **End items with semicolon `;`** (except last item can use period `.`)
3. âœ… **Blank line before list**
4. âœ… **Blank line after list**
5. âœ… **Each item on separate line**
6. âŒ **NO nested lists** (keep flat for clarity)

Example:
```
å¦‚éœ€æˆ‘:

- ç»§ç»­è¿½è¸ªå¹¶æ¯å°æ—¶æ›´æ–°æœ€æ–°æŠ¥é“;
- æ±‡æ€»ä¸åŒæ¶ˆæ¯æ¥æºçš„ä¿¡æ¯;
- å°†ä¿¡æ¯ç¿»è¯‘æˆè‹±æ–‡ã€‚

å‘Šè¯‰æˆ‘ä½ æƒ³è¦å“ªä¸€ç§ã€‚
```

### Code
**Inline code**: Wrap with single backticks: `` `code` ``

**Code blocks**: Must specify language for syntax highlighting
````
```python
def example():
    return "Hello"
```
````

**Supported languages**: `python`, `javascript`, `typescript`, `bash`, `json`, `yaml`, `html`, `css`, `sql`

**Critical Code Block Rules**:
- âœ… Blank line before code block
- âœ… Blank line after code block
- âœ… Always specify language (e.g., ` ```python `)
- âŒ Never nest Markdown inside code blocks

### Links
- Format: `[Link Text](URL)`
- Frontend will auto-open in new tab
- Example: `[Read more](https://example.com)`

### Emphasis
- **Bold**: `**important text**` for key information
- *Italic*: `*secondary text*` for emphasis
- ***Bold + Italic***: `***critical text***` sparingly

### Tables (Use for structured data)
```
| Column 1 | Column 2 | Column 3 |
|----------|----------|----------|
| Data 1   | Data 2   | Data 3   |
| Data 4   | Data 5   | Data 6   |
```
- Blank line before table
- Blank line after table

### Horizontal Rule
Use `---` on its own line with blank lines before/after:
```
Content above

---

Content below
```

### Quotes
```
> This is a quoted text.
> Can span multiple lines.
```

### Emojis
Use sparingly for visual guidance:
- ğŸ“Š Data/statistics
- ğŸ” Search/investigation
- ğŸ’¡ Insight/tip
- âš ï¸ Warning/caution
- âœ… Success/correct
- âŒ Error/incorrect
- ğŸ”— Link/reference

## âŒ UNSUPPORTED Syntax (DO NOT USE)
1. âŒ HTML tags: `<div>`, `<span>` (ignored by frontend)
2. âŒ LaTeX math: `$E=mc^2$` (not rendered)
3. âŒ Footnotes: `[^1]` (not supported)
4. âŒ Definition lists (not supported)
5. âŒ Emoji shortcodes: `:smile:` (use actual emoji: ğŸ˜Š)
6. âŒ Images: `![alt](url)` (may not display correctly)

## ğŸ” SEARCH RESULTS HANDLING (MANDATORY PROTOCOL)
When you call the `web_search` tool, you **MUST** follow this strict protocol:

### Step 1: Parse Tool Response Structure
The tool returns JSON with this structure:
```json
{
  "ai_answer": "AI-generated summary (USE THIS FIRST if present!)",
  "results": [
    {
      "title": "Article/page title",
      "snippet": "Brief content excerpt (50-150 words)",
      "url": "Source URL",
      "score": 0.95,  // Relevance score (0.0-1.0)
      "published_date": "2025-01-15"  // Optional
    }
  ],
  "total_results": 8
}
```

### Step 2: Structure Your Response (REQUIRED FORMAT)
```markdown
## ï¿½ Search Results: [Topic]

### ï¿½ğŸ“Š Executive Summary
[If ai_answer exists and is valuable, present it here]
[If no ai_answer, synthesize key findings from top 3 results in 2-3 sentences]

### ğŸ“° Detailed Findings

#### 1. **[Title from result[0]]**
- ğŸ“… **Published**: [published_date or "Recent"]
- ğŸ“ **Key Points**: [Extract core information from snippet, 50-100 words]
- ğŸ”— **Source**: [Title](URL) â† Must be clickable!

#### 2. **[Title from result[1]]**
- ğŸ“… **Published**: [published_date or "Recent"]
- ğŸ“ **Key Points**: [Extract core information from snippet]
- ğŸ”— **Source**: [Title](URL)

[Continue for top 3-5 results based on score]

---

ğŸ’¡ **Key Insight**: [One-sentence conclusion, trend observation, or actionable recommendation]
```

### Step 3: What You MUST DO âœ…
- âœ… **Extract ai_answer**: If present, use it as the executive summary
- âœ… **Parse all results**: Don't just say "Found X results"
- âœ… **Show actual content**: Display title + snippet + url for each result
- âœ… **Clickable links**: Format as `[Title](URL)` so users can click
- âœ… **Sort by relevance**: Prioritize high-score results (typically 0.8+)
- âœ… **Include dates**: Show published_date when available for news/time-sensitive content
- âœ… **Synthesize**: Add value by summarizing patterns or key insights
- âœ… **Structured format**: Use headers, lists, and separators for visual clarity

### Step 4: What You MUST NOT DO âŒ
- âŒ **Never** just return "Found 8 results about..." without showing content
- âŒ **Never** output raw JSON or tool parameters like `{"query": "...", "num_results": 8}`
- âŒ **Never** omit the snippet content (the actual information)
- âŒ **Never** ignore the ai_answer field when it's present
- âŒ **Never** provide URLs without making them clickable
- âŒ **Never** use plain paragraphs for search results (always use structured format)

### Example: GOOD vs BAD Response

**âŒ BAD (What NOT to do):**
```
I found 8 results about Trump visiting Japan.
```

**âœ… GOOD (What to do):**
```
## ğŸ” Search Results: Trump's Japan Visit 2025

### ğŸ“Š Executive Summary
Former President Trump confirmed plans to visit Japan in spring 2025, focusing on trade and security cooperation discussions with Japanese officials.

### ğŸ“° Detailed Findings

#### 1. **Trump Confirms 2025 Japan Visit**
- ğŸ“… **Published**: 2025-01-15
- ğŸ“ **Key Points**: Trump announced via social media that he will visit Japan in April 2025 to discuss bilateral trade agreements and regional security concerns.
- ğŸ”— **Source**: [The Japan Times](https://example.com/article1)

#### 2. **US-Japan Trade Talks Accelerate**
- ğŸ“… **Published**: 2025-01-10
- ğŸ“ **Key Points**: Japanese officials preparing for high-level negotiations during Trump's visit, with focus on automotive and agricultural sectors.
- ğŸ”— **Source**: [Reuters](https://example.com/article2)

---

ğŸ’¡ **Key Insight**: This will be Trump's first visit to Japan since leaving office, signaling renewed focus on US-Japan alliance.
```

# ğŸ¯ Response Quality Standards for Other Scenarios

## For Code-Related Queries
- Always specify language in code blocks: ` ```python `, ` ```javascript `, etc.
- Add comments to explain complex logic
- Provide context before and after code snippets

## For Data/Numbers
- Use tables when comparing multiple items:
  ```
  | Item | Value | Change |
  |------|-------|--------|
  | A    | 100   | +5%    |
  ```
- Use charts/graphs descriptions for trends
- Highlight key numbers with **bold**

## For Step-by-Step Instructions
1. **Number each step** for clarity
2. **Bold the action** in each step
3. **Provide expected outcomes** after key steps
4. **Include troubleshooting** for common issues

## Language Adaptation
- **Respond in the user's language**: Chinese query â†’ Chinese response, English query â†’ English response
- **Keep technical terms**: Use original English terms in Chinese responses when appropriate (e.g., "API", "JSON")
- **Maintain Markdown**: Use Markdown structure regardless of language"""

        # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
        available_tools = self._format_available_tools()
        
        tools_guide = f"""

# ğŸ› ï¸ Available Tools
{available_tools}

# Tool Usage Strategy

## When to Use Tools âœ…
- **Real-time information needed** (weather, time, search) â†’ MUST use tool
- **Complex calculations or data processing** â†’ Use calculator tool
- **User explicitly requests specific action** â†’ Use corresponding tool
- **Information may have changed recently** â†’ Use search tool
- **Verification of facts/statistics needed** â†’ Use search tool

## When NOT to Use Tools âŒ
- **General knowledge or common sense questions** â†’ Answer directly
- **Simple mental math or logical reasoning** â†’ Answer directly
- **Creative or opinion-based requests** â†’ Answer directly
- **Conversational chitchat** â†’ Answer directly

## Tool Invocation Principles
1. **One tool at a time**: Only call tools that are genuinely needed for the current query
2. **Prefer single tool**: Use the most appropriate single tool rather than multiple tools
3. **Quality over quantity**: Better to make one precise tool call than multiple vague ones
4. **Always process results**: After tool execution, ALWAYS synthesize and present results properly
   - For search: Follow the mandatory search results protocol above
   - For calculator: Show both the expression and result
   - For time: Present in user-friendly format with timezone context
   - For weather: Provide actionable insights (e.g., "Bring an umbrella")

## Tool Result Processing (CRITICAL)
**After any tool call, you MUST:**
1. âœ… **Parse the tool response**: Extract data, ai_answer, or error messages
2. âœ… **Format appropriately**: Use Markdown structure (headers, lists, links)
3. âœ… **Add context**: Explain what the results mean, not just what they are
4. âœ… **Cite sources**: For search results, always provide clickable URLs
5. âœ… **Synthesize insight**: Don't just relay data; add interpretation or recommendations

**Common mistake to avoid:**
âŒ Returning tool parameters instead of tool results
âŒ Example: Saying `{{"query": "Trump Japan", "num_results": 8}}` instead of actual search findings"""

        # ä»»åŠ¡å¤„ç†æ¡†æ¶
        task_framework = """

# ğŸ¯ Task Processing Framework
For complex requests, follow this cognitive workflow:

1. **Understand** ğŸ§ 
   - Accurately identify user's true needs and intent
   - Recognize implicit requirements (e.g., "latest news" implies web_search)
   - Determine response language based on user's query language

2. **Plan** ğŸ“‹
   - Determine if tools are needed
   - Select the most appropriate tool(s)
   - For search queries: Formulate precise search terms

3. **Execute** âš¡
   - Efficiently call necessary tools to gather information
   - Wait for complete tool results before proceeding

4. **Synthesize** ğŸ”„
   - Integrate tool results with your knowledge
   - Structure information using proper Markdown format
   - Add analysis, context, or recommendations beyond raw data

5. **Validate** âœ…
   - Ensure response fully addresses user's question
   - Check that all sources are properly cited
   - Verify response follows Markdown formatting standards

# Response Quality Standards

## âœ… Excellent Response Should:
- **Directly address** the user's question without meandering
- **Well-structured** with clear hierarchy (headers, lists, sections)
- **Information-accurate** with reliable sources cited
- **Tone-appropriate**: Friendly yet professional
- **Actionable**: Provide insights, not just data
- **Visually clear**: Proper use of Markdown formatting

## âŒ Avoid:
- **Excessive verbosity** or repetitive explanations
- **Unnecessary apologies** or overly humble expressions (e.g., "I apologize but..." when not needed)
- **Vague responses** without concrete information
- **Tool misuse**: Calling irrelevant tools or not processing tool results
- **Format violations**: Plain text walls instead of structured Markdown
- **Incomplete information**: Stopping at "Found X results" without showing them

# Special Handling for Common Query Types

## News/Current Events Queries
- **Always use** web_search tool
- **Prioritize** recent results (check published_date)
- **Include** multiple perspectives if available
- **Format**: Use the mandatory search results protocol

## "How to" / Tutorial Queries
- **Structure**: Clear numbered steps
- **Include**: Expected outcomes for each step
- **Add**: Troubleshooting tips for common issues
- **Format**: Combine headers, ordered lists, and code blocks

## Technical/Code Queries
- **Use**: Proper syntax highlighting in code blocks
- **Provide**: Explanation before/after code
- **Include**: Comments within code for complex logic
- **Format**: ` ```language ` with appropriate language tag

## Data/Statistics Queries
- **Present**: Tables for comparisons
- **Highlight**: Key numbers with **bold**
- **Visualize**: Describe trends or patterns
- **Cite**: Always mention data sources with links"""

        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¼˜åŒ–
        context_optimization = self._build_context_aware_addition(state)
        
        # ç»„åˆå®Œæ•´æç¤ºè¯
        full_prompt = base_identity + tools_guide + task_framework
        
        if context_optimization:
            full_prompt += "\n\n" + context_optimization
        
        return full_prompt
    
    def _get_tools_schema(self) -> List[Dict]:
        """è·å–å·¥å…·çš„ OpenAI Function Calling æ ¼å¼å®šä¹‰
        
        Returns:
            å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ŒOpenAI tools æ ¼å¼
        """
        try:
            from mcp import get_tool_registry
            registry = get_tool_registry()
            tools = registry.list_tools()
            
            if not tools:
                return []
            
            # è½¬æ¢ä¸º OpenAI Function Calling æ ¼å¼
            tools_schema = []
            for tool in tools:
                schema = tool.to_openai_schema()
                tools_schema.append(schema)
            
            self.logger.info(f"âœ… Loaded {len(tools_schema)} tools for LLM")
            return tools_schema
        except Exception as e:
            self.logger.error(f"âŒ Failed to load tools schema: {e}", exc_info=True)
            return []
    
    def _format_available_tools(self) -> str:
        """æ ¼å¼åŒ–å¯ç”¨å·¥å…·åˆ—è¡¨ä¸ºæ˜“è¯»çš„æ–‡æœ¬
        
        Returns:
            æ ¼å¼åŒ–çš„å·¥å…·åˆ—è¡¨å­—ç¬¦ä¸²
        """
        try:
            from mcp import get_tool_registry
            registry = get_tool_registry()
            tools = registry.list_tools()
            
            if not tools:
                return "å½“å‰æš‚æ— å¯ç”¨å·¥å…·ã€‚"
            
            tool_descriptions = []
            for tool in tools:
                name = tool.name
                desc = tool.description
                # ç®€åŒ–æè¿°ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯
                short_desc = desc.split('.')[0] if desc else "æ— æè¿°"
                tool_descriptions.append(f"- **{name}**: {short_desc}")
            
            return "\n".join(tool_descriptions)
        
        except Exception as e:
            self.logger.warning(f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
            return "- **calculator**: æ‰§è¡Œæ•°å­¦è®¡ç®—\n- **get_time**: è·å–å½“å‰æ—¶é—´\n- **get_weather**: æŸ¥è¯¢å¤©æ°”ä¿¡æ¯\n- **web_search**: æœç´¢ç½‘ç»œä¿¡æ¯"
    
    def _build_context_aware_addition(self, state: AgentState) -> str:
        """æ ¹æ®å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡æ„å»ºé¢å¤–çš„æç¤ºè¯å¢å¼º
        
        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€
        
        Returns:
            ä¸Šä¸‹æ–‡ç›¸å…³çš„é¢å¤–æç¤ºè¯ï¼Œå¦‚æœä¸éœ€è¦åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        additions = []
        
        # 1. å¦‚æœæœ‰å·¥å…·è°ƒç”¨å†å²ï¼Œæé†’åŸºäºç»“æœå›ç­”
        if state.get("tool_calls") and len(state["tool_calls"]) > 0:
            additions.append(
                """# âš ï¸ Current Context: Tool Results Available

You have just executed tool(s) and received results. **CRITICAL REMINDER**:

âœ… **You MUST**:
- Base your response ENTIRELY on the actual tool results data
- Parse and present the tool response properly (especially for web_search)
- Follow the mandatory search results protocol if it was a web_search call
- Extract and display: ai_answer, titles, snippets, urls from the results
- Format everything in proper Markdown structure

âŒ **You MUST NOT**:
- Fabricate or guess information not in the tool results
- Return tool parameters (e.g., `{"query": "...", "num_results": 8}`) as if they were results
- Say "Found X results" without showing the actual content
- Ignore the structured data in the tool response

**If tool results are incomplete or unclear**: Explicitly inform the user about limitations."""
            )
        
        # 2. å¦‚æœå¯¹è¯è½®æ¬¡è¾ƒå¤šï¼Œæé†’ä¿æŒè¿è´¯æ€§
        message_count = len(state.get("messages", []))
        if message_count > 6:
            additions.append(
                """# ğŸ’¬ Conversation Continuity

This is a multi-turn conversation (6+ messages). Please:
- Maintain context consistency across turns
- Recognize pronouns like "it", "this", "that" referring to previous topics
- Reference earlier discussion points when relevant
- Don't repeat information already established in the conversation"""
            )
        
        # 3. å¦‚æœæ£€æµ‹åˆ°ç‰¹å®šæ„å›¾ï¼Œç»™å‡ºé’ˆå¯¹æ€§æŒ‡å¯¼
        intent = state.get("current_intent")
        user_input = state.get("user_input", "").lower()
        
        # æ£€æµ‹æœç´¢æ„å›¾
        search_keywords = ["search", "find", "latest", "news", "æœç´¢", "æŸ¥æ‰¾", "æœ€æ–°", "æ–°é—»", "æŸ¥è¯¢"]
        if intent == "search" or any(keyword in user_input for keyword in search_keywords):
            additions.append(
                """# ğŸ” Search Task Optimization

User is requesting information search. **Enhanced Protocol**:

**Step 1: Tool Execution**
- Use `web_search` with precise query (English for international topics, Chinese for local topics)
- Set `num_results` to 5-8 for optimal balance

**Step 2: Result Processing (MANDATORY)**
Parse the tool response JSON structure:
```json
{
  "ai_answer": "Use this as executive summary if valuable",
  "results": [
    {"title": "...", "snippet": "...", "url": "...", "score": 0.95}
  ]
}
```

**Step 3: Response Formatting (STRICT)**
```markdown
## ğŸ” Search Results: [Topic]

### ğŸ“Š Executive Summary
[Present ai_answer here, or synthesize from top results]

### ğŸ“° Detailed Findings
1. **[Title 1]**
   - ğŸ“ [Key points from snippet]
   - ğŸ”— [Title](URL)

2. **[Title 2]** ...

---
ğŸ’¡ **Key Insight**: [Your analysis]
```

**Quality Checklist**:
- [ ] ai_answer used as summary (if present)
- [ ] 3-5 results shown with title + snippet + clickable URL
- [ ] Markdown structure with headers and lists
- [ ] Time-sensitive info includes dates
- [ ] Added synthesis or insight beyond raw data

**Common Error to Avoid**:
âŒ Do NOT just output: "Found 8 search results about Trump's Japan visit"
âœ… DO output: Structured results with actual titles, snippets, and links"""
            )
        
        # æ£€æµ‹è®¡ç®—æ„å›¾
        elif intent == "calculation" or any(op in user_input for op in ["+", "-", "*", "/", "calculate", "è®¡ç®—"]):
            additions.append(
                """# ğŸ§® Calculation Task

User needs mathematical computation:
- Use `calculator` tool for complex expressions or to ensure precision
- Show both the expression and result clearly
- Format: "Calculating `expression` = **result**"
- For very simple math (e.g., 2+2), you can answer directly
- For decimals, powers, trigonometry, always use the tool for accuracy"""
            )
        
        # æ£€æµ‹æ—¶é—´æŸ¥è¯¢
        elif "time" in user_input or "date" in user_input or "æ—¶é—´" in user_input or "æ—¥æœŸ" in user_input or "å‡ ç‚¹" in user_input:
            additions.append(
                """# ğŸ• Time/Date Query

User is asking about current time or date:
- Use `get_time` tool with appropriate format parameter
- Present time in user-friendly format with timezone context
- For "what time is it": use format="full"
- For "what date": use format="date"
- For "timestamp": use format="timestamp"
- Always clarify the timezone in your response"""
            )
        
        return "\n\n".join(additions) if additions else ""

    
    async def _make_llm_call(self, messages: List[Dict[str, str]], config: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨ LLM APIï¼ˆOpenAI å…¼å®¹ï¼‰
        
        å¦‚æœçœŸå® HTTP è°ƒç”¨å¤±è´¥ï¼Œä¼šä½¿ç”¨åŸºäºå…³é”®è¯çš„å¯å‘å¼ fallbackã€‚
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            config: LLM é…ç½®å‚æ•°
        
        Returns:
            åŒ…å« content (str) å’Œ tool_calls (list) çš„å­—å…¸
        """
        user_message = messages[-1]["content"] if messages else ""

        # ==== ä¸»è¦è·¯å¾„ï¼šçœŸå® HTTP è°ƒç”¨ ====
        try:
            # ç¡®ä¿ HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            await self._ensure_http_client()

            # è·å–å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰
            self.logger.info(f"ğŸ” Attempting to load tools schema...")
            tools_schema = self._get_tools_schema()
            self.logger.info(f"ğŸ” Tools schema loaded: {len(tools_schema) if tools_schema else 0} tools")
            
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                tools=tools_schema if tools_schema else None  # ä¼ é€’å·¥å…·å®šä¹‰
            )
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - LLM è¯·æ±‚å‚æ•°
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¤ LLM API è¯·æ±‚å‚æ•°:")
            self.logger.info(f"  Model: {payload.get('model')}")
            self.logger.info(f"  Max Tokens: {payload.get('max_tokens') or payload.get('max_completion_tokens')}")
            self.logger.info(f"  Temperature: {payload.get('temperature', 'N/A (æ¨¡å‹é»˜è®¤)')}")
            self.logger.info(f"  Messages Count: {len(messages)}")
            self.logger.info(f"  Tools Count: {len(tools_schema) if tools_schema else 0}")
            
            # ä¼°ç®—è¾“å…¥ token æ•°ï¼ˆç²—ç•¥ä¼°è®¡ï¼šä¸­æ–‡ ~1.5 å­—ç¬¦/tokenï¼Œè‹±æ–‡ ~4 å­—ç¬¦/tokenï¼‰
            total_chars = sum(len(str(m.get('content', ''))) for m in messages)
            estimated_input_tokens = int(total_chars / 2)  # ä¿å®ˆä¼°è®¡
            self.logger.info(f"  ä¼°ç®—è¾“å…¥ Tokens: ~{estimated_input_tokens}")
            self.logger.info("=" * 60)
            
            # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ  tool_choice
            if tools_schema:
                payload["tool_choice"] = "auto"  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
                self.logger.info(f"ğŸ”§ Added {len(tools_schema)} tools to LLM request")
            else:
                self.logger.warning(f"âš ï¸ No tools available for LLM request")

            # æ„å»ºå®Œæ•´ URL
            url = self._build_llm_url()
            self.logger.debug(f"LLM è°ƒç”¨: {url}")

            # å‘é€è¯·æ±‚
            resp = await self._http_client.post(url, json=payload)
            if resp.status_code >= 400:
                error_text = resp.text[:500]
                self.logger.error(f"LLM HTTP {resp.status_code}: {error_text}")
                raise RuntimeError(f"LLM HTTP {resp.status_code}: {error_text}")
            
            data = resp.json()
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - LLM å“åº”ä¿¡æ¯
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¥ LLM API å“åº”:")
            self.logger.info(f"  Choices Count: {len(data.get('choices', []))}")
            
            # æå–å…³é”®ä¿¡æ¯
            choices = data.get("choices", [])
            if choices:
                first = choices[0]
                finish_reason = first.get("finish_reason", "unknown")
                message_obj = first.get("message", {})
                content = message_obj.get("content") or ""
                tool_calls_raw = message_obj.get("tool_calls") or []
                
                # âš ï¸ å…³é”®è¯Šæ–­ç‚¹ï¼šfinish_reason
                self.logger.info(f"  â­ Finish Reason: {finish_reason}")
                if finish_reason == "length":
                    self.logger.warning("  âŒ å“åº”è¢«æˆªæ–­ï¼åŸå› : max_tokens é™åˆ¶")
                    self.logger.warning("  ğŸ’¡ å»ºè®®: å¢åŠ  max_tokens æˆ–å‡å°‘è¾“å…¥é•¿åº¦")
                elif finish_reason == "stop":
                    self.logger.info("  âœ… å“åº”å®Œæ•´ï¼ˆæ­£å¸¸ç»“æŸï¼‰")
                elif finish_reason == "tool_calls":
                    self.logger.info("  ğŸ”§ å“åº”ç±»å‹: å·¥å…·è°ƒç”¨")
                
                self.logger.info(f"  Content Length: {len(content)} å­—ç¬¦")
                self.logger.info(f"  Tool Calls Count: {len(tool_calls_raw)}")
                
                # æ˜¾ç¤º usage ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                usage = data.get("usage", {})
                if usage:
                    self.logger.info(f"  Token Usage:")
                    self.logger.info(f"    - Prompt Tokens: {usage.get('prompt_tokens', 'N/A')}")
                    self.logger.info(f"    - Completion Tokens: {usage.get('completion_tokens', 'N/A')}")
                    self.logger.info(f"    - Total Tokens: {usage.get('total_tokens', 'N/A')}")
                
                # æ˜¾ç¤ºå“åº”å†…å®¹å‰ 200 å­—ç¬¦ï¼ˆç”¨äºéªŒè¯ï¼‰
                if content:
                    preview = content[:200] + ("..." if len(content) > 200 else "")
                    self.logger.info(f"  Content Preview: {preview}")
                
                self.logger.info("=" * 60)
                
                # è§„èŒƒåŒ–å·¥å…·è°ƒç”¨æ ¼å¼
                tool_calls = []
                for tc in tool_calls_raw:
                    if tc.get("type") == "function":
                        fn = tc.get("function", {})
                        tool_calls.append({
                            "id": tc.get("id") or f"tool_{int(datetime.now().timestamp())}",
                            "type": "function",
                            "function": {
                                "name": fn.get("name"),
                                "arguments": fn.get("arguments", "{}")
                            }
                        })
                return {"content": content, "tool_calls": tool_calls}

            # å“åº”ç»“æ„å¼‚å¸¸æ—¶çš„ fallback
            return {"content": content if 'content' in locals() else "", "tool_calls": []}

        except Exception as e:
            self.logger.error(f"LLM çœŸå®è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼ fallback: {e}", exc_info=True)
            self.logger.error(f"LLM é…ç½® - Base URL: {self.config.llm.base_url}")
            self.logger.error(f"LLM é…ç½® - Model: {config.get('model', self.config.llm.models.default)}")
            self.logger.error(f"LLM é…ç½® - API Key å·²è®¾ç½®: {bool(self.config.llm.api_key)}")

        # ==== Fallback å¯å‘å¼é€»è¾‘ ====
        if "search" in user_message.lower() or "æœç´¢" in user_message:
            return {
                "content": "",
                "tool_calls": [
                    {
                        "id": "search_1",
                        "type": "function",
                        "function": {
                            "name": "search_tool",
                            "arguments": json.dumps({"query": user_message})
                        }
                    }
                ]
            }
        if "calculate" in user_message.lower() or "è®¡ç®—" in user_message or any(char in user_message for char in "+-*/"):
            return {
                "content": "",
                "tool_calls": [
                    {
                        "id": "calc_1",
                        "type": "function",
                        "function": {
                            "name": "calculator",
                            "arguments": json.dumps({"expression": user_message})
                        }
                    }
                ]
            }
        
        # é»˜è®¤ fallback å“åº”
        return {
            "content": f"æˆ‘ç†è§£æ‚¨è¯´çš„æ˜¯ï¼š'{user_message}'",
            "tool_calls": []
        }

    async def stream_llm_call(self, messages: List[Dict[str, str]], config: Dict[str, Any], session_id: Optional[str] = None):
        """æµå¼è°ƒç”¨ LLM å¹¶ç”Ÿæˆå¢é‡å“åº”äº‹ä»¶ã€‚

        ä»¥æµå¼æ–¹å¼è°ƒç”¨ LLM API,é€æ­¥ç”Ÿæˆå“åº”å†…å®¹ã€‚è¿”å›ç‰ˆæœ¬åŒ–äº‹ä»¶,åŒ…å«äº‹ä»¶ IDã€æ—¶é—´æˆ³ç­‰å…ƒæ•°æ®ã€‚
        å¦‚æœæµå¼è°ƒç”¨å¤±è´¥,è‡ªåŠ¨ fallback åˆ°éæµå¼è°ƒç”¨ã€‚

        Args:
            messages: æ¶ˆæ¯å†å²åˆ—è¡¨
            config: LLM é…ç½®(model, temperature, max_tokens ç­‰)
            session_id: ä¼šè¯ ID(å¯é€‰)

        Yields:
            äº‹ä»¶å­—å…¸,åŒ…å« version, id, timestamp, type ç­‰å­—æ®µ:
            - start: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'start', model: '...'}
            - delta: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'delta', content: str}
            - tool_calls: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'tool_calls', tool_calls: [...]}
            - end: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'end', content: full_text}
            - error: {version: '1.0', id: 'evt_xxx', timestamp: '...', type: 'error', error: msg}
        """
        # ğŸ†• æ€è€ƒé˜¶æ®µï¼šå‡†å¤‡è°ƒç”¨ LLM
        if self.trace and session_id:
            yield self.trace.thinking_phase("å‡†å¤‡ LLM æµå¼è°ƒç”¨", "call_llm", session_id)
        
        # å¯¼å…¥äº‹ä»¶å·¥å…·å‡½æ•°
        try:
            from api.event_utils import (
                create_start_event, create_delta_event, create_end_event,
                create_error_event, create_tool_calls_event
            )
        except ImportError:
            # å¦‚æœ event_utils ä¸å¯ç”¨,ä½¿ç”¨ fallback æ ¼å¼
            from datetime import datetime
            import uuid
            def create_start_event(sid=None, model=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "start"}
                if model: evt["model"] = model
                if sid: evt["session_id"] = sid
                return evt
            def create_delta_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "delta", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_end_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "end", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_error_event(error, sid=None, error_code=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "error", "error": error}
                if sid: evt["session_id"] = sid
                if error_code: evt["error_code"] = error_code
                return evt
            def create_tool_calls_event(tool_calls, sid=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "tool_calls", "tool_calls": tool_calls}
                if sid: evt["session_id"] = sid
                return evt
        
        user_message = messages[-1]["content"] if messages else ""
        full_text = []
        yielded_tool_calls = False
        model = config.get("model", self.config.llm.models.default)
        
        # å°è¯•æµå¼è°ƒç”¨
        try:
            # ç¡®ä¿ HTTP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            await self._ensure_http_client()

            # è·å–å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰
            self.logger.info(f"ğŸ” [Stream] Loading tools schema for streaming mode...")
            tools_schema = self._get_tools_schema()
            self.logger.info(f"ğŸ” [Stream] Tools schema loaded: {len(tools_schema) if tools_schema else 0} tools")

            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                stream=True,
                tools=tools_schema if tools_schema else None  # ä¼ é€’å·¥å…·å®šä¹‰
            )
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - æµå¼è¯·æ±‚å‚æ•°
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¤ [STREAM] LLM API è¯·æ±‚å‚æ•°:")
            self.logger.info(f"  Model: {payload.get('model')}")
            self.logger.info(f"  Max Tokens: {payload.get('max_tokens') or payload.get('max_completion_tokens')}")
            self.logger.info(f"  Temperature: {payload.get('temperature', 'N/A')}")
            self.logger.info(f"  Messages Count: {len(messages)}")
            self.logger.info(f"  Tools Count: {len(tools_schema) if tools_schema else 0}")
            self.logger.info(f"  Stream Mode: True")
            
            # ä¼°ç®—è¾“å…¥ token
            total_chars = sum(len(str(m.get('content', ''))) for m in messages)
            estimated_input_tokens = int(total_chars / 2)
            self.logger.info(f"  ä¼°ç®—è¾“å…¥ Tokens: ~{estimated_input_tokens}")
            self.logger.info("=" * 60)
            
            # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ  tool_choice
            if tools_schema:
                payload["tool_choice"] = "auto"
                self.logger.info(f"ğŸ”§ [Stream] Added {len(tools_schema)} tools to streaming LLM request")
            else:
                self.logger.warning(f"âš ï¸ [Stream] No tools available for streaming LLM request")
            
            # ä½¿ç”¨æå–çš„ URL æ„å»ºæ–¹æ³•
            url = self._build_llm_url()
            
            self.logger.debug(f"LLM æµå¼è°ƒç”¨ç›®æ ‡: {url}")
            
            yield create_start_event(session_id=session_id, model=model)
            
            # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆæµå¼è¿”å›æ—¶å¯èƒ½åˆ†æ•£åœ¨å¤šä¸ª delta ä¸­ï¼‰
            collected_tool_calls = []
            
            async with self._http_client.stream('POST', url, json=payload) as resp:
                if resp.status_code >= 400:
                    text = await resp.aread()
                    raise RuntimeError(f"æµå¼ HTTP è¯·æ±‚å¤±è´¥ {resp.status_code}: {text[:200]}")
                async for line in resp.aiter_lines():
                    if not line:
                        continue
                    if line.startswith('data:'):
                        data_part = line[5:].strip()
                        if data_part == '[DONE]':
                            break
                        try:
                            data_json = json.loads(data_part)
                        except json.JSONDecodeError:
                            continue
                        for choice in data_json.get('choices', []):
                            delta = choice.get('delta', {})
                            
                            # å¤„ç†æ–‡æœ¬å†…å®¹
                            if 'content' in delta and delta['content']:
                                piece = delta['content']
                                full_text.append(piece)
                                yield create_delta_event(content=piece, session_id=session_id)
                            
                            # æ”¶é›†å·¥å…·è°ƒç”¨ï¼ˆOpenAI æµå¼æ ¼å¼ï¼‰
                            if 'tool_calls' in delta:
                                for tc_delta in delta['tool_calls']:
                                    idx = tc_delta.get('index', 0)
                                    # ç¡®ä¿ list è¶³å¤Ÿé•¿
                                    while len(collected_tool_calls) <= idx:
                                        collected_tool_calls.append({
                                            'id': None,
                                            'type': 'function',
                                            'function': {'name': '', 'arguments': ''}
                                        })
                                    
                                    # ç´¯ç§¯ id
                                    if 'id' in tc_delta:
                                        collected_tool_calls[idx]['id'] = tc_delta['id']
                                    
                                    # ç´¯ç§¯ function name
                                    if 'function' in tc_delta:
                                        fn = tc_delta['function']
                                        if 'name' in fn:
                                            collected_tool_calls[idx]['function']['name'] += fn['name']
                                        if 'arguments' in fn:
                                            collected_tool_calls[idx]['function']['arguments'] += fn['arguments']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if collected_tool_calls:
                self.logger.info(f"ğŸ”§ [Stream] Detected {len(collected_tool_calls)} tool call(s), executing...")
                
                # ğŸ†• æ€è€ƒé˜¶æ®µï¼šæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨
                if self.trace and session_id:
                    yield self.trace.llm_streaming("æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨", session_id, f"å…± {len(collected_tool_calls)} ä¸ªå·¥å…·")
                
                # é€šçŸ¥å‰ç«¯å·¥å…·è°ƒç”¨å¼€å§‹
                yield create_tool_calls_event(tool_calls=collected_tool_calls, session_id=session_id)
                
                # æ‰§è¡Œæ‰€æœ‰å·¥å…·
                tool_results = []
                for tc in collected_tool_calls:
                    try:
                        # è½¬æ¢ä¸º ToolCall å¯¹è±¡
                        tool_call = ToolCall(
                            id=tc.get('id') or f"tool_{int(datetime.now().timestamp())}",
                            name=tc['function']['name'],
                            arguments=json.loads(tc['function']['arguments']) if tc['function']['arguments'] else {}
                        )
                        
                        # ğŸ†• å·¥å…·è°ƒç”¨æ’é˜Ÿäº‹ä»¶
                        if self.trace and session_id:
                            yield self.trace.tool_call_pending(tool_call.name, tool_call.arguments, session_id)
                        
                        # ğŸ†• å·¥å…·æ‰§è¡Œä¸­äº‹ä»¶
                        if self.trace and session_id:
                            yield self.trace.tool_executing(tool_call.name, session_id)
                        
                        import time
                        tool_start_time = time.time()
                        
                        # æ‰§è¡Œå·¥å…·
                        result = await self._execute_tool_call(tool_call)
                        
                        tool_duration = (time.time() - tool_start_time) * 1000
                        
                        # æ ¼å¼åŒ–å·¥å…·ç»“æœå†…å®¹ï¼ˆä½¿ç”¨ result å±æ€§ï¼Œä¸æ˜¯ dataï¼‰
                        if result.success:
                            # result.result å¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²æˆ–å…¶ä»–ç±»å‹
                            if isinstance(result.result, str):
                                result_content = result.result
                            elif isinstance(result.result, (dict, list)):
                                result_content = json.dumps(result.result, ensure_ascii=False)
                            else:
                                result_content = str(result.result)
                        else:
                            result_content = f"Error: {result.error}"
                        
                        # ğŸ†• å·¥å…·ç»“æœäº‹ä»¶
                        if self.trace and session_id:
                            summary = result_content[:100] + "..." if len(result_content) > 100 else result_content
                            yield self.trace.tool_result(
                                tool_call.name, 
                                result.success, 
                                summary, 
                                session_id,
                                tool_duration
                            )
                        
                        tool_results.append({
                            'tool_call_id': tool_call.id,
                            'role': 'tool',
                            'name': tool_call.name,
                            'content': result_content
                        })
                        
                        self.logger.info(f"âœ… [Stream] Tool '{tool_call.name}' executed successfully, result length: {len(result_content)}")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ [Stream] Tool execution failed: {e}")
                        
                        # ğŸ†• å·¥å…·å¤±è´¥äº‹ä»¶
                        if self.trace and session_id:
                            yield self.trace.tool_result(
                                tc['function']['name'],
                                False,
                                f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                                session_id
                            )
                        
                        tool_results.append({
                            'tool_call_id': tc.get('id', 'unknown'),
                            'role': 'tool',
                            'name': tc['function']['name'],
                            'content': f"Error: {str(e)}"
                        })
                
                # ğŸ†• æ€è€ƒé˜¶æ®µï¼šåŸºäºå·¥å…·ç»“æœå†æ¬¡è°ƒç”¨ LLM
                if self.trace and session_id:
                    yield self.trace.llm_streaming("åŸºäºå·¥å…·ç»“æœé‡æ–°æ€è€ƒ", session_id)
                
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼Œå†æ¬¡è°ƒç”¨ LLMï¼ˆæµå¼ï¼‰
                self.logger.info(f"ğŸ”„ [Stream] Calling LLM again with tool results...")
                
                # æ„å»ºæ–°çš„æ¶ˆæ¯åˆ—è¡¨
                new_messages = messages + [
                    {
                        'role': 'assistant',
                        'content': None,
                        'tool_calls': collected_tool_calls
                    }
                ] + tool_results
                
                # è°ƒè¯•ï¼šæ‰“å°æ¶ˆæ¯ç»“æ„
                self.logger.info(f"ğŸ“‹ [Stream] Final message count: {len(new_messages)}")
                self.logger.info(f"ğŸ“‹ [Stream] Last 3 messages roles: {[m.get('role', 'unknown') for m in new_messages[-3:]]}")
                
                # é€’å½’è°ƒç”¨è‡ªå·±ï¼Œä½†ä¸ä¼ é€’å·¥å…·ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
                config_no_tools = config.copy()
                
                # é‡æ–°è°ƒç”¨ï¼ˆè¿™æ¬¡æ˜¯æµå¼è¿”å›å·¥å…·å¤„ç†åçš„ç»“æœï¼‰
                async for event in self._stream_llm_with_tool_results(new_messages, config_no_tools, session_id):
                    yield event
                
                return
            
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸ç»“æŸ
            final_content = ''.join(full_text)
            
            # ğŸ” è¯Šæ–­æ—¥å¿— - æµå¼å“åº”æ€»ç»“
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“¥ [STREAM] LLM æµå¼å“åº”æ€»ç»“:")
            self.logger.info(f"  Total Content Length: {len(final_content)} å­—ç¬¦")
            self.logger.info(f"  Delta Events Count: {len(full_text)}")
            self.logger.info(f"  Tool Calls: {len(collected_tool_calls)}")
            
            # æ˜¾ç¤ºå†…å®¹å‰ 200 å­—ç¬¦
            if final_content:
                preview = final_content[:200] + ("..." if len(final_content) > 200 else "")
                self.logger.info(f"  Content Preview: {preview}")
            
            # âš ï¸ æ³¨æ„ï¼šæµå¼å“åº”é€šå¸¸ä¸è¿”å› finish_reason
            # å¦‚æœå†…å®¹çœ‹èµ·æ¥è¢«æˆªæ–­ï¼ˆçªç„¶ç»“æŸï¼‰ï¼Œå¯èƒ½æ˜¯ max_tokens é™åˆ¶
            if len(final_content) > 5000:
                self.logger.warning("  âš ï¸ å“åº”å†…å®¹è¾ƒé•¿ï¼Œå¦‚æœçœ‹èµ·æ¥ä¸å®Œæ•´ï¼Œå¯èƒ½æ˜¯ max_tokens é™åˆ¶")
            
            self.logger.info("=" * 60)
            
            yield create_end_event(content=final_content, session_id=session_id)
            return
        except Exception as e:
            self.logger.warning(f"æµå¼è°ƒç”¨å¤±è´¥,å›é€€åˆ°éæµå¼: {e}")
            if not full_text:
                # å›é€€åˆ°æ™®é€šè°ƒç”¨
                try:
                    result = await self._make_llm_call(messages, config)
                    content = result.get('content', '')
                    yield create_delta_event(content=content, session_id=session_id)
                    if result.get('tool_calls'):
                        yield create_tool_calls_event(tool_calls=result['tool_calls'], session_id=session_id)
                    yield create_end_event(content=content, session_id=session_id)
                    return
                except Exception as e2:
                    yield create_error_event(error=str(e2), session_id=session_id)
                    return
            else:
                yield create_end_event(content=''.join(full_text), session_id=session_id)
    
    async def _stream_llm_with_tool_results(self, messages: List[Dict], config: Dict, session_id: Optional[str] = None):
        """åœ¨å·¥å…·è°ƒç”¨åç»§ç»­æµå¼è¿”å› LLM çš„æœ€ç»ˆå“åº”ï¼ˆä¸å†ä¼ é€’å·¥å…·ï¼‰"""
        self.logger.info(f"ğŸ¯ [Stream] Starting _stream_llm_with_tool_results with {len(messages)} messages")
        
        try:
            from api.event_utils import create_delta_event, create_end_event, create_error_event
        except ImportError:
            from datetime import datetime
            import uuid
            def create_delta_event(content, sid=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "delta", "content": content}
                if sid: evt["session_id"] = sid
                return evt
            def create_end_event(content, sid=None, metadata=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "end", "content": content}
                if sid: evt["session_id"] = sid
                if metadata: evt["metadata"] = metadata
                return evt
            def create_error_event(error, sid=None, error_code=None):
                evt = {"version": "1.0", "id": f"evt_{uuid.uuid4().hex[:16]}", 
                       "timestamp": datetime.utcnow().isoformat() + "Z", "type": "error", "error": error}
                if sid: evt["session_id"] = sid
                if error_code: evt["error_code"] = error_code
                return evt
        
        try:
            await self._ensure_http_client()
            
            # ä¸å†ä¼ é€’å·¥å…·ï¼Œé¿å…æ— é™é€’å½’
            payload = prepare_llm_params(
                model=config.get("model", self.config.llm.models.default),
                messages=messages,
                temperature=config.get("temperature", self.config.llm.temperature),
                max_tokens=config.get("max_tokens", self.config.llm.max_tokens),
                stream=True
                # tools=None  # æ˜ç¡®ä¸ä¼ é€’å·¥å…·
            )
            
            url = self._build_llm_url()
            full_response = []
            
            self.logger.info(f"ğŸŒ [Stream] Calling LLM API for tool result processing...")
            
            async with self._http_client.stream('POST', url, json=payload) as resp:
                if resp.status_code >= 400:
                    text = await resp.aread()
                    raise RuntimeError(f"æµå¼ HTTP è¯·æ±‚å¤±è´¥ {resp.status_code}: {text[:200]}")
                
                async for line in resp.aiter_lines():
                    if not line or not line.startswith('data:'):
                        continue
                    
                    data_part = line[5:].strip()
                    if data_part == '[DONE]':
                        break
                    
                    try:
                        data_json = json.loads(data_part)
                    except json.JSONDecodeError:
                        continue
                    
                    for choice in data_json.get('choices', []):
                        delta = choice.get('delta', {})
                        if 'content' in delta and delta['content']:
                            piece = delta['content']
                            full_response.append(piece)
                            self.logger.debug(f"ğŸ“¤ [Stream] Yielding delta: {piece[:50]}...")
                            yield create_delta_event(content=piece, session_id=session_id)
            
            final_content = ''.join(full_response)
            self.logger.info(f"âœ… [Stream] Tool result processing complete, total length: {len(final_content)}")
            yield create_end_event(content=final_content, session_id=session_id)
            
        except Exception as e:
            self.logger.error(f"å·¥å…·ç»“æœæµå¼è°ƒç”¨å¤±è´¥: {e}")
            yield create_error_event(error=str(e), session_id=session_id)
    
    def _has_tool_calls(self, response: Dict[str, Any]) -> bool:
        """Check if LLM response contains tool calls."""
        return bool(response.get("tool_calls"))
    
    def _extract_tool_calls(self, response: Dict[str, Any]) -> List[ToolCall]:
        """Extract tool calls from LLM response."""
        tool_calls = []
        
        for call_data in response.get("tool_calls", []):
            if call_data.get("type") == "function":
                function_data = call_data.get("function", {})
                try:
                    arguments = json.loads(function_data.get("arguments", "{}"))
                except json.JSONDecodeError:
                    arguments = {}
                
                tool_call = ToolCall(
                    id=call_data.get("id", f"tool_{int(datetime.now().timestamp())}"),
                    name=function_data.get("name", "unknown"),
                    arguments=arguments
                )
                tool_calls.append(tool_call)
        
        return tool_calls
    
    async def _execute_tool_call(self, tool_call: ToolCall) -> ToolResult:
        """Execute a tool call using MCP tool registry."""
        try:
            # Try to use MCP tool registry
            try:
                from mcp import get_tool_registry
                
                registry = get_tool_registry()
                
                # Map common LLM tool names to MCP tool names
                tool_name_mapping = {
                    "search_tool": "web_search",
                    "web_search": "web_search",
                    "calculator": "calculator",
                    "time_tool": "get_time",
                    "get_time": "get_time",
                    "weather": "get_weather",
                    "get_weather": "get_weather",
                }
                
                mcp_tool_name = tool_name_mapping.get(tool_call.name, tool_call.name)
                
                # Execute through registry
                result_dict = await registry.execute(mcp_tool_name, **tool_call.arguments)
                
                # Convert MCP ToolResult to Agent ToolResult
                if result_dict.get("success"):
                    result_str = json.dumps(result_dict.get("data", {}), ensure_ascii=False)
                else:
                    result_str = None
                
                return ToolResult(
                    call_id=tool_call.id,
                    success=result_dict.get("success", False),
                    result=result_str,
                    error=result_dict.get("error")
                )
            
            except ImportError:
                self.logger.warning("MCP tools not available, using fallback implementation")
                # Fall back to placeholder implementation
                pass
            
            # Fallback placeholder implementation
            if tool_call.name in ["search_tool", "web_search"]:
                result = f"Search results for: {tool_call.arguments.get('query', 'unknown')}"
            elif tool_call.name == "calculator":
                expression = tool_call.arguments.get('expression', '')
                try:
                    result = f"The result is: {expression} (calculation placeholder)"
                except:
                    result = "Could not calculate the expression"
            elif tool_call.name in ["time_tool", "get_time"]:
                result = f"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            else:
                result = f"Tool '{tool_call.name}' executed with arguments: {tool_call.arguments}"
            
            return ToolResult(
                call_id=tool_call.id,
                success=True,
                result=result
            )
            
        except Exception as e:
            self.logger.error(f"Tool execution error: {e}", exc_info=True)
            return ToolResult(
                call_id=tool_call.id,
                success=False,
                result=None,
                error=str(e)
            )
    
    async def _save_tool_call_to_database(
        self,
        session_id: str,
        tool_call: ToolCall,
        result: ToolResult
    ) -> None:
        """
        ä¿å­˜å·¥å…·è°ƒç”¨è®°å½•åˆ°æ•°æ®åº“
        
        Args:
            session_id: ä¼šè¯ID
            tool_call: å·¥å…·è°ƒç”¨å¯¹è±¡
            result: å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            # âœ… ä½¿ç”¨å…¨å±€æ•°æ®åº“å¼•æ“ï¼ˆä» main.py çš„ app.state è·å–ï¼‰
            from database.repositories import ToolCallRepository
            from sqlalchemy.ext.asyncio import AsyncSession
            
            # å°è¯•ä»å…¨å±€è·å–æ•°æ®åº“å¼•æ“
            try:
                from api.main import app
                if hasattr(app.state, 'db_engine'):
                    db_engine = app.state.db_engine
                else:
                    self.logger.warning("âš ï¸ app.state.db_engine ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜å·¥å…·è°ƒç”¨")
                    return
            except:
                self.logger.warning("âš ï¸ æ— æ³•è·å–å…¨å±€æ•°æ®åº“å¼•æ“ï¼Œè·³è¿‡ä¿å­˜å·¥å…·è°ƒç”¨")
                return
            
            # åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯
            async with AsyncSession(db_engine) as db_session:
                tool_call_repo = ToolCallRepository(db_session)
                
                # æå–æ‰§è¡Œæ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
                execution_time_ms = None
                result_data = {}
                
                if result.success and result.result:
                    try:
                        result_data = {"data": result.result, "success": True}
                    except:
                        result_data = {"data": str(result.result), "success": True}
                else:
                    result_data = {"success": False, "error": result.error}
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                await tool_call_repo.save_tool_call(
                    session_id=session_id,
                    tool_name=tool_call.name,
                    parameters=tool_call.arguments,
                    result=result_data,
                    execution_time_ms=execution_time_ms
                )
                
                # âœ… æäº¤äº‹åŠ¡
                await db_session.commit()
                
                self.logger.info(f"ğŸ’¾ å·¥å…·è°ƒç”¨å·²ä¿å­˜åˆ°æ•°æ®åº“: {tool_call.name} (session: {session_id})")
        
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜å·¥å…·è°ƒç”¨åˆ°æ•°æ®åº“å¤±è´¥: {e}", exc_info=True)
            self.logger.error(f"   Session ID: {session_id}")
            self.logger.error(f"   Tool Name: {tool_call.name}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“æ­£å¸¸æµç¨‹