"""
è¯­éŸ³ä»£ç†å›¾(Graph)å®ç°

æœ¬æ¨¡å—å®ç°äº†è¯­éŸ³å¯¹è¯ä»£ç†çš„ä¸»è¦ LangGraph å·¥ä½œæµ,
å®šä¹‰ä¸åŒå¤„ç†èŠ‚ç‚¹ä¹‹é—´çš„æµç¨‹å’Œæ¡ä»¶è·¯ç”±ã€‚
"""
# ä»å„ä¸ªåŒ…ä¸­å¯¼å…¥ç›¸å…³æ¨¡å—
import logging
import asyncio
from typing import Dict, Any, Optional, List

try:
    from langgraph.graph import StateGraph, END
    from langgraph.checkpoint.memory import MemorySaver
    LANGGRAPH_AVAILABLE = True
except ImportError:
    # LangGraph not available in test environment
    LANGGRAPH_AVAILABLE = False
    StateGraph = None
    END = None
    MemorySaver = None

from .state import AgentState, create_initial_state
from .nodes import AgentNodes

# å¯¼å…¥ LLM å…¼å®¹æ€§å·¥å…·
try:
    from utils.llm_compat import prepare_llm_params
except ImportError:
    def prepare_llm_params(model, messages, temperature=0.7, max_tokens=16384, **kwargs):  # ğŸ”§ ä¿®å¤é»˜è®¤å€¼
        params = {"model": model, "messages": messages}
        # GPT-5 ç³»åˆ—ä¸ä¼  temperatureï¼Œä½¿ç”¨ API é»˜è®¤å€¼
        if not model.startswith("gpt-5"):
            params["temperature"] = temperature
        if model.startswith("gpt-5"):
            params["max_completion_tokens"] = max_tokens
        else:
            params["max_tokens"] = max_tokens
        params.update(kwargs)
        return params

try:
    from config.models import VoiceAgentConfig
    from config.settings import ConfigManager
except ImportError:
    # Fallback for when running as script
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from config.models import VoiceAgentConfig
    from config.settings import ConfigManager


logger = logging.getLogger(__name__)

# ä¸»ç±»
class VoiceAgent:
    """
    ä¸»è¯­éŸ³å¯¹è¯ä»£ç†,ä½¿ç”¨ LangGraph è¿›è¡Œæµç¨‹æ§åˆ¶ã€‚
    
    è¯¥ç±»åè°ƒå¯¹è¯æµç¨‹é€šè¿‡ä¸åŒå¤„ç†é˜¶æ®µ:
    è¾“å…¥å¤„ç†ã€LLM è°ƒç”¨ã€å·¥å…·å¤„ç†å’Œå“åº”æ ¼å¼åŒ–ã€‚
    """
    # åˆå§‹åŒ–æ–¹æ³•
    def __init__(self, config: VoiceAgentConfig):
        """ä½¿ç”¨é…ç½®åˆå§‹åŒ–è¯­éŸ³ä»£ç†ã€‚"""
        self.config = config
        self.logger = logger  # Set logger before building graph
        self.nodes = AgentNodes(config)
        self.graph = self._build_graph()
        
        self.logger.info("è¯­éŸ³ä»£ç†åˆå§‹åŒ–æˆåŠŸ")
    
    def _build_graph(self):
        """æ„å»º LangGraph å·¥ä½œæµã€‚"""
        if not LANGGRAPH_AVAILABLE:
            raise ImportError("LangGraph ä¸å¯ç”¨ã€‚è¯·å®‰è£… langgraph åŒ…ã€‚")
        
        # Create graph with state schema
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("process_input", self.nodes.process_input)
        workflow.add_node("call_llm", self.nodes.call_llm)
        workflow.add_node("handle_tools", self.nodes.handle_tools)
        workflow.add_node("format_response", self.nodes.format_response)
        
        # Set entry point
        workflow.set_entry_point("process_input")
        
        # Add conditional edges based on next_action
        workflow.add_conditional_edges(
            "process_input",
            self._route_after_input,
            {
                "call_llm": "call_llm",
                "error": END,
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "call_llm",
            self._route_after_llm,
            {
                "handle_tools": "handle_tools",
                "format_response": "format_response",
                "error": "format_response"
            }
        )
        
        workflow.add_conditional_edges(
            "handle_tools",
            self._route_after_tools,
            {
                "call_llm": "call_llm",
                "format_response": "format_response",
                "error": "format_response"
            }
        )
        
        workflow.add_edge("format_response", END)
        
        # Compile workflow with checkpointer (memory or database)
        checkpointer = self._get_checkpointer()
        graph = workflow.compile(checkpointer=checkpointer)
        
        self.logger.debug("LangGraph å·¥ä½œæµç¼–è¯‘æˆåŠŸ")
        return graph
    
    def _get_checkpointer(self):
        """
        Get appropriate checkpointer based on configuration.
        
        Returns:
            Checkpointer instance (MemorySaver or PostgreSQLCheckpointer)
        """
        # Check if database persistence is enabled
        if hasattr(self.config, 'database') and self.config.database.enabled:
            # Check if session storage is set to database
            if self.config.session.storage_type == "database":
                try:
                    from database import PostgreSQLCheckpointer
                    self.logger.info("Using PostgreSQL checkpointer for persistence")
                    return PostgreSQLCheckpointer()
                except Exception as e:
                    self.logger.warning(
                        f"Failed to initialize PostgreSQL checkpointer: {e}. "
                        f"Falling back to memory."
                    )
        
        # Default to memory saver
        self.logger.info("Using in-memory checkpointer")
        return MemorySaver()
    
    def _route_after_input(self, state: AgentState) -> str:
        """è¾“å…¥å¤„ç†åçš„è·¯ç”±å†³ç­–ã€‚"""
        if state.get("error_state"):
            self.logger.warning(f"è¾“å…¥å¤„ç†å‡ºé”™: {state['error_state']}")
            return "error"
        
        if not state.get("should_continue", True):
            return "end"
        
        next_action = state.get("next_action")
        if next_action == "call_llm":
            return "call_llm"
        
        self.logger.warning(f"è¾“å…¥å¤„ç†åå‡ºç°æ„å¤–çš„ next_action: {next_action}")
        return "error"
    
    def _route_after_llm(self, state: AgentState) -> str:
        """
        LLM è°ƒç”¨åçš„è·¯ç”±å†³ç­–ã€‚
        
        ğŸ†• ä¼˜åŒ–é€»è¾‘: æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
        - å¦‚æœ LLM è¿”å›å·¥å…·è°ƒç”¨ â†’ è¿›å…¥ handle_tools
        - å¦åˆ™ â†’ è¿›å…¥ format_response ç”Ÿæˆæœ€ç»ˆå›å¤
        """
        if state.get("error_state"):
            self.logger.warning(f"LLM è°ƒç”¨å‡ºé”™: {state['error_state']}")
            return "error"
        
        next_action = state.get("next_action")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        if next_action == "handle_tools":
            tool_call_count = state.get("tool_call_count", 0)
            max_tool_iterations = 5  # ğŸ†• æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
            
            if tool_call_count >= max_tool_iterations:
                self.logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•° ({max_tool_iterations})ï¼Œå¼ºåˆ¶ç»“æŸ")
                return "format_response"
            
            self.logger.info(f"ğŸ”§ ç¬¬ {tool_call_count + 1} è½®å·¥å…·è°ƒç”¨")
            return "handle_tools"
        
        elif next_action == "format_response":
            return "format_response"
        
        self.logger.warning(f"LLM è°ƒç”¨åå‡ºç°æ„å¤–çš„ next_action: {next_action}")
        return "error"
    
    def _route_after_tools(self, state: AgentState) -> str:
        """
        å·¥å…·å¤„ç†åçš„è·¯ç”±å†³ç­–ã€‚
        
        ğŸ†• æ ¸å¿ƒä¼˜åŒ–: å·¥å…·è°ƒç”¨åè¿”å› LLM è¿›è¡Œé‡æ–°æ€è€ƒ
        - é»˜è®¤è¡Œä¸º: è¿”å› call_llmï¼Œè®© LLM åŸºäºå·¥å…·ç»“æœé‡æ–°åˆ¤æ–­
        - LLM ä¼šå†³å®š: æ˜¯å¦éœ€è¦æ›´å¤šå·¥å…·ï¼Œè¿˜æ˜¯å·²ç»æœ‰è¶³å¤Ÿä¿¡æ¯ç”Ÿæˆå›å¤
        """
        if state.get("error_state"):
            self.logger.warning(f"å·¥å…·å¤„ç†å‡ºé”™: {state['error_state']}")
            # å³ä½¿å·¥å…·å¤±è´¥ï¼Œä¹Ÿè¿”å› LLM è®©å®ƒç”Ÿæˆ fallback å›å¤
            return "call_llm"
        
        next_action = state.get("next_action")
        
        # ğŸ†• å…³é”®æ”¹åŠ¨: å·¥å…·è°ƒç”¨åï¼Œå§‹ç»ˆè¿”å› call_llm è¿›è¡Œé‡æ–°æ€è€ƒ
        # LLM ä¼šåŸºäºå·¥å…·ç»“æœåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´å¤šå·¥å…·æˆ–ç›´æ¥ç”Ÿæˆå›å¤
        if next_action == "call_llm":
            self.logger.info("ğŸ”„ å·¥å…·è°ƒç”¨å®Œæˆï¼Œè¿”å› LLM é‡æ–°æ€è€ƒ")
            return "call_llm"
        
        # å…œåº•: å¦‚æœèŠ‚ç‚¹å¼ºåˆ¶æŒ‡å®š format_responseï¼ˆä¸å¤ªå¯èƒ½ï¼‰
        elif next_action == "format_response":
            self.logger.info("âš ï¸ å·¥å…·èŠ‚ç‚¹ç›´æ¥æŒ‡å®šç”Ÿæˆå“åº”ï¼ˆç½•è§æƒ…å†µï¼‰")
            return "format_response"
        
        # é»˜è®¤è¡Œä¸º: è¿”å› LLM
        self.logger.info("ğŸ”„ å·¥å…·è°ƒç”¨å®Œæˆï¼Œé»˜è®¤è¿”å› LLM")
        return "call_llm"
    # åŒæ­¥å¤„ç†å•æ¡ä¿¡æ¯
    async def process_message(
        self,
        user_input: str,
        session_id: str,
        user_id: Optional[str] = None,
        model_config: Optional[Dict[str, Any]] = None,
        external_history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›ä»£ç†çš„å“åº”ã€‚
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
            session_id: å”¯ä¸€ä¼šè¯æ ‡è¯†ç¬¦
            user_id: å¯é€‰çš„ç”¨æˆ·æ ‡è¯†ç¬¦
            model_config: å¯é€‰çš„æ¨¡å‹é…ç½®è¦†ç›–
            external_history: å¯é€‰çš„å¤–éƒ¨å†å²æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            åŒ…å«ä»£ç†å“åº”å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        try:
            self.logger.info(f"å¤„ç†ä¼šè¯ {session_id} çš„æ¶ˆæ¯")
            
            # Create initial state
            initial_state = create_initial_state(
                session_id=session_id,
                user_input=user_input,
                user_id=user_id,
                model_config=model_config or {}
            )
            
            # å¦‚æœæœ‰å¤–éƒ¨å†å²ï¼Œæ·»åŠ åˆ° stateï¼ˆå³ä½¿æ˜¯ç©ºåˆ—è¡¨ä¹Ÿè¦æ·»åŠ ï¼‰
            if external_history is not None:
                initial_state["external_history"] = external_history
                self.logger.info(f"ğŸ”„ Added {len(external_history)} messages to initial_state['external_history']")
            else:
                self.logger.warning(f"âš ï¸ No external_history provided to process_message (None)")
            
            # Configure thread for session persistence
            thread_config = {"configurable": {"thread_id": session_id}}
            
            # Run the graph
            final_state = await self.graph.ainvoke(
                initial_state,
                config=thread_config
            )
            
            # Prepare response
            response = {
                "success": True,
                "response": final_state["agent_response"],
                "session_id": session_id,
                "message_count": len(final_state["messages"]),
                "timestamp": final_state["last_activity"].isoformat(),
                "metadata": {
                    "intent": final_state.get("current_intent"),
                    "tool_calls": len(final_state.get("tool_calls", [])),
                    "model": final_state["model_config"].get("model", "unknown"),
                    "error_state": final_state.get("error_state")
                }
            }
            
            self.logger.info(f"ä¼šè¯ {session_id} çš„æ¶ˆæ¯å¤„ç†æˆåŠŸ")
            return response
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return {
                "success": False,
                "response": "æŠ±æ­‰,å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é”™è¯¯ã€‚",
                "session_id": session_id,
                "error": str(e),
                "timestamp": None,
                "metadata": {"error": True}
            }
    # æŸ¥è¯¢è·å–åˆ°å†å²ä¿¡æ¯
    async def get_conversation_history(
        self,
        session_id: str,
        limit: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        æ£€ç´¢ä¼šè¯çš„å¯¹è¯å†å²ã€‚
        
        Args:
            session_id: ä¼šè¯æ ‡è¯†ç¬¦
            limit: å¯é€‰çš„æ¶ˆæ¯æ•°é‡é™åˆ¶
            
        Returns:
            åŒ…å«å¯¹è¯å†å²çš„å­—å…¸
        """
        try:
            # Get state from memory
            thread_config = {"configurable": {"thread_id": session_id}}
            
            # Get current state (this would be the last saved state)
            # In a real implementation, we'd retrieve from the checkpointer
            state = await self.graph.aget_state(thread_config)
            
            if not state or not state.values:
                return {
                    "success": True,
                    "session_id": session_id,
                    "messages": [],
                    "message_count": 0
                }
            
            messages = state.values.get("messages", [])
            if limit:
                messages = messages[-limit:]
            
            return {
                "success": True,
                "session_id": session_id,
                "messages": [msg.dict() if hasattr(msg, 'dict') else msg for msg in messages],
                "message_count": len(messages),
                "last_activity": state.values.get("last_activity")
            }

        except Exception as e:
            self.logger.error(f"æ£€ç´¢å¯¹è¯å†å²æ—¶å‡ºé”™: {e}")
            return {
                "success": False,
                "session_id": session_id,
                "error": str(e),
                "messages": [],
                "message_count": 0
            }
    # æµå¼å¤„ç†å•æ¡ä¿¡æ¯
    async def process_message_stream(
        self,
        user_input: str,
        session_id: str,
        user_id: Optional[str] = None,
        model_config: Optional[Dict[str, Any]] = None,
        external_history: Optional[List[Dict]] = None  # æ–°å¢ï¼šå¤–éƒ¨å†å²å‚æ•°
    ):
        """æµå¼å¤„ç†ç”¨æˆ·æ¶ˆæ¯,ä½œä¸ºå¼‚æ­¥ç”Ÿæˆå™¨äº§ç”Ÿäº‹ä»¶ã€‚

        äº§ç”Ÿå­—å…¸äº‹ä»¶(ç»“æ„è§ AgentNodes.stream_llm_call)ã€‚
        æµå¼ä¼ è¾“å®Œæˆå,å°†å®Œæ•´çš„åŠ©æ‰‹å“åº”æŒä¹…åŒ–åˆ°å¯¹è¯å†å²ã€‚
        """
        # Build initial state similar to process_message but manual step execution
        initial_state = create_initial_state(
            session_id=session_id,
            user_input=user_input,
            user_id=user_id,
            model_config=model_config or {}
        )
        
        # æ·»åŠ å¤–éƒ¨å†å²åˆ° state
        if external_history is not None:
            initial_state["external_history"] = external_history
            self.logger.info(f"ğŸ”„ [Stream] Added {len(external_history)} messages to initial_state['external_history']")
        else:
            self.logger.warning(f"âš ï¸ [Stream] No external_history provided to process_message_stream")
        
        accumulated_content = []  # æ”¶é›† delta ç‰‡æ®µç”¨äºæœ€ç»ˆæŒä¹…åŒ–
        
        try:
            # æ­¥éª¤ 1: process_input
            state = await self.nodes.process_input(initial_state)
            if state.get('error_state'):
                yield {"type": "error", "error": state['error_state']}
                return
            
            # æ­¥éª¤ 2: æµå¼ LLM å¹¶ç´¯ç§¯å†…å®¹
            external_history_for_llm = state.get("external_history")
            messages = self.nodes._prepare_llm_messages(state, external_history=external_history_for_llm)
            model = state["model_config"].get("model", self.config.llm.models.default)
            llm_config = prepare_llm_params(
                model=model,
                messages=messages,
                temperature=state.get("temperature", self.config.llm.temperature),
                max_tokens=state.get("max_tokens", self.config.llm.max_tokens)
            )
            
            async for event in self.nodes.stream_llm_call(messages, llm_config, session_id=session_id):
                # æ”¶é›† delta ç‰‡æ®µ
                if event.get("type") == "delta" and "content" in event:
                    accumulated_content.append(event["content"])
                yield event
            
            # æ­¥éª¤ 3: å°†å®Œæ•´å“åº”æŒä¹…åŒ–åˆ°å¯¹è¯å†å²
            if accumulated_content:
                full_response = "".join(accumulated_content)
                
                # ä½¿ç”¨åŠ©æ‰‹æ¶ˆæ¯æ›´æ–°çŠ¶æ€
                state["messages"].append({
                    "role": "assistant",
                    "content": full_response
                })
                
                # æŒä¹…åŒ–åˆ° checkpointer (LangGraph MemorySaver)
                thread_config = {"configurable": {"thread_id": session_id}}
                
                # ä½¿ç”¨æœ€ç»ˆçŠ¶æ€è°ƒç”¨å›¾ä»¥è¿›è¡ŒæŒä¹…åŒ–
                try:
                    # ä½¿ç”¨ ainvoke é€šè¿‡ checkpointer æŒä¹…åŒ–æœ€ç»ˆçŠ¶æ€
                    final_state = {
                        **state,
                        "next_action": "format_response",  # è·³åˆ°ç»“æŸ
                        "should_continue": False
                    }
                    
                    # è¿™å°†ä¿å­˜æ›´æ–°çš„çŠ¶æ€,åŒ…æ‹¬æ–°çš„åŠ©æ‰‹æ¶ˆæ¯
                    await self.graph.ainvoke(final_state, config=thread_config)
                    
                    self.logger.debug(f"å·²å°†æµå¼å“åº”æŒä¹…åŒ–åˆ°ä¼šè¯ {session_id} å†å²")
                
                except Exception as persist_error:
                    self.logger.warning(f"æŒä¹…åŒ–æµå¼å†å²å¤±è´¥: {persist_error}")
                    # éè‡´å‘½:æµå¼ä¼ è¾“å·²æˆåŠŸå®Œæˆ
        
        except asyncio.CancelledError:
            self.logger.info(f"ä¼šè¯ {session_id} çš„æµè¢«å–æ¶ˆ")
            # ä»å°è¯•æŒä¹…åŒ–éƒ¨åˆ†å†…å®¹(å¦‚æœæœ‰)
            if accumulated_content:
                try:
                    partial_response = "".join(accumulated_content)
                    state["messages"].append({
                        "role": "assistant",
                        "content": f"[å·²å–æ¶ˆ] {partial_response}"
                    })
                    thread_config = {"configurable": {"thread_id": session_id}}
                    await self.graph.ainvoke(state, config=thread_config)
                except Exception:
                    pass
            raise
        
        except Exception as e:
            self.logger.error(f"æµå¼æ¶ˆæ¯å¤„ç†å‡ºé”™: {e}")
            yield {"type": "error", "error": str(e)}
    # æ¸…é™¤å†å²æ¶ˆæ¯
    async def clear_conversation(self, session_id: str) -> Dict[str, Any]:
        """
        æ¸…é™¤ä¼šè¯çš„å¯¹è¯å†å²ã€‚
        
        Args:
            session_id: ä¼šè¯æ ‡è¯†ç¬¦
            
        Returns:
            æŒ‡ç¤ºæˆåŠŸ/å¤±è´¥çš„å­—å…¸
        """
        try:
            # ä¸ºä¼šè¯åˆ›å»ºæ–°çš„ç©ºçŠ¶æ€
            thread_config = {"configurable": {"thread_id": session_id}}
            
            # åœ¨å®é™…å®ç°ä¸­,æˆ‘ä»¬ä¼šæ¸…é™¤ checkpointer çŠ¶æ€
            # ç°åœ¨,æˆ‘ä»¬åªåˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºçŠ¶æ€
            empty_state = create_initial_state(
                session_id=session_id,
                user_input="",
                model_config={}
            )
            
            self.logger.info(f"ä¼šè¯ {session_id} çš„å¯¹è¯å·²æ¸…é™¤")
            return {
                "success": True,
                "session_id": session_id,
                "message": "å¯¹è¯å†å²å·²æ¸…é™¤"
            }
            
        except Exception as e:
            self.logger.error(f"æ¸…é™¤å¯¹è¯æ—¶å‡ºé”™: {e}")
            return {
                "success": False,
                "session_id": session_id,
                "error": str(e)
            }
    
    def get_available_tools(self) -> List[str]:
        """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨ã€‚"""
        return self.config.tools.enabled
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ¨¡å‹é…ç½®ã€‚"""
        return {
            "provider": self.config.llm.provider.value,
            "default_model": self.config.llm.models.default,
            "available_models": {
                "default": self.config.llm.models.default,
                "fast": self.config.llm.models.fast,
                "creative": self.config.llm.models.creative
            },
            "temperature": self.config.llm.temperature,
            "max_tokens": self.config.llm.max_tokens
        }

# åˆ›å»ºè¯­éŸ³åŠ©æ‰‹å®ä¾‹çš„å·¥å‚å‡½æ•°
def create_voice_agent(config_path: Optional[str] = None, environment: str = "development") -> VoiceAgent:
    """
    åˆ›å»ºè¯­éŸ³ä»£ç†å®ä¾‹çš„å·¥å‚å‡½æ•°ã€‚
    
    Args:
        config_path: é…ç½®ç›®å½•çš„å¯é€‰è·¯å¾„
        environment: è¦åŠ è½½çš„ç¯å¢ƒé…ç½®
        
    Returns:
        å·²é…ç½®çš„ VoiceAgent å®ä¾‹
    """
    try:
        # Load configuration
        if config_path:
            config_manager = ConfigManager(config_path)
        else:
            from pathlib import Path
            default_config_path = Path(__file__).parent.parent.parent / "config"
            config_manager = ConfigManager(default_config_path)
        
        config = config_manager.load_config(environment)
        
        # åˆ›å»ºå¹¶è¿”å›ä»£ç†
        agent = VoiceAgent(config)
        logger.info(f"è¯­éŸ³ä»£ç†ä½¿ç”¨ {environment} é…ç½®åˆ›å»ºæˆåŠŸ")
        return agent
        
    except Exception as e:
        logger.error(f"åˆ›å»ºè¯­éŸ³ä»£ç†æ—¶å‡ºé”™: {e}")
        raise