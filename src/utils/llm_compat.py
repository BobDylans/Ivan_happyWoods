"""
LLM API å…¼å®¹æ€§å·¥å…·

å¤„ç†ä¸åŒ LLM æä¾›å•†å’Œæ¨¡å‹ç‰ˆæœ¬ä¹‹é—´çš„å‚æ•°å·®å¼‚ã€‚
"""

from typing import Dict, Any


def prepare_llm_params(
    model: str,
    messages: list,
    temperature: float = 0.7,
    max_tokens: int = 16384,  # ğŸ”§ ä¿®å¤é»˜è®¤å€¼ä» 2048 æå‡åˆ° 16384
    **kwargs
) -> Dict[str, Any]:
    """
    å‡†å¤‡ LLM API è°ƒç”¨å‚æ•°ï¼Œè‡ªåŠ¨å¤„ç†ä¸åŒæ¨¡å‹çš„å‚æ•°å…¼å®¹æ€§
    
    Args:
        model: æ¨¡å‹åç§°
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        temperature: æ¸©åº¦å‚æ•°ï¼ˆæŸäº›æ¨¡å‹ä¸æ”¯æŒä¼šè¢«å¿½ç•¥ï¼‰
        max_tokens: æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ 16384ï¼‰
        **kwargs: å…¶ä»–é¢å¤–å‚æ•°
    
    Returns:
        é€‚é…åçš„å‚æ•°å­—å…¸
    
    Examples:
        >>> params = prepare_llm_params("gpt-5-mini", messages, max_tokens=100)
        >>> # è¿”å› {"model": "gpt-5-mini", "messages": [...], "max_completion_tokens": 100, ...}
        
        >>> params = prepare_llm_params("gpt-5-pro", messages, max_tokens=100)
        >>> # è¿”å› {"model": "gpt-5-pro", "messages": [...], "max_completion_tokens": 100}
        >>> # æ³¨æ„ï¼šgpt-5-pro ä¸æ”¯æŒ temperatureï¼Œæ‰€ä»¥ä¸åŒ…å«è¯¥å‚æ•°
        
        >>> params = prepare_llm_params("gpt-4", messages, max_tokens=100)
        >>> # è¿”å› {"model": "gpt-4", "messages": [...], "max_tokens": 100, "temperature": 0.7}
    """
    params = {
        "model": model,
        "messages": messages,
    }
    
    # è·å–æ¨¡å‹ç‰¹æ€§
    features = get_model_features(model)
    
    # GPT-5 ç³»åˆ—ä¸ä¼  temperatureï¼Œä½¿ç”¨ API é»˜è®¤å€¼
    # å…¶ä»–æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰æ‰æ”¯æŒè‡ªå®šä¹‰ temperature
    if features.get("supports_temperature", True) and not model.startswith("gpt-5"):
        params["temperature"] = temperature
    
    # GPT-5 ç³»åˆ—æ¨¡å‹ä½¿ç”¨ max_completion_tokens
    if model.startswith("gpt-5"):
        params["max_completion_tokens"] = max_tokens
    # GPT-4 åŠæ›´æ—©ç‰ˆæœ¬ä½¿ç”¨ max_tokens
    else:
        params["max_tokens"] = max_tokens
    
    # æ·»åŠ å…¶ä»–å‚æ•°
    params.update(kwargs)
    
    return params


def is_gpt5_model(model: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸º GPT-5 ç³»åˆ—æ¨¡å‹
    
    Args:
        model: æ¨¡å‹åç§°
    
    Returns:
        True å¦‚æœæ˜¯ GPT-5 ç³»åˆ—ï¼ŒFalse å¦åˆ™
    """
    return model.startswith("gpt-5")


def get_max_tokens_param_name(model: str) -> str:
    """
    è·å–æ¨¡å‹çš„ max_tokens å‚æ•°å
    
    Args:
        model: æ¨¡å‹åç§°
    
    Returns:
        å‚æ•°åç§°å­—ç¬¦ä¸²
    """
    if is_gpt5_model(model):
        return "max_completion_tokens"
    return "max_tokens"


# æ¨¡å‹ç‰¹æ€§æ˜ å°„
MODEL_FEATURES = {
    # GPT-5 ç³»åˆ— - æ³¨æ„ï¼šgpt-5-pro ä¸æ”¯æŒ temperature å‚æ•°
    "gpt-5-pro": {
        "max_tokens_param": "max_completion_tokens",
        "supports_temperature": False,  # gpt-5-pro ä¸æ”¯æŒ temperature
        "supports_vision": True,
        "supports_function_calling": True,
        "max_context": 128000,
    },
    "gpt-5-mini": {
        "max_tokens_param": "max_completion_tokens",
        "supports_temperature": True,
        "supports_vision": True,
        "supports_function_calling": True,
        "max_context": 128000,
    },
    "gpt-5-chat-latest": {
        "max_tokens_param": "max_completion_tokens",
        "supports_temperature": True,
        "supports_vision": True,
        "supports_function_calling": True,
        "max_context": 128000,
    },
    "gpt-5-nano": {
        "max_tokens_param": "max_completion_tokens",
        "supports_temperature": True,
        "supports_vision": False,
        "supports_function_calling": True,
        "max_context": 32000,
    },
    
    # GPT-4 ç³»åˆ—ï¼ˆå…¼å®¹ï¼‰
    "gpt-4": {
        "max_tokens_param": "max_tokens",
        "supports_temperature": True,
        "supports_vision": False,
        "supports_function_calling": True,
        "max_context": 8192,
    },
    "gpt-4-turbo": {
        "max_tokens_param": "max_tokens",
        "supports_temperature": True,
        "supports_vision": True,
        "supports_function_calling": True,
        "max_context": 128000,
    },
    
    # GPT-3.5 ç³»åˆ—ï¼ˆå…¼å®¹ï¼‰
    "gpt-3.5-turbo": {
        "max_tokens_param": "max_tokens",
        "supports_temperature": True,
        "supports_vision": False,
        "supports_function_calling": True,
        "max_context": 16385,
    },
}


def get_model_features(model: str) -> Dict[str, Any]:
    """
    è·å–æ¨¡å‹ç‰¹æ€§
    
    Args:
        model: æ¨¡å‹åç§°
    
    Returns:
        æ¨¡å‹ç‰¹æ€§å­—å…¸
    """
    # ç²¾ç¡®åŒ¹é…
    if model in MODEL_FEATURES:
        return MODEL_FEATURES[model]
    
    # å‰ç¼€åŒ¹é…
    for model_prefix, features in MODEL_FEATURES.items():
        if model.startswith(model_prefix):
            return features
    
    # é»˜è®¤ç‰¹æ€§ï¼ˆGPT-4 ç±»ä¼¼ï¼‰
    return {
        "max_tokens_param": "max_tokens",
        "supports_temperature": True,  # é»˜è®¤æ”¯æŒ
        "supports_vision": False,
        "supports_function_calling": True,
        "max_context": 8192,
    }


def validate_model_params(model: str, **params) -> Dict[str, Any]:
    """
    éªŒè¯å¹¶è°ƒæ•´æ¨¡å‹å‚æ•°
    
    Args:
        model: æ¨¡å‹åç§°
        **params: å‚æ•°å­—å…¸
    
    Returns:
        éªŒè¯å¹¶è°ƒæ•´åçš„å‚æ•°
    """
    features = get_model_features(model)
    validated_params = params.copy()
    
    # è°ƒæ•´ max_tokens å‚æ•°å
    if "max_tokens" in validated_params:
        max_tokens_value = validated_params.pop("max_tokens")
        correct_param_name = features["max_tokens_param"]
        validated_params[correct_param_name] = max_tokens_value
    
    # é™åˆ¶ max_tokens ä¸è¶…è¿‡ä¸Šä¸‹æ–‡é•¿åº¦
    max_tokens_param = features["max_tokens_param"]
    if max_tokens_param in validated_params:
        validated_params[max_tokens_param] = min(
            validated_params[max_tokens_param],
            features["max_context"] // 2  # ç•™ä¸€åŠç»™è¾“å…¥
        )
    
    return validated_params


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    print("=" * 60)
    print("LLM å‚æ•°å…¼å®¹æ€§æµ‹è¯•")
    print("=" * 60)
    
    test_messages = [
        {"role": "user", "content": "Hello"}
    ]
    
    # æµ‹è¯• GPT-5
    print("\n1. GPT-5-mini:")
    params = prepare_llm_params("gpt-5-mini", test_messages, max_tokens=100)
    print(f"   å‚æ•°: {params}")
    print(f"   max_tokens å‚æ•°å: {get_max_tokens_param_name('gpt-5-mini')}")
    
    # æµ‹è¯• GPT-4
    print("\n2. GPT-4:")
    params = prepare_llm_params("gpt-4", test_messages, max_tokens=100)
    print(f"   å‚æ•°: {params}")
    print(f"   max_tokens å‚æ•°å: {get_max_tokens_param_name('gpt-4')}")
    
    # æµ‹è¯•ç‰¹æ€§è·å–
    print("\n3. æ¨¡å‹ç‰¹æ€§:")
    for model in ["gpt-5-pro", "gpt-5-mini", "gpt-4", "gpt-3.5-turbo"]:
        features = get_model_features(model)
        print(f"   {model}:")
        print(f"     - max_tokens å‚æ•°: {features['max_tokens_param']}")
        print(f"     - æœ€å¤§ä¸Šä¸‹æ–‡: {features['max_context']}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")
